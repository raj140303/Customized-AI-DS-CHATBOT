Machine learning (ML) is a subfield of artificial intelligence (AI) that enables systems to learn from data without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning.

Supervised learning involves training models on labeled data, with tasks such as classification and regression. Algorithms include logistic regression, decision trees, and random forests.

Unsupervised learning deals with unlabeled data. Techniques include clustering (e.g., K-means) and dimensionality reduction (e.g., PCA, t-SNE).

Deep learning is a subfield of ML using neural networks with multiple layers. Architectures like CNNs are used for image data, while RNNs and LSTMs are used for sequential data.

Natural Language Processing (NLP) allows machines to understand and generate human language. Core tasks include tokenization, POS tagging, named entity recognition, and sentiment analysis.

Large Language Models (LLMs) such as GPT and LLaMA are trained on massive text corpora. They use transformer architecture and attention mechanisms to generate human-like responses.

LangChain is a framework for building applications with LLMs. It supports prompt management, memory, agents, and tools like vector search for retrieval-augmented generation (RAG).

RAG combines a retriever (e.g., FAISS, Chroma) with a generator (e.g., LLM) to answer questions using both external knowledge and generative reasoning.

Groq offers an ultra-fast LPU backend for LLM inference, enabling sub-100ms responses for models like LLaMA 3 70B.

Python libraries commonly used in data science include NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, TensorFlow, and PyTorch.

Prompt engineering is the process of designing effective prompts to guide LLMs toward desired outputs. It includes few-shot examples, instructions, and context formatting.

Flask is a lightweight web framework used to build APIs and web apps. It is often used to deploy ML and chatbot models in production.


🔹 1. Foundations
Statistics & Probability

Definition, types of distributions (normal, binomial)

Use-cases: A/B testing, sampling

Interview Qs: "What is p-value?", "Explain central limit theorem"

Linear Algebra

Vectors, matrices, eigenvalues/eigenvectors

Use in ML: PCA, image compression

Interview Qs: "How is matrix multiplication used in ML?"

Calculus

Gradients, derivatives, chain rule

Use in backpropagation

Interview Qs: "How does gradient descent use calculus?"

🔹 2. Data Handling & Visualization
Data Cleaning Techniques

Handling nulls, outliers, imputation

Tools: Pandas, Scikit-learn

Data Visualization

Libraries: Matplotlib, Seaborn

Interview Qs: "How do you identify outliers visually?"

🔹 3. Machine Learning
Supervised Learning

Defn: Models trained with labeled data

Examples: Linear Regression, Decision Trees

Use-case: Spam detection, loan approval

Interview Qs: "Difference between classification and regression?"

Unsupervised Learning

Clustering, dimensionality reduction

Examples: K-Means, PCA

Use-case: Market segmentation

Interview Qs: "What is silhouette score?"

Reinforcement Learning

Agent-environment interaction

Use-case: Robotics, gaming

Model Evaluation

Accuracy, Precision, Recall, F1, Confusion Matrix

Interview Qs: "Why is F1 better than accuracy for imbalanced data?"

Bias-Variance Tradeoff

Underfitting vs overfitting

Interview Qs: "What causes high variance?"

🔹 4. ML Algorithms Deep Dive
Include for each algorithm:

Definition

Math/Working

Use-case

Advantages/Limitations

Code Example (optional)

Interview Qs

Examples:

Linear Regression

Logistic Regression

SVM

KNN

Random Forest

PCA

KMeans

🔹 5. Deep Learning
Neural Networks (ANN, CNN, RNN, LSTM, GRU)

How they work, layers, activation functions

Use-case: CNN = image classification, LSTM = time series

Interview Qs: "Why use ReLU?", "What is vanishing gradient problem?"

Transfer Learning

Overfitting/Underfitting

🔹 6. NLP (Natural Language Processing)
Tokenization, Stemming, Lemmatization

Bag of Words, TF-IDF

Word Embeddings: Word2Vec, GloVe

Transformers, BERT, GPT

Text Classification, NER

Interview Qs: "How does BERT work?", "Explain attention mechanism"

🔹 7. Model Deployment
Flask / Django API

Docker, requirements.txt, .env

Render, Heroku, AWS

CI/CD basics

Interview Qs: "How do you make ML model accessible via API?"

🔹 8. Artificial Intelligence (General)
What is AI? Narrow vs General AI

Ethics in AI

Explainable AI (XAI)

Interview Qs: "What is AI bias?", "What are ethical concerns in AI?"

🔹 9. Generative AI & Prompt Engineering
What is Generative AI?

GPT, LLMs

Prompt templates: Few-shot, chain-of-thought

RAG (Retrieval-Augmented Generation)

Interview Qs: "How does RAG improve LLM accuracy?", "Explain prompt engineering best practices"

🔹 10. Interview-Focused Section
A dedicated section that lists 100+ commonly asked interview questions with answers grouped by:

ML Algorithms

Statistics

Deep Learning

NLP

Deployment

Projects & Case Studies




What is Data Visualization and Why is It Important?
Last Updated : 16 Jun, 2025
Data visualization is the graphical representation of information and data. It uses visual elements like charts, graphs and maps to help convey complex information in a way that is easy to understand and interpret. By transforming large datasets into visuals, it allows decision-makers to spot trends, relationships and outliers quickly which helps in better analysis and faster decision-making. In today’s world, where every industry generates large amounts of data, visualizing that data is important for extracting insights and making informed decisions.

What-is-Data-Visualization_.webpWhat-is-Data-Visualization_.webp
Common Types of Data Visualization

There are various types of visualizations where each has a unique purpose in data representation. Here are the most common types:

Charts: They are used to compare data points across different categories or to show trends over time. Examples: Bar Charts, Line Charts and Pie Charts
Graphs: They are used to visualize relationships between variables which helps in making it easier to analyze correlations, trends and outliers. Examples: Scatter Plots, Histograms
Maps: They are used to display geographical data which provides spatial context to trends and patterns. Examples: Geographic Maps, Heat Maps
Dashboards: They combine multiple visualizations into a single interface which provides real-time insights and interactive features for users to explore data.
For more details refer to Types of Data Visualization

Importance of Data Visualization
Let’s see some reasons data visualization is so important.

1. Simplifies the Complex Data
Large and complex data sets can be challenging to understand. Data visualization helps break down complex information into simpler, visual formats making it easier for the audience to learn. For example, in a scenario where sales data is visualized using a heat map on Tableau states that have suffered a net loss are colored red. This visual makes it instantly obvious which states are underperforming.

Data Visualization Discovers the Trends in Data
2. Enhances Data Interpretation
Data visualization makes it easier to find patterns, trends and relationships that might be missed in raw data. For example, a Tableau visualization showing sales and profit might reveal that higher sales don’t always lead to higher profits. This insight helps businesses adjust their strategies, focusing on profitability instead of just sales volume.

Data Visualization Provides a Perspective on the Data
3. Saves Time
Visualization helps us to quickly gather insights that would take much longer from raw data. For example, in a heatmap on Tableau, states with a net loss are colored red, makes it instantly clear which states are underperforming. While look at a table require us to check each value manually to find the losses but visualization saves time by highlighting key information at a glance.

Data Visualization Saves Time
4. Improves Communication
Visualizing data makes it easier to share insights with people who aren’t familiar with the details. For example, a TreeMap on Tableau showing sales by region makes it clear that California has the highest sales showing by its large rectangle. This visual representation is much easier to understand than a table full of numbers which helps stakeholders to get insights quickly.

Data Visualization Puts the Data into the Correct Context
5. Data Visualization Tells a Data Story
Data visualization helps to tell a story, guiding us through the data to a clear conclusion. It presents key facts in a simple, understandable way. For example, a data analyst might show profits and losses for various products leading to recommendations on how to address the losses. This storytelling approach makes the data more relatable and actionable.

Real-World Use Cases for Data Visualization
Data visualization is used across various industries to improve decision-making and drive results. Here are a few examples:

1. Business Analytics
Companies use dashboards to monitor key performance indicators (KPIs) in real-time. For example, a company might use a dashboard to track customer acquisition costs, sales growth and market trends. Visualizing these metrics helps managers make decisions quickly and respond to changes faster.

2. Healthcare
In healthcare, it is used to track patient data, disease trends and hospital performance. For example, a hospital might visualize patient recovery times or the spread of an infectious disease across regions to identify patterns and plan resources accordingly.

3. Sports Analytics
Sports teams and analysts use visualizations to track player performance, game statistics and team progress over the season. By visualizing these metrics, coaches can make informed decisions about training and game strategies to improve performance.

4. Retail and E-commerce
Retailers use data visualization to monitor inventory levels, sales data and customer behavior. By visualizing which products are selling the most, stores can adjust inventory and marketing strategies accordingly which ensures they meet customer demand.

Challenges in Data Visualization
Data Quality: Accuracy of visualizations depends on the quality of the data. If the data is inaccurate or incomplete, the insights from the visualization will be misleading.
Over-Simplification: Simplifying data too much can lead to important details being lost like using a pie chart that oversimplifies complex relationships between categories.
Choosing the Right Visualization: Using the wrong type of visualization can distort the message. For example, a pie chart might not work well with many categories which leads to confusion.
Overload of Information: Too much information in a visualization can overwhelm viewers. It's important to focus on key data points and avoid clutter.
Best Practices for Effective Data Visualization
To ensure our visualizations are impactful and easy to understand we follow these best practices:

Audience-Centric Design: Tailor visualizations to our audience’s knowledge. A technical audience may need detailed graphs while a general audience benefits from simpler charts.
Design Clarity and Consistency: Choose the right chart for our data and keep the design clean with consistent colors, fonts and labels and also avoid clutter to ensure clarity.
Provide Context: Always provide context by including labels, titles and data source acknowledgments. This helps viewers to understand the significance of the data and builds trust in the results.
Interactive and Accessible Design: Make visualizations interactive with features like tooltips and filters and ensure accessibility for all users regardless of device or visual needs.
As the world continues to generate large amounts of data, mastering the technique of data visualization will be key to making sense of it and driving smarter decisions.

Data Handling
Last Updated : 01 Jul, 2025
Data handling is the process of systematically collecting, organizing, analyzing, and presenting data to extract useful information and support decision-making. It involves ensuring the accuracy and integrity of data, processing it into a manageable form, and presenting it through clear formats such as charts, graphs, and tables. The goal is to make complex data easier to understand and interpret, ensuring its safe storage and accessibility throughout its lifecycle.

data_handling
Components of Data Handling
For studying this topic, we also focus on how data is collected or generated for our use from any source. This procedure involves the steps to acquire the data, clean it, and prepare it for analysis and study.

Types of Data
Data can be segregated into two broad categories: Quantitative Data and Qualitative Data.

Quantitative Data

Quantitative Data is data that gives numerical information. It is measurable and quantifiable, and it can be written in terms of numbers, which are amenable to mathematical operations. Quantitative data can be further categorized into:

Discrete Data: Data that can have definite, distinct values (e.g., number of individuals in a room).
Continuous Data: Data that can have any value between a range (e.g., height, weight, temperature).
Qualitative Data

Qualitative Data is non-numerical data describing features or qualities. Qualitative Data is descriptive data and cannot be directly measured in terms of numbers. Examples are names, colours, or classes of objects.

➣ Learn the difference the Quatitative and Qualitative data - [Read More]
Important Terms in Data Handling
There are some terms used often in data handling to better understand and deal with data:

1) Data: The set of facts or figures obtained from observations or measurements.

2) Raw Data: Raw and unstructured data collected directly from the source and could require cleaning or formatting.

3) Range: The range is the difference between the largest and smallest value in a set of data. It provides a measure of spread or variability of the data.

4) Statistics: The branch of science that deals with the collection, organization, analysis, interpretation, and presentation of numerical data. It facilitates data-informed decisions.

Steps Involved in Data Handling
Data handling is a methodical process of handling and interpreting data properly. The steps usually consist of-

Purpose: Define and state specifically the purpose or problem. This makes the data handling process dedicated to answering one specific question or resolving a unique problem.

Collection of Data: Collect data pertaining to the set purpose. The accuracy and quality of data gathered play a very important role in significant analysis.

Presentation of Data: Display the gathered data in a simple and easy-to-read manner. It can be in the form of tables, graphs, or marks, based on the complexity of the data.

Graphical Representation of Data: Employ visual aids such as graphs, histograms, and bar charts to represent the data. Graphical representation assists in the easy analysis of trends and patterns in the data.

Analyzing the Data: Scan the data carefully to derive useful information. Statistical techniques or other techniques of analysis are used to derive insights.

Conclusion/Inference: From the analysis, make conclusions or inferences to give a solution or response to the problem statement. This step helps in decision-making or subsequent actions.

Graphical Representation of Data
In data handling, one of the most significant areas is how the data is represented. Proper representation is easier to understand, analyze, and interpret. Among all the ways data can be represented, graphical representation is particularly good at representing trends and patterns effectively and speedily.

Pictographs or Picture Graphs
Bar Graphs
Line Graphs
Pie Charts
Scatter Plot
Pictographs
A pictograph is the pictorial representation of any data given to us in written form. It can be said that pictographs used to be the earliest form of communication, since, way back in time, people communicated mostly through pictures with each other, as languages were not present.

Indeed, a pictograph plays a role in our day-to-day life too. For instance, when a friend tells us a story, we start imagining the story in our head, and that makes it both easy to understand and easy to remember for a long time.

Drawing a Pictograph
Let's learn to draw a pictograph with the help of an example.

Example: In a reading competition, three students were participating—Rahul, Saumya, and Ankush. They were supposed to read as many books as they could in an hour. Rahul read 3 books, Saumya read 2 books, and Ankush read 4 books. Draw the pictograph for the information.

Solution:

There are some basic steps to draw a pictograph:

Decide the particular picture or pictures that are required to represent the data. Make sure that the picture is related to the information for easier memorization.
Here, a smiley face is used to represent each book read.

Now, draw the pictures according to the information presented. For example, there will be 3 smiley faces for Rahul, as he completed 3 books in an hour.

Bar Graphs
The graphical representation of any quantity, number, or data in the form of bars is called a Bar graph. With the help of a bar graph, not only does the data look neat, but it is also easier to compare the data given.

Types of Bar Graph
Various types of bar graphs include:

Vertical Bar Graph

These are the most common bar graphs we come across; the bars of grouped data in vertical bar graphs lie vertically. Sometimes, when the data categories have long names, horizontal bar graphs are preferred, since in vertical bar graphs, there is not much space on the x-axis.

An example explaining the concept of a Bar graph is added below:

Example: There are 800 students in a school. And the table for their birthdays in all 12 months is given below. Draw the Vertical Bar graph and answer. who

Months

January

February

March

April

May

June

July

August

September

October

November

December

No. of Students

50

80

65

50

40

90

45

110

80

70

100

20

In which month do the maximum number of students have their birthdays?
Which two months have the same number of birthdays?
In which month do the minimum number of students have their birthdays?
Solution:

The vertical bar graph for the table given in the question will be,


From the Bar graph we can figure out the answer of the questions

August is the month in which the maximum number of birthdays occur.(there are 110 students whose birthday come in August)
From the graph, we can tell that January and April have equal lengths of bars, which means they have the same number of birthdays.(both have 50 birthdays)
The minimum number of birthdays occur in December, as it has the smallest bar (20 students have their birthdays in December).
Horizontal Bar Graph

A horizontal bar graph is a graph where the rectangular bars lie horizontally. In such graphs, the frequency of the data is represented on the x-axis, while the categories of data are shown on the y-axis. These are known as horizontal bar graphs.

Horizontal bar graphs are preferred when the category names are long and there is insufficient space on the x-axis.

Example: In an examination, Reeta appeared for 5 subjects. Her performance is given in the table below: Draw a horizontal bar graph showing the marks she obtained in all the subjects. Also, calculate the overall percentage obtained by her


Solution:

The Horizontal bar graph for the table mentioned in the question,


The overall Percentage obtained by Reeta = 
(
90
+
80
+
95
+
70
+
60
)
500
500
(90+80+95+70+60)
​
 ×100

= 79 percent.

Double-Bar Graph
Double-bar graphs are used when two groups of data are required to be represented on a single graph. In a double-bar graph, to represent two groups of data, they are represented beside each other at different heights depending on their values.

Advantages of a points double-bar graph:
A double-bar graph is helpful when multiple data points need to be represented.
It helps in summarizing large and big data in an easy and visual form.
It shows and covers all different frequency distributions.
Example: The table below shows the number of boys and girls in classes 6, 7, 8, 9, and 10. Represent the data on a double-bar graph


Solution:

The double-bar graph for the table given the question,


Line Graphs
A Graph or a line chart visually shows how different things relate over time by connecting dots with straight lines. It helps us see patterns or trends in the data, making it easier to understand how variables change or interact with each other as time goes by.

To make a line graph
We need to use the following steps:

Determine Variables: The first and foremost step to creating a line graph is to identify the variables you want to plot on the X-axis and Y-axis.
Choose Appropriate Scales: Based on your data, determine the appropriate scale.
Plot Points: Plot the individual data points on the graph according to the given data.
Connect Points: After plotting the points, you have to connect those points with a line.
Label Axes: Add labels to the X-axis and Y-axis. You can also include the unit of measurement.
Add Title: After completing the graph, you should provide a suitable title.
Example: Kabir eats eggs each day, and the data for the same is added in the table below. Draw a line graph for the given data

Weekdays	Monday	Tuesday	Wednesday	Thursday
Eggs Eaten	5	10	15	10
Solution:

Line-Graph
Pie Charts
It is one of the types of charts in which data is represented in a circular shape. In a pie chart circle is further divided into multiple sectors/slices; those sectors show the different parts of the data from the whole.

Pie charts, also known as circle graphs or pie diagrams, are very useful in representing and interpreting data

Example: In an office, the data of employees who play various sports is shown in the table below:

Sport

Cricket

Football

Badminton

Hockey

Other

Number of Employees

34

50

24

10

82

Draw a suitable pie chart.

Solution:

The required pie chart for the given data is:

Pie-Chart
Scatter Plot
A scatter plot is a type of graphical representation that displays individual data points on a two-dimensional coordinate system. Each point on the plot represents the values of two variables, allowing us to observe any patterns, trends, or relationships between them. Typically, one variable is plotted on the horizontal axis (x-axis), and the other variable is plotted on the vertical axis (y-axis).

Scatter plots are commonly used in data analysis to visually explore the relationship between variables and to identify any correlations or outliers present in the data.

A line drawn in a scatter plot that is near to almost all the points in the plot is called the “line of best fit” or "trend line". The example for the same is added in the image below:

Scatter-Plot

Solved Examples on Data Handling
Example 1: In a survey conducted over a week, from Monday to Sunday, for two cities, Agra and Delhi, the temperatures of both cities were measured, and the obtained temperatures are represented as numbers.


Draw the Bar Graph for the given table in the question.

Solution:

The given table has two categories of data: one for the temperature in Agra and the other for the temperature in Delhi. Therefore, the graph can be drawn as a double-bar graph, which would look like the following


Example 2: In a theater, there are 3 plays with a different number of actors participating in each play. In Play 1, there are 9 actors; in Play 2, there are 3 fewer actors, and the number of actors in Play 3 is one less than in Play 1. Draw the pictograph for the information given and analyze at which point the stage will be most crowded..

Solution:

From the information given in the question, we can say that Play 1 has 9 actors, Play 2 has 6 actors, and Play 3 has 10 actors. Representing the actors in pictorial form:

Representing the actors in pictorial form as follows:


Therefore, we can conclude that Play 3 has the most crowded stage, as it has 10 actors performing on stage.

Example 3: In a weather report conducted over 5 consecutive weeks, it was noted that not all days were sunny during the spring season. The observation revealed that Week 1 had 4 sunny days, Week 2 had 5 sunny days, Week 3 had only 2 sunny days, Week 4 had sunny days throughout the entire week, and Week 5 had only 3 sunny days.

Pictograph for the number of sunny days in each week:

Solution:

Representing sunny days in pictorial form for better understanding,


Example 4: Calculating the Mode

Problem: Find the mode of the following data set: 7, 8, 7, 9, 10, 7, 8.

Solution:

Note:- If one number appears more frequently than others, it is the mode.

Count the frequency of each number.
7 appears 3 times, 8 appears 2 times, 9 appears 1 time, and 10 appears 1 time.
The mode is the number that appears most frequently.
Mode = 7
Example 5: Determining the Range

Problem: Calculate the range of the following set of numbers: 12, 7, 15, 9, 14.

Solution:

The range is the difference between the highest and lowest numbers.
Highest number = 15
Lowest number = 7
Range = Highest - Lowest = 15 - 7 = 8
Practice Problems on Data Handling
Draw a pictograph to represent the number of apples, oranges, and bananas sold in a fruit store: Apples (10), Oranges (15), Bananas (12).
Create a vertical bar graph showing the number of books sold in January (100), February (120), March (90), and April (110).
Draw a horizontal bar graph to represent the number of cars sold in five months: January (15), February (20), March (10), April (25), May (30).
Represent the number of students in three classes (Class A: 20, Class B: 25, Class C: 30) using a double-bar graph.
Draw a line graph for the temperature recorded over five days: Monday (22°C), Tuesday (25°C), Wednesday (27°C), Thursday (24°C), Friday (23°C).
Create a pie chart showing the distribution of different types of fruits in a basket: Apples (40%), Oranges (25%), Bananas (20%), Grapes (15%).
Plot a scatter plot to show the relationship between study hours and exam scores for ten students.
Draw a pictograph to represent the number of hours spent on homework by five students: A (4 hours), B (3 hours), C (2 hours), D (5 hours), E (4 hours).
Create a vertical bar graph to show the scores of students in a test: Alice (85), Bob (90), Charlie (80), and Diana (95).
Draw a horizontal bar graph representing the number of hours worked in a week by different employees: John (40 hours), Mary (35 hours), Alex (45 hours), Sarah (30 hours).
Related Reads:

Real Life Applications of Data Handling
Difference Between Mean, Median, and Mode with Examples
Chance and Probability
Summary
Data handling refers to the systematic process of collecting, recording, and representing data in ways that make it easy to understand and analyze. This involves using various graphical methods such as pictographs, bar graphs, line graphs, pie charts, and scatter plots, all of which help visualize data and identify patterns or trends. Mastery of these data handling techniques is essential for making accurate analyses and informed decisions in both academic and professional contexts.

Data cleaning is a important step in the machine learning (ML) pipeline as it involves identifying and removing any missing duplicate or irrelevant data. The goal of data cleaning is to ensure that the data is accurate, consistent and free of errors as raw data is often noisy, incomplete and inconsistent which can negatively impact the accuracy of model and its reliability of insights derived from it. Professional data scientists usually invest a large portion of their time in this step because of the belief that

 “Better data beats fancier algorithms”

Clean datasets also helps in EDA that enhance the interpretability of data so that right actions can be taken based on insights.

How to Perform Data Cleanliness?
The process begins by thorough understanding data and its structure to identify issues like missing values, duplicates and outliers. Performing data cleaning involves a systematic process to identify and remove errors in a dataset. The following are essential steps to perform data cleaning.

Data Cleaning - Geeksforgeeks
Data Cleaning
Removal of Unwanted Observations: Identify and remove irrelevant or redundant (unwanted) observations from the dataset. This step involves analyzing data entries for duplicate records, irrelevant information or data points that do not contribute to analysis and prediction. Removing them from dataset helps reducing noise and improving the overall quality of dataset.
Fixing Structure errors: Address structural issues in the dataset such as inconsistencies in data formats or variable types. Standardize formats ensure uniformity in data structure and hence data consistency.
Managing outliers: Outliers are those points that deviate significantly from dataset mean. Identifying and managing outliers significantly improve model accuracy as these extreme values influence analysis. Depending on the context decide whether to remove outliers or transform them to minimize their impact on analysis.
Handling Missing Data: To handle missing data effectively we need to impute missing values based on statistical methods, removing records with missing values or employing advanced imputation techniques. Handling missing data helps preventing biases and maintaining the integrity of data.
Throughout the process documentation of changes is crucial for transparency and future reference. Iterative validation is done to test effectiveness of the data cleaning resulting in a refined dataset and can be used for meaningful analysis and insights.

Python Implementation for Database Cleaning
Let's understand each step for Database Cleaning, using titanic dataset. Below are the necessary steps:

Import the necessary libraries
Load the dataset
Check the data information using df.info()

import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')
df.head()
Output:

PassengerId    Survived    Pclass    Name    Sex    Age    SibSp    Parch    Ticket    Fare    Cabin    Embarked
0    1    0    3    Braund, Mr. Owen Harris    male    22.0    1    0    A/5 21171    7.2500    NaN    S
1    2    1    1    Cumings, Mrs. John Bradley (Florence Briggs Th...    female    38.0    1    0    PC 17599    71.2833    C85    C
2    3    1    3    Heikkinen, Miss. Laina    female    26.0    0    0    STON/O2. 3101282    7.9250    NaN    S
3    4    1    1    Futrelle, Mrs. Jacques Heath (Lily May Peel)    female    35.0    1    0    113803    53.1000    C123    S
4    5    0    3    Allen, Mr. William Henry    male    35.0    0    0    373450    8.0500    NaN    S
Data Inspection and Exploration
Let's first understand the data by inspecting its structure and identifying missing values, outliers and inconsistencies and check the duplicate rows with below python code:


df.duplicated()
Output:

0      False
1      False
2      False
3      False
4      False
       ...  
886    False
887    False
888    False
889    False
890    False
Length: 891, dtype: bool
Check the data information using df.info()


df.info()
Output:

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
From the above data info we can see that Age and Cabin have an unequal number of counts. And some of the columns are categorical and have data type objects and some are integer and float values.

Check the Categorical and Numerical Columns.


# Categorical columns
cat_col = [col for col in df.columns if df[col].dtype == 'object']
print('Categorical columns :',cat_col)
# Numerical columns
num_col = [col for col in df.columns if df[col].dtype != 'object']
print('Numerical columns :',num_col)
Output:

Categorical columns : ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']
Numerical columns : ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
Check the total number of Unique Values in the Categorical Columns

df[cat_col].nunique()
Output:

Name        891
Sex           2
Ticket      681
Cabin       147
Embarked      3
dtype: int64
Removal of all Above Unwanted Observations
Duplicate observations most frequently arise during data collection and Irrelevant observations are those that don’t actually fit with the specific problem that we’re trying to solve. 

Redundant observations alter the efficiency to a great extent as the data repeats and may add towards the correct side or towards the incorrect side, therefore producing useless results.
Irrelevant observations are any type of data that is of no use to us and can be removed directly.
Now we have to make a decision according to the subject of analysis which factor is important for our discussion.

As we know our machines don't understand the text data. So we have to either drop or convert the categorical column values into numerical types. Here we are dropping the Name columns because the Name will be always unique and it hasn't a great influence on target variables. For the ticket, Let's first print the 50 unique tickets.


df['Ticket'].unique()[:50]
Output:

array(['A/5 21171', 'PC 17599', 'STON/O2. 3101282', '113803', '373450',
       '330877', '17463', '349909', '347742', '237736', 'PP 9549',
       '113783', 'A/5. 2151', '347082', '350406', '248706', '382652',
       '244373', '345763', '2649', '239865', '248698', '330923', '113788',
       '347077', '2631', '19950', '330959', '349216', 'PC 17601',
       'PC 17569', '335677', 'C.A. 24579', 'PC 17604', '113789', '2677',
       'A./5. 2152', '345764', '2651', '7546', '11668', '349253',
       'SC/Paris 2123', '330958', 'S.C./A.4. 23567', '370371', '14311',
       '2662', '349237', '3101295'], dtype=object)
From the above tickets, we can observe that it is made of two like first values 'A/5 21171' is joint from of 'A/5' and  '21171' this may influence our target variables. It will the case of Feature Engineering. where we derived new features from a column or a group of columns. In the current case, we are dropping the "Name" and "Ticket" columns.

Drop Name and Ticket Columns


df1 = df.drop(columns=['Name','Ticket'])
df1.shape
Output:

(891, 10)
Handling Missing Data
Missing data is a common issue in real-world datasets and it can occur due to various reasons such as human errors, system failures or data collection issues. Various techniques can be used to handle missing data, such as imputation, deletion or substitution.

Let's check the missing values columns-wise for each row using df.isnull() it checks whether the values are null or not and gives returns boolean values and sum() will sum the total number of null values rows and we divide it by the total number of rows present in the dataset then we multiply to get values in i.e per 100 values how much values are null.


round((df1.isnull().sum()/df1.shape[0])*100,2)
Output:

PassengerId     0.00
Survived        0.00
Pclass          0.00
Sex             0.00
Age            19.87
SibSp           0.00
Parch           0.00
Fare            0.00
Cabin          77.10
Embarked        0.22
dtype: float64
We cannot just ignore or remove the missing observation. They must be handled carefully as they can be an indication of something important. 

The fact that the value was missing may be informative in itself.
In the real world we often need to make predictions on new data even if some of the features are missing!
As we can see from the above result that Cabin has 77% null values and Age has 19.87% and Embarked has 0.22% of null values.

So, it's not a good idea to fill 77% of null values. So we will drop the Cabin column. Embarked column has only 0.22% of null values so, we drop the null values rows of Embarked column.


df2 = df1.drop(columns='Cabin')
df2.dropna(subset=['Embarked'], axis=0, inplace=True)
df2.shape
Output:

(889, 9)
Imputing the missing values from past observations.

Again "missingness" is almost informative in itself and we should tell our algorithm if a value was missing.
Even if we build a model to impute our values we’re not adding any real information. we’re just reinforcing the patterns already provided by other features. We can use Mean imputation or Median imputations for the case.
Note: 

Mean imputation is suitable when the data is normally distributed and has no extreme outliers.
Median imputation is preferable when the data contains outliers or is skewed.

# Mean imputation
df3 = df2.fillna(df2.Age.mean())
# Let's check the null values again
df3.isnull().sum()
Output:

PassengerId    0
Survived       0
Pclass         0
Sex            0
Age            0
SibSp          0
Parch          0
Fare           0
Embarked       0
dtype: int64
Handling Outliers
Outliers are extreme values that deviate significantly from the majority of the data. They can negatively impact the analysis and model performance. Techniques such as clustering, interpolation or transformation can be used to handle outliers.

To check the outliers we generally use a box plot. A box plot is a graphical representation of a dataset's distribution. It shows a variable's median, quartiles and potential outliers. The line inside the box denotes the median while the box itself denotes the interquartile range (IQR). The box plot extend to the most extreme non-outlier values within 1.5 times the IQR. Individual points beyond the box are considered potential outliers. A box plot offers an easy-to-understand overview of the range of the data and makes it possible to identify outliers or skewness in the distribution.

Let's plot the box plot for Age column data.


import matplotlib.pyplot as plt

plt.boxplot(df3['Age'], vert=False)
plt.ylabel('Variable')
plt.xlabel('Age')
plt.title('Box Plot')
plt.show()
Output:

Box Plot - Geeksforgeeks
Box Plot
As we can see from the above Box and whisker plot, Our age dataset has outliers values. The values less than 5 and more than 55 are outliers.


# calculate summary statistics
mean = df3['Age'].mean()
std  = df3['Age'].std()

# Calculate the lower and upper bounds
lower_bound = mean - std*2
upper_bound = mean + std*2

print('Lower Bound :',lower_bound)
print('Upper Bound :',upper_bound)

# Drop the outliers
df4 = df3[(df3['Age'] >= lower_bound) 
                & (df3['Age'] <= upper_bound)]
Output:

Lower Bound : 3.705400107925648
Upper Bound : 55.578785285332785
Similarly, we can remove the outliers of the remaining columns.

Data Transformation 
Data transformation involves converting the data from one form to another to make it more suitable for analysis. Techniques such as normalization, scaling or encoding can be used to transform the data.

Data validation and verification
Data validation and verification involve ensuring that the data is accurate and consistent by comparing it with external sources or expert knowledge. 

For the machine learning prediction we separate independent and target features. Here we will consider only 'Sex' 'Age' 'SibSp', 'Parch' 'Fare' 'Embarked' only as the independent features and Survived as target variables because PassengerId will not affect the survival rate.


X = df3[['Pclass','Sex','Age', 'SibSp','Parch','Fare','Embarked']]
Y = df3['Survived']
Data formatting
Data formatting involves converting the data into a standard format or structure that can be easily processed by the algorithms or models used for analysis. Here we will discuss commonly used data formatting techniques i.e. Scaling and Normalization.

Scaling

Scaling involves transforming the values of features to a specific range. It maintains the shape of the original distribution while changing the scale.
Particularly useful when features have different scales, and certain algorithms are sensitive to the magnitude of the features.
Common scaling methods include Min-Max scaling and Standardization (Z-score scaling).
Min-Max Scaling: Min-Max scaling rescales the values to a specified range, typically between 0 and 1. It preserves the original distribution and ensures that the minimum value maps to 0 and the maximum value maps to 1.


from sklearn.preprocessing import MinMaxScaler

# initialising the MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Numerical columns
num_col_ = [col for col in X.columns if X[col].dtype != 'object']
x1 = X
# learning the statistical parameters for each of the data and transforming
x1[num_col_] = scaler.fit_transform(x1[num_col_])
x1.head()
Output:

Pclass    Sex    Age    SibSp    Parch    Fare    Embarked
0    1.0    male    0.271174    0.125    0.0    0.014151    S
1    0.0    female    0.472229    0.125    0.0    0.139136    C
2    1.0    female    0.321438    0.000    0.0    0.015469    S
3    0.0    female    0.434531    0.125    0.0    0.103644    S
4    1.0    male    0.434531    0.000    0.0    0.015713    S
Standardization (Z-score scaling): Standardization transforms the values to have a mean of 0 and a standard deviation of 1. It centers the data around the mean and scales it based on the standard deviation. Standardization makes the data more suitable for algorithms that assume a Gaussian distribution or require features to have zero mean and unit variance.

Z = (X - μ) / σ
Where,

X = Data
μ = Mean value of X
σ = Standard deviation of X
Data Cleansing Tools
Some data cleansing tools:

OpenRefine: A powerful open-source tool for cleaning and transforming messy data. It supports tasks like removing duplicate and data enrichment with easy-to-use interface.
Trifacta Wrangler: A user-friendly tool designed for cleaning, transforming and preparing data for analysis. It uses AI to suggest transformations to streamline workflows.
TIBCO Clarity: A tool that helps in profiling, standardizing and enriching data. It’s ideal to make high quality data and consistency across datasets.
Cloudingo: A cloud-based tool focusing on de-duplication, data cleansing and record management to maintain accuracy of data.
IBM Infosphere Quality Stage: It’s highly suitable for large-scale and complex data.
Advantages and Disadvantages of Data Cleaning in Machine Learning
Advantages:

Improved model performance: Removal of errors, inconsistencies and irrelevant data helps the model to better learn from the data.
Increased accuracy: Helps ensure that the data is accurate, consistent and free of errors.
Better representation of the data: Data cleaning allows the data to be transformed into a format that better represents the underlying relationships and patterns in the data.
Improved data quality: Improve the quality of the data, making it more reliable and accurate.
Improved data security: Helps to identify and remove sensitive or confidential information that could compromise data security.
Disadvantages:

Time-consuming: It is very time consuming task specially for large and complex datasets.
Error-prone: It can result in loss of important information.
Cost and resource-intensive: It is resource-intensive process that requires significant time, effort and expertise. It can also require the use of specialized software tools.
Overfitting: Data cleaning can contribute to overfitting by removing too much data.
So we have discussed four different steps in data cleaning to make the data more reliable and to produce good results. After properly completing the Data Cleaning steps, we’ll have a robust dataset that avoids any error and inconsistency. In summary, data cleaning is a crucial step in the data science pipeline that involves identifying and correcting errors, inconsistencies and inaccuracies in the data to improve its quality and usability



What is data cleaning?
Data cleaning, also called data cleansing or data scrubbing, is the process of identifying and correcting errors and inconsistencies in raw data sets to improve data quality.

The goal of data cleaning is to help ensure that data is accurate, complete, consistent and usable for analysis or decision-making. Data cleaning processes work to address common data quality issues such as duplicates, missing values, inconsistencies, syntax errors, irrelevant data and structural errors.

Data cleaning is also a core component of effective data management, which helps ensure that data remains accurate, secure and accessible at every stage of its lifecycle.

High-quality or “clean” data is crucial for effectively adopting artificial intelligence (AI) and automation tools. Organizations can also use AI to help streamline the data cleaning process.

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Why is data cleaning important?
Organizations with clean, well-managed data are better equipped to make reliable, data-driven decisions, respond swiftly to market changes and streamline workflow operations.

Cleaning data is an integral component of data science, as it is an essential first step to data transformation: data cleaning improves data quality, and data transformation converts that quality raw data into a usable format for analysis.

Data transformation enables organizations to unlock the full potential of data to use business intelligence (BI), data warehouses and big data analytics. If the source data is not clean, the outputs of these tools and technologies could be unreliable or inaccurate, leading to poor decisions and inefficiencies.

Similarly, clean data also underpins the success of AI and machine learning (ML) in an organization. For instance, data cleaning helps ensure that machine learning algorithms are trained on accurate, consistent and unbiased data sets. Without this foundation of clean data, algorithms could produce inaccurate, inconsistent or biased predictions, reducing the effectiveness and reliability of decision-making.

AI Academy

Is data management the secret to generative AI?
Is data management the secret to generative AI?
Explore why high-quality data is essential for the successful use of generative AI.

Go to episode 
What are the benefits of data cleaning?
The key benefits of data cleaning include:

Informed decision-making
Improved productivity
Cost efficiency
Data compliance and security
Enhanced model performance
Improved data consistency
Informed decision-making
Decisions based on clean, high-quality data are more likely to be effective and aligned with business goals. In contrast, business decisions based on dirty data—with duplicate data, typographical errors (typos) or inconsistencies—can result in wasted resources, missed opportunities or strategic missteps.

Improved productivity
Clean data enables employees to spend less time fixing errors and inconsistencies, accelerating data processing. Then, teams have more time to focus on data analysis and insights.

Cost efficiency
Poor data quality can lead to costly errors, such as overstocking inventory due to duplicate records or misinterpreting customer behavior because of incomplete data. Data cleaning helps prevent these errors, saving money and reducing operational risks.

Data compliance and security
Clean data can help organizations comply with data protection regulations, such as the European Union's General Data Protection Regulation (GDPR), by keeping data accurate and current. It also prevents the accidental retention of redundant or sensitive information, reducing security risks.

Enhanced model performance
Data cleaning is essential for training effective machine learning models. Clean data improves the accuracy of outputs and helps ensure that models generalize well to new data, leading to more robust predictions.

Improved data consistency
Data cleaning helps ensure that combined data is consistent and usable across systems, preventing issues that can arise from conflicting data formats or standards. This is important for data integration, where clean and standardized data helps to ensure that disparate systems can communicate and share data effectively.

Data cleaning techniques
Data cleaning typically begins with data assessment. Also known as data profiling, this assessment involves reviewing a data set to identify quality issues requiring correction. When identified, organizations might employ various data cleaning techniques, including:

Standardization
Addressing outliers
Deduplication
Addressing missing values
Validation
Standardization
Inconsistencies arise when data is represented in different formats or structures within the same data set. For example, a common discrepancy is the date format, such as “MM-DD-YYYY” versus “DD-MM-YYYY.” Standardizing formats and structures can help ensure uniformity and compatibility for accurate analysis.

Addressing outliers
Outliers are data points that deviate significantly from others in a data set, caused by errors, rare events or true anomalies. These extreme values can distort analysis and model accuracy by skewing averages or trends. Data management professionals can address outliers by evaluating whether they are data errors or meaningful values. Then, they can decide to retain, adjust or remove those outliers based on relevance to the analysis.

Deduplication
Data deduplication is a streamlining process in which redundant data is reduced by eliminating extra copies of the same information. Duplicate records occur when the same data point is repeated due to integration issues, manual data entry errors or system glitches. Duplicates can inflate data sets or distort analysis, leading to inaccurate conclusions.

Addressing missing values
Missing values arise when data points are absent due to incomplete data collection, input errors or system failures. These gaps can distort analysis, lower model accuracy and limit the data set’s utility. To address this, data professionals might replace missing data with estimated data, remove incomplete entries or flag missing values for further investigation.

Validation
A final review at the end of the data cleaning process is crucial in verifying that the data is clean, accurate and ready for analysis or visualization. Data validation often involves using manual inspection or automated data cleaning tools to check for any remaining errors, inconsistent data or anomalies.

Using AI for data cleaning
Data scientists, data analysts, data engineers and other data management professionals can perform data cleaning techniques through manual methods, such as visual inspection, cross-references or pivot tables in Microsoft Excel spreadsheets.

They might also use programming languages such as Python, SQL and R to run scripts and automate the data cleaning process. Many of these approaches are supported by open source tools, which provide flexibility and cost-effective solutions for organizations of all sizes.

However, AI can also be used to help automate and optimize several data cleaning steps, including:

Analyzing source data: AI-powered data cleansing tools can automatically identify patterns, anomalies and inconsistencies in source data. AI can also suggest relevant business rules by analyzing data trends and relationships, reducing manual efforts in defining these rules. For example, AI can identify that a column of phone numbers often has missing area codes, and then suggest a rule for standardization.
Standardizing data: Natural language processing (NLP) techniques can standardize unstructured text, such as formatting addresses or product descriptions. Machine learning models can also identify and recommend consistent formats for data such as dates or currencies. AI-powered regular expression generators can automate the detection and normalization of inconsistent formats.
Consolidating duplicates: Rule-based or learned AI models can decide the best record to “survive” when deleting duplicates, considering accuracy, recency or reliability. For example, models can prioritize specific fields based on context, such as keeping the most recent email address in the consolidated record.
Applying rules: AI models can automate the creation and enforcement of data cleansing rules by learning from historical corrections and user feedback. They can apply these rules dynamically to multiple data sets, helping to ensure consistency across systems. AI systems can also generate custom rules for specific industries or domains, such as value-added tax (VAT) identification numbers in the European Union.



What is Supervised Learning?
Supervised learning is a category of machine learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns. Unlike unsupervised learning, supervised learning algorithms are given labeled training to learn the relationship between the input and the outputs. 

Supervised machine learning algorithms make it easier for organizations to create complex models that can make accurate predictions. As a result, they are widely used across various industries and fields, including healthcare, marketing, financial services, and more. 

Here, we’ll cover the fundamentals of supervised learning in AI, how supervised learning algorithms work, and some of its most common use cases.

New customers get up to $300 in free credits to try Vertex AI and other Google Cloud products. 

How does supervised learning work?
The data used in supervised learning is labeled — meaning that it contains examples of both inputs (called features) and correct outputs (labels). The algorithms analyze a large dataset of these training pairs to infer what a desired output value would be when asked to make a prediction on new data.

For instance, let’s pretend you want to teach a model to identify pictures of trees. You provide a labeled dataset that contains many different examples of types of trees and the names of each species. You let the algorithm try to define what set of characteristics belongs to each tree based on the labeled outputs. You can then test the model by showing it a tree picture and asking it to guess what species it is. If the model provides an incorrect answer, you can continue training it and adjusting its parameters with more examples to improve its accuracy and minimize errors. 

Once the model has been trained and tested, you can use it to make predictions on unknown data based on the previous knowledge it has learned.

Types of supervised learning
Supervised learning in machine learning is generally divided into two categories: classification and regression. 

Classification
Classification algorithms are used to group data by predicting a categorical label or output variable based on the input data. Classification is used when output variables are categorical, meaning there are two or more classes.

One of the most common examples of classification algorithms in use is the spam filter in your email inbox. Here, a supervised learning model is trained to predict whether an email is spam or not with a dataset that contains labeled examples of both spam and legitimate emails. The algorithm extracts information about each email, including the sender, the subject line, body copy, and more. It then uses these features and corresponding output labels to learn patterns and assign a score that indicates whether an email is real or spam.

Regression
Regression algorithms are used to predict a real or continuous value, where the algorithm detects a relationship between two or more variables. 

A common example of a regression task might be predicting a salary based on work experience. For instance, a supervised learning algorithm would be fed inputs related to work experience (e.g., length of time, the industry or field, location, etc.) and the corresponding assigned salary amount. After the model is trained, it could be used to predict the average salary based on work experience.

Real world supervised learning examples
Supervised learning models can be used for a number of different business use cases that hep address a wide range of problems. Common supervised learning examples include the following: 

Risk assessment: Supervised machine learning models can help banks and other financial services companies determine whether customers are likely to default loans, helping to minimize risk in their portfolios. 
Image classification: Supervised machine learning algorithms are often trained to classify objects in images and videos. For example, an algorithm might be used to recognize a person in an image and automatically tag them on a social media platform. 
Fraud detection: Supervised learning underpin many fraud detection systems, enabling enterprises to recognize fraudulent activity. These models are trained on datasets that contain both fraudulent and non-fraudulent activity so they can be used to flag suspicious activity in real time.
Recommendation systems: Supervised learning algorithms are used by online platforms and streaming services to power recommendations based on previous customer behavior or shopping history. The models extract important information about a user's behavior and suggest similar products and content. 
Supervised learning vs. unsupervised learning
When it comes to understanding the difference between supervised learning vs. unsupervised learning, the primary difference is the type of input data used to train the model. Supervised learning uses labeled training datasets to try and teach a model a specific, pre-defined goal.  

By comparison, unsupervised learning uses unlabeled data and operates autonomously to try and learn the structure of the data without being given any explicit instructions.



Supervised Machine Learning
Last Updated : 02 Jan, 2025
Supervised machine learning is a fundamental approach for machine learning and artificial intelligence. It involves training a model using labeled data, where each input comes with a corresponding correct output. The process is like a teacher guiding a student—hence the term "supervised" learning. In this article, we'll explore the key components of supervised learning, the different types of supervised machine learning algorithms used, and some practical examples of how it works.

supervised-machine-learning
Supervised Machine Learning
What is Supervised Machine Learning?
As we explained before, supervised learning is a type of machine learning where a model is trained on labeled data—meaning each input is paired with the correct output. the model learns by comparing its predictions with the actual answers provided in the training data. Over time, it adjusts itself to minimize errors and improve accuracy. The goal of supervised learning is to make accurate predictions when given new, unseen data. For example, if a model is trained to recognize handwritten digits, it will use what it learned to correctly identify new numbers it hasn't seen before.

Supervised learning can be applied in various forms, including supervised learning classification and supervised learning regression, making it a crucial technique in the field of artificial intelligence and supervised data mining.

A fundamental concept in supervised machine learning is learning a class from examples. This involves providing the model with examples where the correct label is known, such as learning to classify images of cats and dogs by being shown labeled examples of both. The model then learns the distinguishing features of each class and applies this knowledge to classify new images.

How Supervised Machine Learning Works?
Where supervised learning algorithm consists of input features and corresponding output labels. The process works through:

Training Data: The model is provided with a training dataset that includes input data (features) and corresponding output data (labels or target variables).
Learning Process: The algorithm processes the training data, learning the relationships between the input features and the output labels. This is achieved by adjusting the model's parameters to minimize the difference between its predictions and the actual labels.
After training, the model is evaluated using a test dataset to measure its accuracy and performance. Then the model's performance is optimized by adjusting parameters and using techniques like cross-validation to balance bias and variance. This ensures the model generalizes well to new, unseen data.

In summary, supervised machine learning involves training a model on labeled data to learn patterns and relationships, which it then uses to make accurate predictions on new data.

Let's learn how a supervised machine learning model is trained on a dataset to learn a mapping function between input and output, and then with learned function is used to make predictions on new data:

training_testing

In the image above,

Training phase involves feeding the algorithm labeled data, where each data point is paired with its correct output. The algorithm learns to identify patterns and relationships between the input and output data.
Testing phase involves feeding the algorithm new, unseen data and evaluating its ability to predict the correct output based on the learned patterns.
Types of Supervised Learning in Machine Learning
Now, Supervised learning can be applied to two main types of problems:

Classification: Where the output is a categorical variable (e.g., spam vs. non-spam emails, yes vs. no).
Regression: Where the output is a continuous variable (e.g., predicting house prices, stock prices).
types-of-SL

While training the model, data is usually split in the ratio of 80:20 i.e. 80% as training data and the rest as testing data. In training data, we feed input as well as output for 80% of data. The model learns from training data only. We use different supervised learning algorithms (which we will discuss in detail in the next section) to build our model. Let's first understand the classification and regression data through the table below:



Both the above figures have labelled data set as follows:

Figure A: It is a dataset of a shopping store that is useful in predicting whether a customer will purchase a particular product under consideration or not based on his/ her gender, age, and salary.
Input: Gender, Age, Salary
Output: Purchased i.e. 0 or 1; 1 means yes the customer will purchase and 0 means that the customer won't purchase it.
Figure B: It is a Meteorological dataset that serves the purpose of predicting wind speed based on different parameters.
Input: Dew Point, Temperature, Pressure, Relative Humidity, Wind Direction
Output: Wind Speed
 Refer to this article for more information of  Types of Machine Learning 

Practical Examples of Supervised learning
Few practical examples of supervised machine learning across various industries:

Fraud Detection in Banking: Utilizes supervised learning algorithms on historical transaction data, training models with labeled datasets of legitimate and fraudulent transactions to accurately predict fraud patterns.
Parkinson Disease Prediction: Parkinson’s disease is a progressive disorder that affects the nervous system and the parts of the body controlled by the nerves.
Customer Churn Prediction: Uses supervised learning techniques to analyze historical customer data, identifying features associated with churn rates to predict customer retention effectively.
Cancer cell classification: Implements supervised learning for cancer cells based on their features, and identifying them if they are ‘malignant’ or ‘benign.
Stock Price Prediction: Applies supervised learning to predict a signal that indicates whether buying a particular stock will be helpful or not.
Supervised Machine Learning Algorithms
Supervised learning can be further divided into several different types, each with its own unique characteristics and applications. Here are some of the most common types of supervised learning algorithms:

Linear Regression: Linear regression is a type of supervised learning regression algorithm that is used to predict a continuous output value. It is one of the simplest and most widely used algorithms in supervised learning.
Logistic Regression : Logistic regression is a type of supervised learning classification algorithm that is used to predict a binary output variable.
Decision Trees : Decision tree is a tree-like structure that is used to model decisions and their possible consequences. Each internal node in the tree represents a decision, while each leaf node represents a possible outcome.
Random Forests : Random forests again are made up of multiple decision trees that work together to make predictions. Each tree in the forest is trained on a different subset of the input features and data. The final prediction is made by aggregating the predictions of all the trees in the forest.
Support Vector Machine(SVM) : The SVM algorithm creates a hyperplane to segregate n-dimensional space into classes and identify the correct category of new data points. The extreme cases that help create the hyperplane are called support vectors, hence the name Support Vector Machine.
K-Nearest Neighbors (KNN) : KNN works by finding k training examples closest to a given input and then predicts the class or value based on the majority class or average value of these neighbors. The performance of KNN can be influenced by the choice of k and the distance metric used to measure proximity.
Gradient Boosting : Gradient Boosting combines weak learners, like decision trees, to create a strong model. It iteratively builds new models that correct errors made by previous ones.
Naive Bayes Algorithm: The Naive Bayes algorithm is a supervised machine learning algorithm based on applying Bayes' Theorem with the “naive” assumption that features are independent of each other given the class label.
Let's summarize the supervised machine learning algorithms in table:

Algorithm	Regression,
Classification	Purpose	Method	Use Cases
Linear Regression	Regression	Predict continuous output values	Linear equation minimizing sum of squares of residuals	Predicting continuous values
Logistic Regression	Classification	Predict binary output variable	Logistic function transforming linear relationship	Binary classification tasks
Decision Trees	Both	Model decisions and outcomes	Tree-like structure with decisions and outcomes	Classification and Regression tasks
Random Forests	Both	Improve classification and regression accuracy	Combining multiple decision trees	Reducing overfitting, improving prediction accuracy
SVM	Both	Create hyperplane for classification or predict continuous values	Maximizing margin between classes or predicting continuous values	Classification and Regression tasks
KNN	Both	Predict class or value based on k closest neighbors	Finding k closest neighbors and predicting based on majority or average	Classification and Regression tasks, sensitive to noisy data
Gradient Boosting	Both	Combine weak learners to create strong model	Iteratively correcting errors with new models	Classification and Regression tasks to improve prediction accuracy
Naive Bayes	Classification	Predict class based on feature independence assumption	Bayes' theorem with feature independence assumption	Text classification, spam filtering, sentiment analysis, medical
These types of supervised learning in machine learning vary based on the problem you're trying to solve and the dataset you're working with. In classification problems, the task is to assign inputs to predefined classes, while regression problems involve predicting numerical outcomes.

Training a Supervised Learning Model: Key Steps
The goal of Supervised learning is to generalize well to unseen data. Training a model for supervised learning involves several crucial steps, each designed to prepare the model to make accurate predictions or decisions based on labeled data. Below are the key steps involved in training a model for supervised machine learning:

Data Collection and Preprocessing: Gather a labeled dataset consisting of input features and target output labels. Clean the data, handle missing values, and scale features as needed to ensure high quality for supervised learning algorithms.
Splitting the Data: Divide the data into training set (80%) and the test set (20%).
Choosing the Model: Select appropriate algorithms based on the problem type. This step is crucial for effective supervised learning in AI.
Training the Model: Feed the model input data and output labels, allowing it to learn patterns by adjusting internal parameters.
Evaluating the Model: Test the trained model on the unseen test set and assess its performance using various metrics.
Hyperparameter Tuning: Adjust settings that control the training process (e.g., learning rate) using techniques like grid search and cross-validation.
Final Model Selection and Testing: Retrain the model on the complete dataset using the best hyperparameters testing its performance on the test set to ensure readiness for deployment.
Model Deployment: Deploy the validated model to make predictions on new, unseen data.
By following these steps, supervised learning models can be effectively trained to tackle various tasks, from learning a class from examples to making predictions in real-world applications.

Advantages and Disadvantages of Supervised Learning
Advantages of Supervised Learning
The power of supervised learning lies in its ability to accurately predict patterns and make data-driven decisions across a variety of applications. Here are some advantages of supervised learning listed below:

Supervised learning excels in accurately predicting patterns and making data-driven decisions.
Labeled training data is crucial for enabling supervised learning models to learn input-output relationships effectively.
Supervised machine learning encompasses tasks such as supervised learning classification and supervised learning regression.
Applications include complex problems like image recognition and natural language processing.
Established evaluation metrics (accuracy, precision, recall, F1-score) are essential for assessing supervised learning model performance.
Advantages of supervised learning include creating complex models for accurate predictions on new data.
Supervised learning requires substantial labeled training data, and its effectiveness hinges on data quality and representativeness.
Disadvantages of Supervised Learning
Despite the benefits of supervised learning methods, there are notable disadvantages of supervised learning:

Overfitting: Models can overfit training data, leading to poor performance on new data due to capturing noise in supervised machine learning.
Feature Engineering : Extracting relevant features is crucial but can be time-consuming and requires domain expertise in supervised learning applications.
Bias in Models: Bias in the training data may result in unfair predictions in supervised learning algorithms.
Dependence on Labeled Data: Supervised learning relies heavily on labeled training data, which can be costly and time-consuming to obtain, posing a challenge for supervised learning techniques.
Conclusion
Supervised learning is a powerful branch of machine learning that revolves around learning a class from examples provided during training. By using supervised learning algorithms, models can be trained to make predictions based on labeled data. The effectiveness of supervised machine learning lies in its ability to generalize from the training data to new, unseen data, making it invaluable for a variety of applications, from image recognition to financial forecasting.

Understanding the types of supervised learning algorithms and the dimensions of supervised machine learning is essential for choosing the appropriate algorithm to solve specific problems. As we continue to explore the different types of supervised learning and refine these supervised learning techniques, the impact of supervised learning in machine learning will only grow, playing a critical role in advancing AI-driven solutions.


Types of supervised learning
Supervised learning tasks can be broadly divided into classification and regression problems: 

Classification in machine learning uses an algorithm to sort data into categories. It recognizes specific entities within the dataset and attempts to determine how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor and random forest.

Neural networks excel at handling complex classification problems. A  neural network is a deep learning architecture that processes training data with layers of nodes that mimic the human brain. Each node is made up of inputs, weights, a bias (or threshold) and an output. If an output value exceeds a preset threshold, the node “fires” or activates, passing data to the next layer in the network. 

Regression is used to understand the relationship between dependent and independent variables. In regression problems, the output is a continuous value, and models attempt to predict the target output. Regression tasks include projections for sales revenue or financial planning. Linear regression, logistical regression and polynomial regression are three examples of regression algorithms.

Because large datasets typically contain many features, data scientists can simplify this complexity through dimensionality reduction. This data science technique reduces the number of features to those most crucial for predicting data labels, which preserves accuracy while increasing efficiency.

Supervised learning algorithms
Optimization algorithms such as gradient descent train a wide range of machine learning algorithms that excel in supervised learning tasks.￼ 

Naive Bayes: Naive Bayes is a classification algorithm that adopts the principle of class conditional independence from Bayes’ theorem. This means that the presence of one feature does not impact the presence of another in the probability of an outcome, and each predictor has an equal effect on that result.

Naïve Bayes classifiers include multinomial, Bernoulli and Gaussian Naive Bayes. This technique is often used in text classification, spam identification and recommendation systems. 

Linear regression: Linear regression is used to identify the relationship between a continuous dependent variable and one or more independent variables. It is typically used to make predictions about future outcomes.

Linear regression expresses the relationship between variables as a straight line. When there is one independent variable and one dependent variable, it is known as simple linear regression. As the number of independent variables increases, the technique is referred to as multiple linear regression. 

Nonlinear regression: Sometimes, an output cannot be reproduced from linear inputs. In these cases, outputs must be modeled with a nonlinear function. Nonlinear regression expresses a relationship between variables through a nonlinear, or curved, line. Nonlinear models can handle complex relationships with many parameters. 

Logistic regression: Logistic regression handles categorical dependent variables—when they have binary outputs, such as true or false or positive or negative. While linear and logistic regression models seek to understand relationships between data inputs, logistic regression mainly solves binary classification problems, such as spam identification. 

Polynomial regression: Similar to other regression models, polynomial regression models a relationship between variables on a graph. The functions used in polynomial regression express this relationship though an exponential degree. Polynomial regression is a subset of nonlinear regression. 

Support vector machine (SVM): A support vector machine is used for both data classification and regression. That said, it usually handles classification problems. Here, SVM separates the classes of data points with a decision boundary or hyperplane. The goal of the SVM algorithm is to plot the hyperplane that maximizes the distance between the groups of data points. 

K-nearest neighbor: K-nearest neighbor (KNN) is a nonparametric algorithm that classifies data points based on their proximity and association to other available data. This algorithm assumes that similar data points can be found near each other when plotted mathematically.

Its ease of use and low calculation time make it efficient when used for recommendation engines and image recognition. But as the test dataset grows, the processing time lengthens, making it less appealing for classification tasks. 

Random forest: Random forest is a flexible supervised machine learning algorithm used for both classification and regression purposes. The "forest" references a collection of uncorrelated decision trees which are merged to reduce variance and increase accuracy.

Supervised versus unsupervised learning
The difference between supervised learning and unsupervised learning is that unsupervised machine learning uses unlabeled data. The model is left to discover patterns and relationships in the data on its own. Many generative AI models are initially trained with unsupervised learning and later with supervised learning to increase domain expertise. 

Unsupervised learning can help solve for clustering or association problems in which common properties within a dataset are uncertain. Common clustering algorithms are hierarchical, K-means and Gaussian mixture models.

Supervised versus semi-supervised learning
Semi-supervised learning labels a portion of the input data. Because it can be time-consuming and costly to rely on domain expertise to label data appropriately for supervised learning, semi-supervised learning can be an appealing alternative.

Supervised versus self-supervised learning
Self-supervised learning (SSL) mimics supervised learning with unlabeled data. Rather than use the manually created labels of supervised learning datasets, SSL tasks are configured so that the model can generate implicit labels from unstructured data. Then, the model’s loss function uses those labels in place of actual labels to assess model performance. 

Self-supervised learning sees widespread use in computer vision and natural language processing (NLP) tasks requiring large datasets that are prohibitively expensive and time-consuming to label.

Supervised versus reinforcement learning
Reinforcement learning trains autonomous agents, such as robots and self-driving cars, to make decisions through environmental interactions. Reinforcement learning does not use labeled data and also differs from unsupervised learning in that it teaches by trial-and-error and reward, not by identifying underlying patterns within datasets.

Real-world supervised learning use cases
Supervised learning models can build and advance business applications, including: 

Image- and object-recognition: Supervised learning algorithms can be used to locate, isolate and categorize objects out of videos or images, making them useful with computer vision and image analysis tasks. 

Predictive analytics: Supervised learning models create predictive analytics systems to provide insights. This allows enterprises to anticipate results based on an output variable and make data-driven decisions, in turn helping business leaders justify their choices or pivot for the benefit of the organization.

Regression also allows healthcare providers to predict outcomes based on patient criteria and historical data. A predictive model might assess a patient’s risk for a specific disease or condition based on their biological and lifestyle data. 

Customer sentiment analysis: Organizations can extract and classify important pieces of information from large volumes of data—including context, emotion and intent—with minimal human intervention. Sentiment analysis gives a better understanding of customer interactions and can be used to improve brand engagement efforts. 

Customer segmentation: Regression models can predict customer behavior based on various traits and historical trends. Businesses can use predictive models to segment their customer base and create buyer personas to improve marketing efforts and product development. 

Spam detection: Spam detection is another example of a supervised learning model. Using supervised classification algorithms, organizations can train databases to recognize patterns or anomalies in new data to organize spam and non-spam-related correspondences effectively. 

Forecasting: Regressive models excel at forecasting based on historical trends, making them suitable for use in the financial industries. Enterprises can also use regression to predict inventory needs, estimate employee salaries and avoid potential supply chain hiccups. 

Recommendation engines: With supervised learning models in play, content providers and online marketplaces can analyze customer choices, preferences and purchases and build recommendation engines that offer tailored recommendations more likely to convert.

Challenges of supervised learning
Although supervised learning can offer businesses advantages such as deep data insights and improved automation, it might not be the best choice for all situations. 

Personnel limitations: Supervised learning models can require certain levels of expertise to structure accurately. 

Human involvement: Supervised learning models are incapable of self-learning. Data scientists must validate the models’ performance output. 

Time requirements: Training datasets are large and must be manually labeled, which makes the supervised learning process time-intensive. 

Inflexibility: Supervised learning models struggle to label data outside the bounds of their training datasets. An unsupervised learning model might be more capable of dealing with new data. 

Bias: Datasets risk a higher likelihood of human error and bias, resulting in algorithms learning incorrectly. 

Overfitting: Supervised learning can sometimes result in overfitting: where a model becomes too closely tailored to its training dataset. High accuracy in training can indicate overfitting as opposed to generally strong performance. Avoiding overfitting requires that models be tested with data that is different from the training data.


Regression in machine learning
Last Updated : 13 Jan, 2025
Regression in machine learning refers to a supervised learning technique where the goal is to predict a continuous numerical value based on one or more independent features. It finds relationships between variables so that predictions can be made. we have two types of variables present in regression:

Dependent Variable (Target): The variable we are trying to predict e.g house price.
Independent Variables (Features): The input variables that influence the prediction e.g locality, number of rooms.
Regression analysis problem works with if output variable is a real or continuous value such as “salary” or “weight”. Many different regression models can be used but the simplest model in them is linear regression.

Types of Regression
Regression can be classified into different types based on the number of predictor variables and the nature of the relationship between variables:

1. Simple Linear Regression
Linear regression is one of the simplest and most widely used statistical models. This assumes that there is a linear relationship between the independent and dependent variables. This means that the change in the dependent variable is proportional to the change in the independent variables. For example predicting the price of a house based on its size.

2. Multiple Linear Regression
Multiple linear regression extends simple linear regression by using multiple independent variables to predict target variable. For example predicting the price of a house based on multiple features such as size, location, number of rooms, etc.

3. Polynomial Regression
Polynomial regression is used to model with non-linear relationships between the dependent variable and the independent variables. It adds polynomial terms to the linear regression model to capture more complex relationships. For example when we want to predict a non-linear trend like population growth over time we use polynomial regression.

4. Ridge & Lasso Regression
Ridge & lasso regression are regularized versions of linear regression that help avoid overfitting by penalizing large coefficients. When there’s a risk of overfitting due to too many features we use these type of regression algorithms.

5. Support Vector Regression (SVR)
SVR is a type of regression algorithm that is based on the Support Vector Machine (SVM) algorithm. SVM is a type of algorithm that is used for classification tasks but it can also be used for regression tasks. SVR works by finding a hyperplane that minimizes the sum of the squared residuals between the predicted and actual values.

6. Decision Tree Regression
Decision tree Uses a tree-like structure to make decisions where each branch of tree represents a decision and leaves represent outcomes. For example predicting customer behavior based on features like age, income, etc there we use decison tree regression.

7. Random Forest Regression
Random Forest is a ensemble method that builds multiple decision trees and each tree is trained on a different subset of the training data. The final prediction is made by averaging the predictions of all of the trees. For example customer churn or sales data using this.

Regression Evaluation Metrics
Evaluation in machine learning measures the performance of a model. Here are some popular evaluation metrics for regression:

Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values of the target variable.
Mean Squared Error (MSE): The average squared difference between the predicted and actual values of the target variable.
Root Mean Squared Error (RMSE): Square root of the mean squared error.
Huber Loss: A hybrid loss function that transitions from MAE to MSE for larger errors, providing balance between robustness and MSE’s sensitivity to outliers.
R2 – Score: Higher values indicate better fit ranging from 0 to 1.
Regression Model Machine Learning
1. Goal of the Best-Fit Line
The goal of linear regression is to find a straight line that minimizes the error (the difference) between the observed data points and the predicted values. This line helps us predict the dependent variable for new, unseen data.

Linear Regression in Machine learning
Linear Regression
Here Y is called a dependent or target variable and X is called an independent variable also known as the predictor of Y. There are many types of functions or modules that can be used for regression. A linear function is the simplest type of function. Here, X may be a single feature or multiple features representing the problem.

2. Equation of the Best-Fit Line
For simple linear regression (with one independent variable), the best-fit line is represented by the equation

y
=
m
x
+
b
y=mx+b

Where:

y is the predicted value (dependent variable)
x is the input (independent variable)
m is the slope of the line (how much y changes when x changes)
b is the intercept (the value of y when x = 0)
The best-fit line will be the one that optimizes the values of m (slope) and b (intercept) so that the predicted y values are as close as possible to the actual data points.

3. Minimizing the Error: The Least Squares Method
To find the best-fit line, we use a method called Least Squares. The idea behind this method is to minimize the sum of squared differences between the actual values (data points) and the predicted values from the line. These differences are called residuals.

The formula for residuals is:

R
e
s
i
d
u
a
l
=
y
ᵢ
−
y
^
ᵢ
Residual=yᵢ− 
y
^
​
 ᵢ

Where:

y
ᵢ
yᵢ is the actual observed value
y
^
ᵢ
y
^
​
 ᵢ is the predicted value from the line for that 
x
ᵢ
xᵢ
The least squares method minimizes the sum of the squared residuals:

S
u
m
o
f
s
q
u
a
r
e
d
e
r
r
o
r
s
(
S
S
E
)
=
Σ
(
y
ᵢ
−
y
^
ᵢ
)
²
Sumofsquarederrors(SSE)=Σ(yᵢ− 
y
^
​
 ᵢ)²

This method ensures that the line best represents the data where the sum of the squared differences between the predicted values and actual values is as small as possible.

4. Interpretation of the Best-Fit Line
Slope (m): The slope of the best-fit line indicates how much the dependent variable (y) changes with each unit change in the independent variable (x). For example if the slope is 5, it means that for every 1-unit increase in x, the value of y increases by 5 units.
Intercept (b): The intercept represents the predicted value of y when x = 0. It’s the point where the line crosses the y-axis.
In linear regression some hypothesis are made to ensure reliability of the model's results.

 Limitations
Assumes Linearity: The method assumes the relationship between the variables is linear. If the relationship is non-linear, linear regression might not work well.
Sensitivity to Outliers: Outliers can significantly affect the slope and intercept, skewing the best-fit line.
Hypothesis function in Linear Regression
In linear regression, the hypothesis function is the equation used to make predictions about the dependent variable based on the independent variables. It represents the relationship between the input features and the target output.

For a simple case with one independent variable, the hypothesis function is:

h
(
x
)
=
β
₀
+
β
₁
x
h(x)=β₀+β₁x

Where:

h
(
x
)
(
o
r
y
^
)
h(x)(or 
y
^
​
 ) is the predicted value of the dependent variable (y).
x 
x
xis the independent variable.
β
₀
β₀ is the intercept, representing the value of y when x is 0.
β
₁
β₁ is the slope, indicating how much y changes for each unit change in x.
For multiple linear regression (with more than one independent variable), the hypothesis function expands to:

h
(
x
₁
,
x
₂
,
.
.
.
,
x
ₖ
)
=
β
₀
+
β
₁
x
₁
+
β
₂
x
₂
+
.
.
.
+
β
ₖ
x
ₖ
h(x₁,x₂,...,xₖ)=β₀+β₁x₁+β₂x₂+...+βₖxₖ

Where:

x
₁
,
x
₂
,
.
.
.
,
x
ₖ
x₁,x₂,...,xₖ are the independent variables.
β
₀
β₀ is the intercept.
β
₁
,
β
₂
,
.
.
.
,
β
ₖ
β₁,β₂,...,βₖ are the coefficients, representing the influence of each respective independent variable on the predicted output.
Assumptions of the Linear Regression
1. Linearity: The relationship between inputs (X) and the output (Y) is a straight line.

python-linear-regression-4
Linearity
2. Independence of Errors: The errors in predictions should not affect each other.

3. Constant Variance (Homoscedasticity): The errors should have equal spread across all values of the input. If the spread changes (like fans out or shrinks), it's called heteroscedasticity and it's a problem for the model.

python-linear-regression-5
Homoscedasticity
4. Normality of Errors: The errors should follow a normal (bell-shaped) distribution.

5. No Multicollinearity(for multiple regression): Input variables shouldn’t be too closely related to each other.

6. No Autocorrelation: Errors shouldn't show repeating patterns, especially in time-based data.

7. Additivity: The total effect on Y is just the sum of effects from each X, no mixing or interaction between them.'

To understand Multicollinearityin detail refer to article: Multicollinearity.

Types of Linear Regression
When there is only one independent feature it is known as Simple Linear Regression or Univariate Linear Regression and when there are more than one feature it is known as Multiple Linear Regression or Multivariate Regression.

1. Simple Linear Regression
Simple linear regression is used when we want to predict a target value (dependent variable) using only one input feature (independent variable). It assumes a straight-line relationship between the two.

Formula:
y
^
=
θ
0
+
θ
1
x
y
^
​
 =θ 
0
​
 +θ 
1
​
 x

Where:

y
^
y
^
​
 ​ is the predicted value
x
xis the input (independent variable)
θ
0
θ 
0
​
  is the intercept (value of 
y
^
​
y
^
​
 ​ when x=0)
θ
1
θ 
1
​
 ​ is the slope or coefficient (how much 
y
^
y
^
​
 ​ changes with one unit of x)
Example:
Predicting a person’s salary (y) based on their years of experience (x).

2. Multiple Linear Regression
Multiple linear regression involves more than one independent variable and one dependent variable. The equation for multiple linear regression is:

y
^
=
θ
0
+
θ
1
x
1
+
θ
2
x
2
+
⋯
+
θ
n
x
n
y
^
​
 =θ 
0
​
 +θ 
1
​
 x 
1
​
 +θ 
2
​
 x 
2
​
 +⋯+θ 
n
​
 x 
n
​
 

where:

y
^
y
^
​
 ​ is the predicted value
x
1
,
x
2
,
…
,
x
n
x 
1
​
 ,x 
2
​
 ,…,x 
n
​
  are the independent variables
θ
1
,
θ
2
,
…
,
θ
n
θ 
1
​
 ,θ 
2
​
 ,…,θ 
n
​
  are the coefficients (weights) corresponding to each predictor.
θ
0
θ 
0
​
  is the intercept.
The goal of the algorithm is to find the best Fit Line equation that can predict the values based on the independent variables.

In regression set of records are present with X and Y values and these values are used to learn a function so if you want to predict Y from an unknown X this learned function can be used. In regression we have to find the value of Y, So, a function is required that predicts continuous Y in the case of regression given X as independent features.

Use Case of Multiple Linear Regression
Multiple linear regression allows us to analyze relationship between multiple independent variables and a single dependent variable. Here are some use cases:

Real Estate Pricing: In real estate MLR is used to predict property prices based on multiple factors such as location, size, number of bedrooms, etc. This helps buyers and sellers understand market trends and set competitive prices.
Financial Forecasting: Financial analysts use MLR to predict stock prices or economic indicators based on multiple influencing factors such as interest rates, inflation rates and market trends. This enables better investment strategies and risk management24.
Agricultural Yield Prediction: Farmers can use MLR to estimate crop yields based on several variables like rainfall, temperature, soil quality and fertilizer usage. This information helps in planning agricultural practices for optimal productivity
E-commerce Sales Analysis: An e-commerce company can utilize MLR to assess how various factors such as product price, marketing promotions and seasonal trends impact sales.
Now that we have understood about linear regression, its assumption and its type now we will learn how to make a linear regression model.

Cost function for Linear Regression
As we have discussed earlier about best fit line in linear regression, its not easy to get it easily in real life cases so we need to calculate errors that affects it. These errors need to be calculated to mitigate them. The difference between the predicted value 
Y
^
     
Y
^
      and the true value Y and it is called cost function or the loss function.

In Linear Regression, the Mean Squared Error (MSE) cost function is employed, which calculates the average of the squared errors between the predicted values 
y
^
i
y
^
​
  
i
​
  and the actual values 
y
i
y 
i
​
 . The purpose is to determine the optimal values for the intercept 
θ
1
θ 
1
​
  and the coefficient of the input feature 
θ
2
θ 
2
​
  providing the best-fit line for the given data points. The linear equation expressing this relationship is 
y
^
i
=
θ
1
+
θ
2
x
i
y
^
​
  
i
​
 =θ 
1
​
 +θ 
2
​
 x 
i
​
 .

MSE function can be calculated as:

Cost function
(
J
)
=
1
n
∑
n
i
(
y
i
^
−
y
i
)
2
Cost function(J)= 
n
1
​
 ∑ 
n
i
​
 ( 
y 
i
​
 
^
​
 −y 
i
​
 ) 
2
 

Utilizing the MSE function, the iterative process of gradient descent is applied to update the values of \
θ
1
&
θ
2
θ 
1
​
 &θ 
2
​
 . This ensures that the MSE value converges to the global minima, signifying the most accurate fit of the linear regression line to the dataset.

This process involves continuously adjusting the parameters \(\theta_1\) and \(\theta_2\) based on the gradients calculated from the MSE. The final result is a linear regression line that minimizes the overall squared differences between the predicted and actual values, providing an optimal representation of the underlying relationship in the data.

Now we have calculated loss function we need to optimize model to mtigate this error and it is done through gradient descent.

Gradient Descent for Linear Regression
A linear regression model can be trained using the optimization algorithm gradient descent by iteratively modifying the model's parameters to reduce the mean squared error (MSE) of the model on a training dataset. To update θ1 and θ2 values in order to reduce the Cost function (minimizing RMSE value) and achieve the best-fit line the model uses Gradient Descent. The idea is to start with random θ1 and θ2 values and then iteratively update the values, reaching minimum cost. 

A gradient is nothing but a derivative that defines the effects on outputs of the function with a little bit of variation in inputs.

Evaluation Metrics for Linear Regression
A variety of evaluation measures can be used to determine the strength of any linear regression model. These assessment metrics often give an indication of how well the model is producing the observed outputs.

The most common measurements are:

1. Mean Square Error (MSE)
Mean Squared Error (MSE) is an evaluation metric that calculates the average of the squared differences between the actual and predicted values for all the data points. The difference is squared to ensure that negative and positive differences don't cancel each other out.

M
S
E
=
1
n
∑
i
=
1
n
(
y
i
−
y
i
^
)
2
MSE= 
n
1
​
 ∑ 
i=1
n
​
 (y 
i
​
 − 
y 
i
​
 
​
 ) 
2
 

Here,

n
nis the number of data points.
y
i
y 
i
​
 is the actual or observed value for the
i
t
h
i 
th
 data point.
y
i
^
y 
i
​
 
​
  is the predicted value for the 
i
t
h
i 
th
 data point.
MSE is a way to quantify the accuracy of a model's predictions. MSE is sensitive to outliers as large errors contribute significantly to the overall score.

2. Mean Absolute Error (MAE)
Mean Absolute Error is an evaluation metric used to calculate the accuracy of a regression model. MAE measures the average absolute difference between the predicted values and actual values.

Mathematically MAE is expressed as:

M
A
E
=
1
n
∑
i
=
1
n
∣
Y
i
−
Y
i
^
∣
MAE= 
n
1
​
 ∑ 
i=1
n
​
 ∣Y 
i
​
 − 
Y 
i
​
 
​
 ∣

Here,

n is the number of observations
Yi represents the actual values.
Y
i
^
Y 
i
​
 
​
  represents the predicted values
Lower MAE value indicates better model performance. It is not sensitive to the outliers as we consider absolute differences.

3. Root Mean Squared Error (RMSE)
The square root of the residuals' variance is the Root Mean Squared Error. It describes how well the observed data points match the expected values or the model's absolute fit to the data.
In mathematical notation, it can be expressed as:

R
M
S
E
=
R
S
S
n
=
∑
i
=
2
n
(
y
i
a
c
t
u
a
l
−
y
i
p
r
e
d
i
c
t
e
d
)
2
n
RMSE= 
n
RSS
​
 
​
 = 
n
∑ 
i=2
n
​
 (y 
i
actual
​
 −y 
i
predicted
​
 ) 
2
 
​
 
​
 

Rather than dividing the entire number of data points in the model by the number of degrees of freedom, one must divide the sum of the squared residuals to obtain an unbiased estimate. Then, this figure is referred to as the Residual Standard Error (RSE).

In mathematical notation, it can be expressed as:

R
M
S
E
=
R
S
S
n
=
∑
i
=
2
n
(
y
i
a
c
t
u
a
l
−
y
i
p
r
e
d
i
c
t
e
d
)
2
(
n
−
2
)
RMSE= 
n
RSS
​
 
​
 = 
(n−2)
∑ 
i=2
n
​
 (y 
i
actual
​
 −y 
i
predicted
​
 ) 
2
 
​
 
​
 

RSME is not as good of a metric as R-squared. Root Mean Squared Error can fluctuate when the units of the variables vary since its value is dependent on the variables' units (it is not a normalized measure).

4. Coefficient of Determination (R-squared)
R-Squared is a statistic that indicates how much variation the developed model can explain or capture. It is always in the range of 0 to 1. In general, the better the model matches the data, the greater the R-squared number.
In mathematical notation, it can be expressed as:

R
2
=
1
−
(
R
S
S
T
S
S
)
R 
2
 =1−( 
TSS
RSS
​
 
 )

Residual sum of Squares(RSS): The sum of squares of the residual for each data point in the plot or data is known as the residual sum of squares or RSS. It is a measurement of the difference between the output that was observed and what was anticipated.
R
S
S
=
∑
i
=
1
n
(
y
i
−
b
0
−
b
1
x
i
)
2
RSS=∑ 
i=1
n
​
 (y 
i
​
 −b 
0
​
 −b 
1
​
 x 
i
​
 ) 
2
 

Total Sum of Squares (TSS): The sum of the data points' errors from the answer variable's mean is known as the total sum of squares or TSS.
T
S
S
=
∑
i
=
1
n
(
y
−
y
i
‾
)
2
TSS=∑ 
i=1
n
​
 (y− 
y 
i
​
 
​
 ) 
2
 .

R squared metric is a measure of the proportion of variance in the dependent variable that is explained the independent variables in the model.

5. Adjusted R-Squared Error
Adjusted 
R
2
R 
2
 measures the proportion of variance in the dependent variable that is explained by independent variables in a regression model. Adjusted R-square accounts the number of predictors in the model and penalizes the model for including irrelevant predictors that don't contribute significantly to explain the variance in the dependent variables.

Mathematically, adjusted 
R
2
R 
2
 is expressed as:

A
d
j
u
s
t
e
d
R
2
=
1
−
(
(
1
−
R
2
)
.
(
n
−
1
)
n
−
k
−
1
)
AdjustedR 
2
 =1−( 
n−k−1
(1−R 
2
 ).(n−1)
​
 )

Here,

n is the number of observations
k is the number of predictors in the model
R2 is coeeficient of determination
Adjusted R-square helps to prevent overfitting. It penalizes the model with additional predictors that do not contribute significantly to explain the variance in the dependent variable.

While evaluation metrics help us measure the performance of a model, regularization helps in improving that performance by addressing overfitting and enhancing generalization.

Regularization Techniques for Linear Models
1. Lasso Regression (L1 Regularization)
Lasso Regression is a technique used for regularizing a linear regression model, it adds a penalty term to the linear regression objective function to prevent overfitting.

The objective function after applying lasso regression is:

J
(
θ
)
=
1
2
m
∑
i
=
1
m
(
y
i
^
−
y
i
)
2
+
λ
∑
j
=
1
n
∣
θ
j
∣
J(θ)= 
2m
1
​
 ∑ 
i=1
m
​
 ( 
y 
i
​
 
​
 −y 
i
​
 ) 
2
 +λ∑ 
j=1
n
​
 ∣θ 
j
​
 ∣

the first term is the least squares loss, representing the squared difference between predicted and actual values.
the second term is the L1 regularization term, it penalizes the sum of absolute values of the regression coefficient θj.
2. Ridge Regression (L2 Regularization)
Ridge regression is a linear regression technique that adds a regularization term to the standard linear objective. Again, the goal is to prevent overfitting by penalizing large coefficient in linear regression equation. It useful when the dataset has multicollinearity where predictor variables are highly correlated.

The objective function after applying ridge regression is:

J
(
θ
)
=
1
2
m
∑
i
=
1
m
(
y
i
^
−
y
i
)
2
+
λ
∑
j
=
1
n
θ
j
2
J(θ)= 
2m
1
​
 ∑ 
i=1
m
​
 ( 
y 
i
​
 
​
 −y 
i
​
 ) 
2
 +λ∑ 
j=1
n
​
 θ 
j
2
​
 

the first term is the least squares loss, representing the squared difference between predicted and actual values.
the second term is the L1 regularization term, it penalizes the sum of square of values of the regression coefficient θj.
3. Elastic Net Regression
Elastic Net Regression is a hybrid regularization technique that combines the power of both L1 and L2 regularization in linear regression objective.

J
(
θ
)
=
1
2
m
∑
i
=
1
m
(
y
i
^
−
y
i
)
2
+
α
λ
∑
j
=
1
n
∣
θ
j
∣
+
1
2
(
1
−
α
)
λ
∑
j
=
1
n
θ
j
2
J(θ)= 
2m
1
​
 ∑ 
i=1
m
​
 ( 
y 
i
​
 
​
 −y 
i
​
 ) 
2
 +αλ∑ 
j=1
n
​
 ∣θ 
j
​
 ∣+ 
2
1
​
 (1−α)λ∑ 
j=1
​
 nθ 
j
2
​
 

the first term is least square loss.
the second term is L1 regularization and third is ridge regression.
λ
λis the overall regularization strength.
α
αcontrols the mix between L1 and L2 regularization.
Now that we have learned how to make a linear regression model, now we will implement it.

Python Implementation of Linear Regression
1. Import the necessary libraries:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.axes as ax
from matplotlib.animation import FuncAnimation
2. Load the dataset and separate input and Target variables
Here is the link for dataset: Dataset Link


url = 'https://media.geeksforgeeks.org/wp-content/uploads/20240320114716/data_for_lr.csv'
data = pd.read_csv(url)

data = data.dropna()

train_input = np.array(data.x[0:500]).reshape(500, 1)
train_output = np.array(data.y[0:500]).reshape(500, 1)

test_input = np.array(data.x[500:700]).reshape(199, 1)
test_output = np.array(data.y[500:700]).reshape(199, 1)
3. Build the Linear Regression Model and Plot the regression line
In forward propagation Linear regression function 
Y
=
m
x
+
c
Y=mx+c is applied by initially assigning random value of parameter (m and c). The we have written the function to finding the cost function i.e the mean 


class LinearRegression: 
    def __init__(self): 
        self.parameters = {} 

    def forward_propagation(self, train_input): 
        m = self.parameters['m'] 
        c = self.parameters['c'] 
        predictions = np.multiply(m, train_input) + c 
        return predictions 

    def cost_function(self, predictions, train_output): 
        cost = np.mean((train_output - predictions) ** 2) 
        return cost 

    def backward_propagation(self, train_input, train_output, predictions): 
        derivatives = {} 
        df = (predictions-train_output) 
        dm = 2 * np.mean(np.multiply(train_input, df)) 
        dc = 2 * np.mean(df) 
        derivatives['dm'] = dm 
        derivatives['dc'] = dc 
        return derivatives 

    def update_parameters(self, derivatives, learning_rate): 
        self.parameters['m'] = self.parameters['m'] - learning_rate * derivatives['dm'] 
        self.parameters['c'] = self.parameters['c'] - learning_rate * derivatives['dc'] 

    def train(self, train_input, train_output, learning_rate, iters): 
        self.parameters['m'] = np.random.uniform(0, 1) * -1
        self.parameters['c'] = np.random.uniform(0, 1) * -1

        self.loss = [] 

        fig, ax = plt.subplots() 
        x_vals = np.linspace(min(train_input), max(train_input), 100) 
        line, = ax.plot(x_vals, self.parameters['m'] * x_vals + self.parameters['c'], color='red', label='Regression Line') 
        ax.scatter(train_input, train_output, marker='o', color='green', label='Training Data') 

        ax.set_ylim(0, max(train_output) + 1) 

        def update(frame): 
            predictions = self.forward_propagation(train_input) 
            cost = self.cost_function(predictions, train_output) 
            derivatives = self.backward_propagation(train_input, train_output, predictions) 
            self.update_parameters(derivatives, learning_rate) 
            line.set_ydata(self.parameters['m'] * x_vals + self.parameters['c']) 
            self.loss.append(cost) 
            print("Iteration = {}, Loss = {}".format(frame + 1, cost)) 
            return line, 

        ani = FuncAnimation(fig, update, frames=iters, interval=200, blit=True) 
        ani.save('linear_regression_A.gif', writer='ffmpeg') 

        plt.xlabel('Input') 
        plt.ylabel('Output') 
        plt.title('Linear Regression') 
        plt.legend() 
        plt.show() 

        return self.parameters, self.loss 
ezgif-1-ba0c9540c5
The linear regression line provides valuable insights into the relationship between the two variables. It represents the best-fitting line that captures the overall trend of how a dependent variable (Y) changes in response to variations in an independent variable (X).

Positive Linear Regression Line: A positive linear regression line indicates a direct relationship between the independent variable (
X
X) and the dependent variable (
Y
Y). This means that as the value of X increases, the value of Y also increases. The slope of a positive linear regression line is positive, meaning that the line slants upward from left to right.
Negative Linear Regression Line: A negative linear regression line indicates an inverse relationship between the independent variable (
X
X) and the dependent variable (
Y
Y). This means that as the value of X increases, the value of Y decreases. The slope of a negative linear regression line is negative, meaning that the line slants downward from left to right.
4. Trained the model and Final Prediction

linear_reg = LinearRegression()
parameters, loss = linear_reg.train(train_input, train_output, 0.0001, 20)
Output:

model-training
Model Training
Applications of Linear Regression
Linear regression is used in many different fields including finance, economics and psychology to understand and predict the behavior of a particular variable.

For example linear regression is widely used in finance to analyze relationships and make predictions. It can model how a company's earnings per share (EPS) influence its stock price. If the model shows that a $1 increase in EPS results in a $15 rise in stock price, investors gain insights into the company's valuation. Similarly, linear regression can forecast currency values by analyzing historical exchange rates and economic indicators, helping financial professionals make informed decisions and manage risks effectively.

Also read - Linear Regression - In Simple Words, with real-life Examples

Advantages and Disadvantages of Linear Regression
Advantages of Linear Regression
Linear regression is a relatively simple algorithm, making it easy to understand and implement. The coefficients of the linear regression model can be interpreted as the change in the dependent variable for a one-unit change in the independent variable, providing insights into the relationships between variables.
Linear regression is computationally efficient and can handle large datasets effectively. It can be trained quickly on large datasets, making it suitable for real-time applications.
Linear regression is relatively robust to outliers compared to other machine learning algorithms. Outliers may have a smaller impact on the overall model performance.
Linear regression often serves as a good baseline model for comparison with more complex machine learning algorithms.
Linear regression is a well-established algorithm with a rich history and is widely available in various machine learning libraries and software packages.
Disadvantages of Linear Regression
Linear regression assumes a linear relationship between the dependent and independent variables. If the relationship is not linear, the model may not perform well.
Linear regression is sensitive to multicollinearity, which occurs when there is a high correlation between independent variables. Multicollinearity can inflate the variance of the coefficients and lead to unstable model predictions.
Linear regression assumes that the features are already in a suitable form for the model. Feature engineering may be required to transform features into a format that can be effectively used by the model.
Linear regression is susceptible to both overfitting and underfitting. Overfitting occurs when the model learns the training data too well and fails to generalize to unseen data. Underfitting occurs when the model is too simple to capture the underlying relationships in the data.
Linear regression provides limited explanatory power for complex relationships between variables. More advanced machine learning techniques may be necessary for deeper insights.


Logistic Regression in Machine Learning
Last Updated : 03 Jun, 2025
Logistic Regression is a supervised machine learning algorithm used for classification problems. Unlike linear regression which predicts continuous values it predicts the probability that an input belongs to a specific class. It is used for binary classification where the output can be one of two possible categories such as Yes/No, True/False or 0/1. It uses sigmoid function to convert inputs into a probability value between 0 and 1. In this article, we will see the basics of logistic regression and its core concepts.

_what_is_logistic_regression.webp_what_is_logistic_regression.webp
Types of Logistic Regression
Logistic regression can be classified into three main types based on the nature of the dependent variable:

Binomial Logistic Regression: This type is used when the dependent variable has only two possible categories. Examples include Yes/No, Pass/Fail or 0/1. It is the most common form of logistic regression and is used for binary classification problems.
Multinomial Logistic Regression: This is used when the dependent variable has three or more possible categories that are not ordered. For example, classifying animals into categories like "cat," "dog" or "sheep." It extends the binary logistic regression to handle multiple classes.
Ordinal Logistic Regression: This type applies when the dependent variable has three or more categories with a natural order or ranking. Examples include ratings like "low," "medium" and "high." It takes the order of the categories into account when modeling.
Assumptions of Logistic Regression
Understanding the assumptions behind logistic regression is important to ensure the model is applied correctly, main assumptions are:

Independent observations: Each data point is assumed to be independent of the others means there should be no correlation or dependence between the input samples.
Binary dependent variables: It takes the assumption that the dependent variable must be binary, means it can take only two values. For more than two categories SoftMax functions are used.
Linearity relationship between independent variables and log odds: The model assumes a linear relationship between the independent variables and the log odds of the dependent variable which means the predictors affect the log odds in a linear way.
No outliers: The dataset should not contain extreme outliers as they can distort the estimation of the logistic regression coefficients.
Large sample size: It requires a sufficiently large sample size to produce reliable and stable results.
Understanding Sigmoid Function
1. The sigmoid function is a important part of logistic regression which is used to convert the raw output of the model into a probability value between 0 and 1.

2. This function takes any real number and maps it into the range 0 to 1 forming an "S" shaped curve called the sigmoid curve or logistic curve. Because probabilities must lie between 0 and 1, the sigmoid function is perfect for this purpose.

3. In logistic regression, we use a threshold value usually 0.5 to decide the class label.

If the sigmoid output is same or above the threshold, the input is classified as Class 1.
If it is below the threshold, the input is classified as Class 0.
This approach helps to transform continuous input values into meaningful class predictions.

How does Logistic Regression work?
Logistic regression model transforms the linear regression function continuous value output into categorical value output using a sigmoid function which maps any real-valued set of independent variables input into a value between 0 and 1. This function is known as the logistic function.

 

Terminologies involved in Logistic Regression
Here are some common terms involved in logistic regression:

Independent Variables: These are the input features or predictor variables used to make predictions about the dependent variable.
Dependent Variable: This is the target variable that we aim to predict. In logistic regression, the dependent variable is categorical.
Logistic Function: This function transforms the independent variables into a probability between 0 and 1 which represents the likelihood that the dependent variable is either 0 or 1.
Odds: This is the ratio of the probability of an event happening to the probability of it not happening. It differs from probability because probability is the ratio of occurrences to total possibilities.
Log-Odds (Logit): The natural logarithm of the odds. In logistic regression, the log-odds are modeled as a linear combination of the independent variables and the intercept.
Coefficient: These are the parameters estimated by the logistic regression model which shows how strongly the independent variables affect the dependent variable.
Intercept: The constant term in the logistic regression model which represents the log-odds when all independent variables are equal to zero.
Maximum Likelihood Estimation (MLE): This method is used to estimate the coefficients of the logistic regression model by maximizing the likelihood of observing the given data.
Implementation for Logistic Regression
Now, let's see the implementation of logistic regression in Python. Here we will be implementing two main types of Logistic Regression:

1. Binomial Logistic regression: 
In binomial logistic regression, the target variable can only have two possible values such as "0" or "1", "pass" or "fail". The sigmoid function is used for prediction.

We will be using sckit-learn library for this and shows how to use the breast cancer dataset to implement a Logistic Regression model for classification.


from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)

clf = LogisticRegression(max_iter=10000, random_state=0)
clf.fit(X_train, y_train)

acc = accuracy_score(y_test, clf.predict(X_test)) * 100
print(f"Logistic Regression model accuracy: {acc:.2f}%")
Output:

Logistic Regression model accuracy (in %): 96.49%

This code uses logistic regression to classify whether a sample from the breast cancer dataset is malignant or benign.

2. Multinomial Logistic Regression:
Target variable can have 3 or more possible types which are not ordered i.e types have no quantitative significance like “disease A” vs “disease B” vs “disease C”.

In this case, the softmax function is used in place of the sigmoid function. Softmax function for K classes will be:

softmax
(
z
i
)
=
e
z
i
∑
j
=
1
K
e
z
j
softmax(z 
i
​
 )= 
∑ 
j=1
K
​
 e 
z 
j
​
 
 
e 
z 
i
​
 
 
​
 

Here 
K
K represents the number of elements in the vector 
z
z and 
i
,
j
i,j iterates over all the elements in the vector.

Then the probability for class 
c
c will be:

P
(
Y
=
c
∣
X
→
=
x
)
=
e
w
c
⋅
x
+
b
c
∑
k
=
1
K
e
w
k
⋅
x
+
b
k
P(Y=c∣ 
X
 =x)= 
∑ 
k=1
K
​
 e 
w 
k
​
 ⋅x+b 
k
​
 
 
e 
w 
c
​
 ⋅x+b 
c
​
 
 
​
 

Below is an example of implementing multinomial logistic regression using the Digits dataset from scikit-learn:


from sklearn.model_selection import train_test_split
from sklearn import datasets, linear_model, metrics

digits = datasets.load_digits()

X = digits.data
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)

reg = linear_model.LogisticRegression(max_iter=10000, random_state=0)
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)

print(f"Logistic Regression model accuracy: {metrics.accuracy_score(y_test, y_pred) * 100:.2f}%")
Output:

Logistic Regression model accuracy: 96.66%

This model is used to predict one of 10 digits (0-9) based on the image features.

How to Evaluate Logistic Regression Model?
Evaluating the logistic regression model helps assess its performance and ensure it generalizes well to new, unseen data. The following metrics are commonly used:

Accuracy: Accuracy provides the proportion of correctly classified instances.
A
c
c
u
r
a
c
y
=
T
r
u
e
P
o
s
i
t
i
v
e
s
+
T
r
u
e
N
e
g
a
t
i
v
e
s
T
o
t
a
l
Accuracy= 
Total
TruePositives+TrueNegatives
​
 
Precision: Precision focuses on the accuracy of positive predictions.
P
r
e
c
i
s
i
o
n
=
T
r
u
e
P
o
s
i
t
i
v
e
s
T
r
u
e
P
o
s
i
t
i
v
e
s
+
F
a
l
s
e
P
o
s
i
t
i
v
e
s
Precision= 
TruePositives+FalsePositives
TruePositives
​
 
Recall (Sensitivity or True Positive Rate): Recall measures the proportion of correctly predicted positive instances among all actual positive instances.
R
e
c
a
l
l
=
T
r
u
e
P
o
s
i
t
i
v
e
s
T
r
u
e
P
o
s
i
t
i
v
e
s
+
F
a
l
s
e
N
e
g
a
t
i
v
e
s
Recall= 
TruePositives+FalseNegatives
TruePositives
​
 
F1 Score: F1 score is the harmonic mean of precision and recall.
F
1
S
c
o
r
e
=
2
∗
P
r
e
c
i
s
i
o
n
∗
R
e
c
a
l
l
P
r
e
c
i
s
i
o
n
+
R
e
c
a
l
l
F1Score=2∗ 
Precision+Recall
Precision∗Recall
​
 
Area Under the Receiver Operating Characteristic Curve (AUC-ROC): The ROC curve plots the true positive rate against the false positive rate at various thresholds. AUC-ROC measures the area under this curve which provides an aggregate measure of a model's performance across different classification thresholds.
Area Under the Precision-Recall Curve (AUC-PR): Similar to AUC-ROC, AUC-PR measures the area under the precision-recall curve helps in providing a summary of a model's performance across different precision-recall trade-offs.
Differences Between Linear and Logistic Regression
Logistic regression and linear regression differ in their application and output. Here's a comparison:

Linear Regression

Logistic Regression

Linear regression is used to predict the continuous dependent variable using a given set of independent variables.

Logistic regression is used to predict the categorical dependent variable using a given set of independent variables.

It is used for solving regression problem.

It is used for solving classification problems.

In this we predict the value of continuous variables

In this we predict values of categorical variables

In this we find best fit line.

In this we find S-Curve.

Least square estimation method is used for estimation of accuracy.

Maximum likelihood estimation method is used for Estimation of accuracy.

The output must be continuous value, such as price, age etc.

Output must be categorical value such as 0 or 1, Yes or no etc.

It required linear relationship between dependent and independent variables.

It not required linear relationship.

There may be collinearity between the independent variables.

There should be little to no collinearity between independent variables.


Decision Tree
Last Updated : 30 Jun, 2025
A Decision Tree helps us to make decisions by mapping out different choices and their possible outcomes. It’s used in machine learning for tasks like classification and prediction. In this article, we’ll see more about Decision Trees, their types and other core concepts.

types_of_decision_trees-.webptypes_of_decision_trees-.webp
A Decision Tree helps us make decisions by showing different options and how they are related. It has a tree-like structure that starts with one main question called the root node which represents the entire dataset. From there, the tree branches out into different possibilities based on features in the data.

Root Node: Starting point representing the whole dataset.
Branches: Lines connecting nodes showing the flow from one decision to another.
Internal Nodes: Points where decisions are made based on data features.
Leaf Nodes: End points of the tree where the final decision or prediction is made.
Decision-tree-1
Decision Tree
A Decision Tree also helps with decision-making by showing possible outcomes clearly. By looking at the "branches" we can quickly compare options and figure out the best choice.

There are mainly two types of Decision Trees based on the target variable:

Classification Trees: Used for predicting categorical outcomes like spam or not spam. These trees split the data based on features to classify data into predefined categories.
Regression Trees: Used for predicting continuous outcomes like predicting house prices. Instead of assigning categories, it provides numerical predictions based on the input features.
How Decision Trees Work?
1. Start with the Root Node: It begins with a main question at the root node which is derived from the dataset’s features.

2. Ask Yes/No Questions: From the root, the tree asks a series of yes/no questions to split the data into subsets based on specific attributes.

3. Branching Based on Answers: Each question leads to different branches:

If the answer is yes, the tree follows one path.
If the answer is no, the tree follows another path.
4. Continue Splitting: This branching continues through further decisions helps in reducing the data down step-by-step.

5. Reach the Leaf Node: The process ends when there are no more useful questions to ask leading to the leaf node where the final decision or prediction is made.

Let’s look at a simple example to understand how it works. Imagine we need to decide whether to drink coffee based on the time of day and how tired we feel. The tree first checks the time:

1. In the morning: It asks “Tired?”

If yes, the tree suggests drinking coffee.
If no, it says no coffee is needed.
2. In the afternoon: It asks again “Tired?”

If yes, it suggests drinking coffee.
If no, no coffee is needed.
Decision-Tree-2
Example
Splitting Criteria in Decision Trees
In a Decision Tree, the process of splitting data at each node is important. The splitting criteria finds the best feature to split the data on. Common splitting criteria include Gini Impurity and Entropy.

Gini Impurity: This criterion measures how "impure" a node is. The lower the Gini Impurity the better the feature splits the data into distinct categories.
Entropy: This measures the amount of uncertainty or disorder in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.
These criteria help decide which features are useful for making the best split at each decision point in the tree.

Pruning in Decision Trees
Pruning is an important technique used to prevent overfitting in Decision Trees. Overfitting occurs when a tree becomes too deep and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.
This technique reduces the complexity of the tree by removing branches that have little predictive power. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.
It is useful when a Decision Tree is too deep and starts to capture noise in the data.
Advantages of Decision Trees
Easy to Understand: Decision Trees are visual which makes it easy to follow the decision-making process.
Versatility: Can be used for both classification and regression problems.
No Need for Feature Scaling: Unlike many machine learning models, it don’t require us to scale or normalize our data.
Handles Non-linear Relationships: It capture complex, non-linear relationships between features and outcomes effectively.
Interpretability: The tree structure is easy to interpret helps in allowing users to understand the reasoning behind each decision.
Handles Missing Data: It can handle missing values by using strategies like assigning the most common value or ignoring missing data during splits.
Disadvantages of Decision Trees
Overfitting: They can overfit the training data if they are too deep which means they memorize the data instead of learning general patterns. This leads to poor performance on unseen data.
Instability: It can be unstable which means that small changes in the data may lead to significant differences in the tree structure and predictions.
Bias towards Features with Many Categories: It can become biased toward features with many distinct values which focuses too much on them and potentially missing other important features which can reduce prediction accuracy.
Difficulty in Capturing Complex Interactions: Decision Trees may struggle to capture complex interactions between features which helps in making them less effective for certain types of data.
Computationally Expensive for Large Datasets: For large datasets, building and pruning a Decision Tree can be computationally intensive, especially as the tree depth increases.
Applications of Decision Trees
Decision Trees are used across various fields due to their simplicity, interpretability and versatility lets see some key applications:

Loan Approval in Banking: Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.
Medical Diagnosis: In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.
Predicting Exam Results in Education: Educational institutions use to predict whether a student will pass or fail based on factors like attendance, study time and past grades. This helps teachers identify at-risk students and offer targeted support.
Customer Churn Prediction: Companies use Decision Trees to predict whether a customer will leave or stay based on behavior patterns, purchase history, and interactions. This allows businesses to take proactive steps to retain customers.
Fraud Detection: In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud. By analyzing past transaction data and patterns, Decision Trees can identify suspicious activities and flag them for further investigation.
A decision tree can also be used to help build automated predictive models which have applications in machine learning, data mining and statistics. By mastering Decision Trees, we can gain a deeper understanding of data and make more informed decisions across different fields.


What is a decision tree?
A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.

As you can see from the diagram below, a decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.

Model of a decision tree
As an example, let’s imagine that you were trying to assess whether or not you should go surf, you may use the following decision rules to make a choice:

Example of a decision tree
This type of flowchart structure also creates an easy to digest representation of decision-making, allowing different groups across an organization to better understand why a decision was made.

Decision tree learning employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels.

Whether or not all data points are classified as homogenous sets is largely dependent on the complexity of the decision tree. Smaller trees are more easily able to attain pure leaf nodes—i.e. data points in a single class. However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation, and it can often lead to overfitting.

As a result, decision trees have preference for small trees, which is consistent with the principle of parsimony in Occam’s Razor; that is, “entities should not be multiplied beyond necessity.” Said differently, decision trees should add complexity only if necessary, as the simplest explanation is often the best. To reduce complexity and prevent overfitting, pruning is usually employed; this is a process, which removes branches that split on features with low importance. The model’s fit can then be evaluated through the process of cross-validation.

Another way that decision trees can maintain their accuracy is by forming an ensemble via a random forest algorithm; this classifier predicts more accurate results, particularly when the individual trees are uncorrelated with each other.

Types of decision trees
Hunt’s algorithm, which was developed in the 1960s to model human learning in Psychology, forms the foundation of many popular decision tree algorithms, such as the following:

- ID3: Ross Quinlan is credited within the development of ID3, which is shorthand for “Iterative Dichotomiser 3.” This algorithm leverages entropy and information gain as metrics to evaluate candidate splits. Some of Quinlan’s research on this algorithm from 1986 can be found here.

- C4.5: This algorithm is considered a later iteration of ID3, which was also developed by Quinlan. It can use information gain or gain ratios to evaluate split points within the decision trees.

- CART: The term, CART, is an abbreviation for “classification and regression trees” and was introduced by Leo Breiman. This algorithm typically utilizes Gini impurity to identify the ideal attribute to split on. Gini impurity measures how often a randomly chosen attribute is misclassified. When evaluating using Gini impurity, a lower value is more ideal.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

How to choose the best attribute at each node
While there are multiple ways to select the best attribute at each node, two methods, information gain and Gini impurity, act as popular splitting criterion for decision tree models. They help to evaluate the quality of each test condition and how well it will be able to classify samples into a class.

Entropy and information gain
It’s difficult to explain information gain without first discussing entropy. Entropy is a concept that stems from information theory, which measures the impurity of the sample values. It is defined with by the following formula, where:

Entropy formula
S represents the data set that entropy is calculated 
c represents the classes in set, S
p(c) represents the proportion of data points that belong to class c to the number of total data points in set, S
Entropy values can fall between 0 and 1. If all samples in data set, S, belong to one class, then entropy will equal zero. If half of the samples are classified as one class and the other half are in another class, entropy will be at its highest at 1. In order to select the best feature to split on and find the optimal decision tree, the attribute with the smallest amount of entropy should be used.

Information gain represents the difference in entropy before and after a split on a given attribute. The attribute with the highest information gain will produce the best split as it’s doing the best job at classifying the training data according to its target classification. Information gain is usually represented with the following formula,

Information Gain formula
where

a represents a specific attribute or class label
Entropy(S) is the entropy of dataset, S
|Sv|/|S| represents the proportion of the values in Sv to the number of values in dataset, S.
Let’s walk through an example to solidify these concepts. Imagine that we have the following arbitrary dataset:

Table with an example of arbitrary dataset
For this dataset, the entropy is 0.94. This can be calculated by finding the proportion of days where “Play Tennis” is “Yes”, which is 9/14, and the proportion of days where “Play Tennis” is “No”, which is 5/14. Then, these values can be plugged into the entropy formula above.

Entropy (Tennis) = -(9/14) log2(9/14) – (5/14) log2 (5/14) = 0.94

We can then compute the information gain for each of the attributes individually. For example, the information gain for the attribute, “Humidity” would be the following:

Gain (Tennis, Humidity) = (0.94)-(7/14)*(0.985) – (7/14)*(0.592) = 0.151

As a recap,

- 7/14 represents the proportion of values where humidity equals “high” to the total number of humidity values. In this case, the number of values where humidity equals “high” is the same as the number of values where humidity equals “normal”.

- 0.985 is the entropy when Humidity = “high”

- 0.59 is the entropy when Humidity = “normal”

Then, repeat the calculation for information gain for each attribute in the table above, and select the attribute with the highest information gain to be the first split point in the decision tree. In this case, outlook produces the highest information gain. From there, the process is repeated for each subtree.

Gini Impurity
Gini impurity is the probability of incorrectly classifying random data point in the dataset if it were labeled based on the class distribution of the dataset. Similar to entropy, if set, S, is pure—i.e. belonging to one class) then, its impurity is zero. This is denoted by the following formula:

Gini impurity formula
Advantages and disadvantages of decision trees
While decision trees can be used in a variety of use cases, other algorithms typically outperform decision tree algorithms. That said, decision trees are particularly useful for data mining and knowledge discovery tasks. Let’s explore the key benefits and challenges of utilizing decision trees more below:

Advantages
Easy to interpret: The Boolean logic and visual representations of decision trees make them easier to understand and consume. The hierarchical nature of a decision tree also makes it easy to see which attributes are most important, which isn’t always clear with other algorithms, like neural networks.

Little to no data preparation required: Decision trees have a number of characteristics, which make it more flexible than other classifiers. It can handle various data types—i.e. discrete or continuous values, and continuous values can be converted into categorical values through the use of thresholds. Additionally, it can also handle values with missing values, which can be problematic for other classifiers, like Naïve Bayes.

More flexible: Decision trees can be leveraged for both classification and regression tasks, making it more flexible than some other algorithms. It’s also insensitive to underlying relationships between attributes; this means that if two variables are highly correlated, the algorithm will only choose one of the features to split on.
Disadvantages
Prone to overfitting: Complex decision trees tend to overfit and do not generalize well to new data. This scenario can be avoided through the processes of pre-pruning or post-pruning. Pre-pruning halts tree growth when there is insufficient data while post-pruning removes subtrees with inadequate data after tree construction.

High variance estimators: Small variations within data can produce a very different decision tree. Bagging, or the averaging of estimates, can be a method of reducing variance of decision trees. However, this approach is limited as it can lead to highly correlated predictors.

More costly: Given that decision trees take a greedy search approach during construction, they can be more expensive to train compared to other algorithms.


What is regularization?
Regularization is a set of methods for reducing overfitting in machine learning models. Typically, regularization trades a marginal decrease in training accuracy for an increase in generalizability.

Regularization encompasses a range of techniques to correct for overfitting in machine learning models. As such, regularization is a method for increasing a model’s generalizability—that is, it’s ability to produce accurate predictions on new datasets.1 Regularization provides this increased generalizability at the sake of increased training error. In other words, regularization methods typically lead to less accurate predictions on training data but more accurate predictions on test data.

Regularization differs from optimization. Essentially, the former increases model generalizability while the latter increases model training accuracy. Both are important concepts in machine learning and data science.

There are many forms of regularization. Anything in the way of a complete guide requires a much longer book-length treatment. Nevertheless, this article provides an overview of the theory necessary to understand regularization’s purpose in machine learning as well as a survey of several popular regularization techniques.

Bias-variance tradeoff
This concession of increased training error for decreased testing error is known as bias-variance tradeoff. Bias-variance tradeoff is a well-known problem in machine learning. It’s necessary to first define “bias” and “variance.” To put it briefly:

- Bias measures the average difference between predicted values and true values. As bias increases, a model predicts less accurately on a training dataset. High bias refers to high error in training.

- Variance measures the difference between predictions across various realizations of a given model. As variance increases, a model predicts less accurately on unseen data. High variance refers to high error during testing and validation.

Bias and variance thus inversely represent model accuracy on training and test sets respectively.2 Obviously, developers aim to reduce both model bias and variance. Simultaneous reduction in both is not always possible, resulting in the need for regularization. Regularization decreases model variance at the cost of increased bias.

Regression model fits
By increasing bias and decreasing variance, regularization resolves model overfitting. Overfitting occurs when error on training data decreases while error on testing data ceases decreasing or begins increasing.3 In other words, overfitting describes models with low bias and high variance. However, if regularization introduces too much bias, then a model will underfit.

Despite its name, underfitting does not denote overfitting’s opposite. Rather underfitting describes models characterized by high bias and high variance. An underfitted model produces unsatisfactorily erroneous predictions during training and testing. This often results from insufficient training data or parameters.

Regularization, however, can potentially lead to model underfitting as well. If too much bias is introduced through regularization, model variance can cease to decrease and even increase. Regularization may have this effect particularly on simple models, that is, models with few parameters. In determining the type and degree of regularization to implement, then, one must consider a model’s complexity, dataset, and so forth.4

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Types of regularization with linear models
Linear regression and logistic regression are both predictive models underpinning machine learning. Linear regression (or ordinary least squares) aims to measure and predict the impact of one or more predictors on a given output by finding the best fitting line through provided data points (that is, training data). Logistic regression aims to determine the class probabilities of by way of a binary output given a range of predictors. In other words, linear regression makes continuous quantitative predictions while logistic regression produces discrete categorical predictions.5

Of course, as the number of predictors increase in either regression model, the input-output relationship is not always straightforward and requires manipulation of the regression formula. Enter regularization. There are three main forms of regularization for regression models. Note that this list is only a brief survey. Application of these regularization techniques in either linear or logistic regression varies minutely.

- Lasso regression (or L1 regularization) is a regularization technique that penalizes high-value, correlated coefficients. It introduces a regularization term (also called, penalty term) into the model’s sum of squared errors (SSE) loss function. This penalty term is the absolute value of the sum of coefficients. Controlled in turn by the hyperparameter lambda (λ), it reduces select feature weights to zero. Lasso regression thereby removes multicollinear features from the model altogether.

- Ridge regression (or L2 regularization) is regularization technique that similarly penalizes high-value coefficients by introducing a penalty term in the SSE loss function. It differs from lasso regression however. First, the penalty term in ridge regression is the squared sum of coefficients rather than the absolute value of coefficients. Second, ridge regression does not enact feature selection. While lasso regression’s penalty term can remove features from the model by shrinking coefficient values to zero, ridge regression only shrinks feature weights toward zero but never to zero.

- Elastic net regularization essentially combines both ridge and lasso regression but inserting both the L1 and L2 penalty terms into the SSE loss function. L2 and L1 derive their penalty term value, respectively, by squaring or taking the absolute value of the sum of the feature weights. Elastic net inserts both of these penalty values into the cost function (SSE) equation. In this way, elastic net addresses multicollinearity while also enabling feature selection.6

In statistics, these methods are also dubbed “coefficient shrinkage,” as they shrink predictor coefficient values in the predictive model. In all three techniques, the strength of the penalty term is controlled by lambda, which can be calculated using various cross-validation techniques.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Types of regularization in machine learning
Dataset
Data augmentation is a regularization technique that modifies model training data. It expands the size of the training set by creating artificial data samples derived from pre-existing training data. Adding more samples to the training set, particularly of instances rare in real world data, exposes a model to a greater quantity and diversity of data from which it learns. Machine learning research has recently explored data augmentation for classifiers, particularly as a means of resolving imbalanced datasets.7 Data augmentation differs from synthetic data however. The latter involves creating new, artificial data while the former produces modified duplicates of preexisting data to diversify and enlarge the dataset.

Visualization of modification techniques for diversifying imagesets
Model training
Early stopping is perhaps the most readily implemented regularization technique. In short, it limits the number of iterations during model training. Here, a model continuously passes through the training data, stopping once there is no improvement (and perhaps even deterioration) in training and validation accuracy. The goal is to train a model until it has reached the lowest possible training error preceding a plateau or increase in validation error.8

Many machine learning Python packages provide a training command options for early stopping. In fact, in some, early stopping is a default training setting.

Graph visualization of early stopping in relation to training and validation accuracy
Neural networks
Neural networks are complex machine learning models that drive many artificial intelligence applications and services. Neural networks are composed of an input layer, one or more hidden layers, and an output layer, each layer in turn comprised of several nodes.

Dropout regularizes neural networks by randomly dropping out nodes, along with their input and output connections, from the network during training (Fig. 3). Dropout trains several variations of a fixed-sized architecture, with each variation having different randomized nodes left out of the architecture. A single neural net without dropout is used for testing, employing an approximate averaging method derived from the randomly modified training architectures. In this way, dropout approximates training large a quantity of neural networks with a multitude of diversified architectures.9

Diagram comparison of neural network and dropout network
Weight decay is another form of regularization used for deep neural networks. It reduces the sum of squared network weights by way of a regularization parameter, much like L2 regularization in linear models.10 But when employed in neural networks, this reduction has an effect similar to L1 regularization: select neuron weights decrease to zero.11 This effectively removes nodes from the network, reducing network complexity through sparsity.12

Weight decay may appear superficially similar to dropout in deep neural networks, but the two techniques differ. One primary difference is that, in dropout, the penalty value grows exponentially in the network’s depth in cases, whereas weight decay’s penalty value grows linearly. Some believe this allows dropout to more meaningfully penalize network complexity than weight decay.13

Many online articles and tutorials incorrectly conflate L2 regularization and weight decay. In fact, scholarship is inconsistent—some distinguish between L2 and weight decay,14 some equate them,15 while others are inconsistent in describing the relationship between them.16 Resolving such inconsistencies in terminology is a needed yet overlooked area for future scholarship.


Regularization in Machine Learning
Last Updated : 23 May, 2025
Regularization is an important technique in machine learning that helps to improve model accuracy by preventing overfitting which happens when a model learns the training data too well including noise and outliers and perform poor on new data. By adding a penalty for complexity it helps simpler models to perform better on new data. In this article, we will see main types of regularization i.e Lasso, Ridge and Elastic Net and see how they help to build more reliable models.

Table of Content

Types of Regularization
What are Overfitting and Underfitting?
What are Bias and Variance?
Benefits of Regularization
Types of Regularization
1. Lasso Regression
A regression model which uses the L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L). This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones.

C
o
s
t
=
1
n
∑
i
=
1
n
(
y
i
−
y
i
^
)
2
+
λ
∑
i
=
1
m
∣
w
i
∣
Cost= 
n
1
​
 ∑ 
i=1
n
​
 (y 
i
​
 − 
y 
i
​
 
^
​
 ) 
2
 +λ∑ 
i=1
m
​
 ∣w 
i
​
 ∣

where

m
m - Number of Features
n
n- Number of Examples
y
i
yi- Actual Target Value
y
^
i
y
^
​
  
i
​
  - Predicted Target Value
Lets see how to implement this using python:

X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42) : Generates a regression dataset with 100 samples, 5 features and some noise.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) : Splits the data into 80% training and 20% testing sets.
lasso = Lasso(alpha=0.1) : Creates a Lasso regression model with regularization strength alpha set to 0.1.



from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error
​
X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
​
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
​
y_pred = lasso.predict(X_test)
​
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
​
print("Coefficients:", lasso.coef_)
Output:

regularization1
Lasso Regression
The output shows the model's prediction error and the importance of features with some coefficients reduced to zero due to L1 regularization.

2. Ridge Regression
A regression model that uses the L2 regularization technique is called Ridge regression. It adds the squared magnitude of the coefficient as a penalty term to the loss function(L).

C
o
s
t
=
1
n
∑
i
=
1
n
(
y
i
−
y
i
^
)
2
+
λ
∑
i
=
1
m
w
i
2
Cost= 
n
1
​
 ∑ 
i=1
n
​
 (y 
i
​
 − 
y 
i
​
 
^
​
 ) 
2
 +λ∑ 
i=1
m
​
 w 
i
2
​
 

where,

n
n= Number of examples or data points
m
m = Number of features i.e predictor variables
y
i
y 
i
​
  = Actual target value for the 
i
t
h
ith example
y
^
i
y
^
​
  
i
​
 ​ = Predicted target value for the 
i
t
h
ith example
w
i
w 
i
​
  = Coefficients of the features
λ
λ= Regularization parameter that controls the strength of regularization
Lets see how to implement this using python:

ridge = Ridge(alpha=1.0) : Creates a Ridge regression model with regularization strength alpha set to 1.0.



from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
​
X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
​
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred = ridge.predict(X_test)
​
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
print("Coefficients:", ridge.coef_)
Output:

regualrization2
Ridge Regression
The output shows the MSE showing model performance. Lower MSE means better accuracy. The coefficients reflect the regularized feature weights.

3. Elastic Net Regression
Elastic Net Regression is a combination of both L1 as well as L2 regularization. That shows that we add the absolute norm of the weights as well as the squared measure of the weights. With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization.

C
o
s
t
=
1
n
∑
i
=
1
n
(
y
i
−
y
i
^
)
2
+
λ
(
(
1
−
α
)
∑
i
=
1
m
∣
w
i
∣
+
α
∑
i
=
1
m
w
i
2
)
Cost= 
n
1
​
 ∑ 
i=1
n
​
 (y 
i
​
 − 
y 
i
​
 
^
​
 ) 
2
 +λ((1−α)∑ 
i=1
m
​
 ∣w 
i
​
 ∣+α∑ 
i=1
m
​
 w 
i
2
​
 )

where

n
n = Number of examples (data points)
m
m = Number of features (predictor variables)
y
i
y 
i
​
 ​ = Actual target value for the 
i
t
h
ith example
y
^
i
y
^
​
  
i
​
 ​ = Predicted target value for the 
i
t
h
ith example
w
i
wi= Coefficients of the features
λ
λ= Regularization parameter that controls the strength of regularization
α = Mixing parameter where 0 ≤ 
α
α≤ 1 and 
α
α= 1 corresponds to Lasso (L1) regularization, 
α
α= 0 corresponds to Ridge (L2) regularization and Values between 0 and 1 provide a balance of both L1 and L2 regularization
Lets see how to implement this using python:

model = ElasticNet(alpha=1.0, l1_ratio=0.5) : Creates an Elastic Net model with regularization strength alpha=1.0 and L1/L2 mixing ratio 0.5.



from sklearn.linear_model import ElasticNet
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
​
X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
​
model = ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(X_train, y_train)
​
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
​
print("Mean Squared Error:", mse)
print("Coefficients:", model.coef_)
Output:

regularization3
Elastic Net Regression
The output shows MSE which measures how far off predictions are from actual values i.e lower is better and coefficients show feature importance.

Learn more about the difference between the regularization techniques here: Lasso vs Ridge vs Elastic Net

What are Overfitting and Underfitting?
Overfitting and underfitting are terms used to describe the performance of machine learning models in relation to their ability to generalize from the training data to unseen data.

overfitting_21-2

Overfitting happens when a machine learning model learns the training data too well including the noise and random details. This makes the model to perform poorly on new, unseen data because it memorizes the training data instead of understanding the general patterns.

For example, if we only study last week’s weather to predict tomorrow’s i.e our model might focus on one-time events like a sudden rainstorm which won’t help for future predictions.

Underfitting is the opposite problem which happens when the model is too simple to learn even the basic patterns in the data. An underfitted model performs poorly on both training and new data. To fix this we need to make the model more complex or add more features.

For example if we use only the average temperature of the year to predict tomorrow’s weather hence the model misses important details like seasonal changes which results in bad predictions.

What are Bias and Variance?
Bias refers to the errors which occur when we try to fit a statistical model on real-world data which does not fit perfectly well on some mathematical model. If we use a way too simplistic a model to fit the data then we are more probably face the situation of High Bias (underfitting) refers to the case when the model is unable to learn the patterns in the data at hand and perform poorly.
Variance shows the error value that occurs when we try to make predictions by using data that is not previously seen by the model. There is a situation known as high variance (overfitting) that occurs when the model learns noise that is present in the data.
Finding a proper balance between the two is also known as the Bias-Variance Tradeoff which helps us to design an accurate model.

Bias Variance tradeoff
The Bias-Variance Tradeoff refers to the balance between bias and variance which affect predictive model performance. Finding the right tradeoff is important for creating models that generalize well to new data.

The bias-variance tradeoff shows the inverse relationship between bias and variance. When one decreases, the other tends to increase and vice versa.
Finding the right balance is important. An overly simple model with high bias won't capture the underlying patterns while an overly complex model with high variance will fit the noise in the data.
ML--Bias-Vs-Variance-(1)-(1)-(1)
Benefits of Regularization
Now, let’s see various benefits of regularization which are as follows:

Prevents Overfitting: Regularization helps models focus on underlying patterns instead of memorizing noise in the training data.
Improves Interpretability: L1 (Lasso) regularization simplifies models by reducing less important feature coefficients to zero.
Enhances Performance: Prevents excessive weighting of outliers or irrelevant features helps in improving overall model accuracy.
Stabilizes Models: Reduces sensitivity to minor data changes which ensures consistency across different data subsets.
Prevents Complexity: Keeps model from becoming too complex which is important for limited or noisy data.
Handles Multicollinearity: Reduces the magnitudes of correlated coefficients helps in improving model stability.
Allows Fine-Tuning: Hyperparameters like alpha and lambda control regularization strength helps in balancing bias and variance.
Promotes Consistency: Ensures reliable performance across different datasets which reduces the risk of large performance shifts.
Mastering regularization techniques helps us to create models that balance complexity and accuracy which leads to better predictions in real-world problems.

Support Vector Machine (SVM) Algorithm
Last Updated : 28 May, 2025
Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It tries to find the best boundary known as hyperplane that separates different classes in the data. It is useful when you want to do binary classification like spam vs. not spam or cat vs. dog.

The main goal of SVM is to maximize the margin between the two classes. The larger the margin the better the model performs on new and unseen data.

Support-Vectors-Hyperplane.webpSupport-Vectors-Hyperplane.webp
Key Concepts of Support Vector Machine
Hyperplane: A decision boundary separating different classes in feature space and is represented by the equation wx + b = 0 in linear classification.
Support Vectors: The closest data points to the hyperplane, crucial for determining the hyperplane and margin in SVM.
Margin: The distance between the hyperplane and the support vectors. SVM aims to maximize this margin for better classification performance.
Kernel: A function that maps data to a higher-dimensional space enabling SVM to handle non-linearly separable data.
Hard Margin: A maximum-margin hyperplane that perfectly separates the data without misclassifications.
Soft Margin: Allows some misclassifications by introducing slack variables, balancing margin maximization and misclassification penalties when data is not perfectly separable.
C: A regularization term balancing margin maximization and misclassification penalties. A higher C value forces stricter penalty for misclassifications.
Hinge Loss: A loss function penalizing misclassified points or margin violations and is combined with regularization in SVM.
Dual Problem: Involves solving for Lagrange multipliers associated with support vectors, facilitating the kernel trick and efficient computation.
How does Support Vector Machine Algorithm Work?
The key idea behind the SVM algorithm is to find the hyperplane that best separates two classes by maximizing the margin between them. This margin is the distance from the hyperplane to the nearest data points (support vectors) on each side.

Multiple hyperplanes separating the data from two classes
Multiple hyperplanes separate the data from two classes
The best hyperplane also known as the "hard margin" is the one that maximizes the distance between the hyperplane and the nearest data points from both classes. This ensures a clear separation between the classes. So from the above figure, we choose L2 as hard margin. Let's consider a scenario like shown below:

Selecting hyperplane for data with outlier
Selecting hyperplane for data with outlier
Here, we have one blue ball in the boundary of the red ball.

How does SVM classify the data?
The blue ball in the boundary of red ones is an outlier of blue balls. The SVM algorithm has the characteristics to ignore the outlier and finds the best hyperplane that maximizes the margin. SVM is robust to outliers.

Hyperplane which is the most optimized one
Hyperplane which is the most optimized one
A soft margin allows for some misclassifications or violations of the margin to improve generalization. The SVM optimizes the following equation to balance margin maximization and penalty minimization:

Objective Function
=
(
1
margin
)
+
λ
∑
penalty 
Objective Function=( 
margin
1
​
 )+λ∑penalty 

The penalty used for violations is often hinge loss which has the following behavior:

If a data point is correctly classified and within the margin there is no penalty (loss = 0).
If a point is incorrectly classified or violates the margin the hinge loss increases proportionally to the distance of the violation.
Till now we were talking about linearly separable data that seprates group of blue balls and red balls by a straight line/linear line.

What to do if data are not linearly separable?
When data is not linearly separable i.e it can't be divided by a straight line, SVM uses a technique called kernels to map the data into a higher-dimensional space where it becomes separable. This transformation helps SVM find a decision boundary even for non-linear data.

Original 1D dataset for classification
Original 1D dataset for classification
A kernel is a function that maps data points into a higher-dimensional space without explicitly computing the coordinates in that space. This allows SVM to work efficiently with non-linear data by implicitly performing the mapping. For example consider data points that are not linearly separable. By applying a kernel function SVM transforms the data points into a higher-dimensional space where they become linearly separable.

Linear Kernel: For linear separability.
Polynomial Kernel: Maps data into a polynomial space.
Radial Basis Function (RBF) Kernel: Transforms data into a space based on distances between data points.
Mapping 1D data to 2D to become able to separate the two classes
Mapping 1D data to 2D to become able to separate the two classes
In this case the new variable y is created as a function of distance from the origin.

Mathematical Computation of SVM
Consider a binary classification problem with two classes, labeled as +1 and -1. We have a training dataset consisting of input feature vectors X and their corresponding class labels Y. The equation for the linear hyperplane can be written as:

w
T
x
+
b
=
0
w 
T
 x+b=0

Where:

w
w is the normal vector to the hyperplane (the direction perpendicular to it).
b
b is the offset or bias term representing the distance of the hyperplane from the origin along the normal vector 
w
w.
Distance from a Data Point to the Hyperplane
The distance between a data point x_i and the decision boundary can be calculated as:

d
i
=
w
T
x
i
+
b
∣
∣
w
∣
∣
d 
i
​
 = 
∣∣w∣∣
w 
T
 x 
i
​
 +b
​
 

where ||w|| represents the Euclidean norm of the weight vector w. Euclidean norm of the normal vector W

Linear SVM Classifier
Distance from a Data Point to the Hyperplane:

y
^
=
{
1
:
 
w
T
x
+
b
≥
0
0
:
  
w
T
x
+
b
 
<
0
y
^
​
 ={ 
1
0
​
  
: w 
T
 x+b≥0
:  w 
T
 x+b <0
​
 

Where 
y
^
y
^
​
  is the predicted label of a data point.

Optimization Problem for SVM
For a linearly separable dataset the goal is to find the hyperplane that maximizes the margin between the two classes while ensuring that all data points are correctly classified. This leads to the following optimization problem:

minimize
w
,
b
1
2
∥
w
∥
2
w,b
minimize
​
  
2
1
​
 ∥w∥ 
2
 

Subject to the constraint:

y
i
(
w
T
x
i
+
b
)
≥
1
f
o
r
i
=
1
,
2
,
3
,
⋯
,
m
y 
i
​
 (w 
T
 x 
i
​
 +b)≥1fori=1,2,3,⋯,m

Where:

y
i
y 
i
​
 ​ is the class label (+1 or -1) for each training instance.
x
i
x 
i
​
 ​ is the feature vector for the 
i
i-th training instance.
m
m is the total number of training instances.
The condition 
y
i
(
w
T
x
i
+
b
)
≥
1
y 
i
​
 (w 
T
 x 
i
​
 +b)≥1 ensures that each data point is correctly classified and lies outside the margin.

Soft Margin in Linear SVM Classifier
In the presence of outliers or non-separable data the SVM allows some misclassification by introducing slack variables 
ζ
i
ζ 
i
​
 ​. The optimization problem is modified as:

minimize 
w
,
b
1
2
∥
w
∥
2
+
C
∑
i
=
1
m
ζ
i
w,b
minimize 
​
  
2
1
​
 ∥w∥ 
2
 +C∑ 
i=1
m
​
 ζ 
i
​
 

Subject to the constraints:

y
i
(
w
T
x
i
+
b
)
≥
1
−
ζ
i
and
ζ
i
≥
0
for 
i
=
1
,
2
,
…
,
m
y 
i
​
 (w 
T
 x 
i
​
 +b)≥1−ζ 
i
​
 andζ 
i
​
 ≥0for i=1,2,…,m

Where:

C
C is a regularization parameter that controls the trade-off between margin maximization and penalty for misclassifications.
ζ
i
ζ 
i
​
 ​ are slack variables that represent the degree of violation of the margin by each data point.
Dual Problem for SVM
The dual problem involves maximizing the Lagrange multipliers associated with the support vectors. This transformation allows solving the SVM optimization using kernel functions for non-linear classification.

The dual objective function is given by:

maximize 
α
1
2
∑
i
=
1
m
∑
j
=
1
m
α
i
α
j
t
i
t
j
K
(
x
i
,
x
j
)
−
∑
i
=
1
m
α
i
α
maximize 
​
  
2
1
​
 ∑ 
i=1
m
​
 ∑ 
j=1
m
​
 α 
i
​
 α 
j
​
 t 
i
​
 t 
j
​
 K(x 
i
​
 ,x 
j
​
 )−∑ 
i=1
m
​
 α 
i
​
 

Where:

α
i
α 
i
​
 ​ are the Lagrange multipliers associated with the 
i
t
h
i 
th
  training sample.
t
i
t 
i
​
 ​ is the class label for the 
i
t
h
i 
th
 -th training sample.
K
(
x
i
,
x
j
)
K(x 
i
​
 ,x 
j
​
 ) is the kernel function that computes the similarity between data points 
x
i
x 
i
​
 ​ and 
x
j
x 
j
​
 ​. The kernel allows SVM to handle non-linear classification problems by mapping data into a higher-dimensional space.
The dual formulation optimizes the Lagrange multipliers 
α
i
α 
i
​
 ​ and the support vectors are those training samples where 
α
i
>
0
α 
i
​
 >0.

SVM Decision Boundary
Once the dual problem is solved, the decision boundary is given by:

w
=
∑
i
=
1
m
α
i
t
i
K
(
x
i
,
x
)
+
b
w=∑ 
i=1
m
​
 α 
i
​
 t 
i
​
 K(x 
i
​
 ,x)+b

Where 
w
w is the weight vector, 
x
x is the test data point and 
b
b is the bias term. Finally the bias term 
b
b is determined by the support vectors, which satisfy:

t
i
(
w
T
x
i
−
b
)
=
1
⇒
b
=
w
T
x
i
−
t
i
t 
i
​
 (w 
T
 x 
i
​
 −b)=1⇒b=w 
T
 x 
i
​
 −t 
i
​
 

Where 
x
i
x 
i
​
 ​ is any support vector.

This completes the mathematical framework of the Support Vector Machine algorithm which allows for both linear and non-linear classification using the dual problem and kernel trick.

Types of Support Vector Machine
Based on the nature of the decision boundary, Support Vector Machines (SVM) can be divided into two main parts:

Linear SVM: Linear SVMs use a linear decision boundary to separate the data points of different classes. When the data can be precisely linearly separated, linear SVMs are very suitable. This means that a single straight line (in 2D) or a hyperplane (in higher dimensions) can entirely divide the data points into their respective classes. A hyperplane that maximizes the margin between the classes is the decision boundary.
Non-Linear SVM: Non-Linear SVM can be used to classify data when it cannot be separated into two classes by a straight line (in the case of 2D). By using kernel functions, nonlinear SVMs can handle nonlinearly separable data. The original input data is transformed by these kernel functions into a higher-dimensional feature space where the data points can be linearly separated. A linear SVM is used to locate a nonlinear decision boundary in this modified space. 
Implementing SVM Algorithm in Python
Predict if cancer is Benign or malignant. Using historical data about patients diagnosed with cancer enables doctors to differentiate malignant cases and benign ones are given independent attributes.

Load the breast cancer dataset from sklearn.datasets
Separate input features and target variables.
Build and train the SVM classifiers using RBF kernel.
Plot the scatter plot of the input features.

# Load the important packages
from sklearn.datasets import load_breast_cancer
import matplotlib.pyplot as plt
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.svm import SVC

# Load the datasets
cancer = load_breast_cancer()
X = cancer.data[:, :2]
y = cancer.target

#Build the model
svm = SVC(kernel="rbf", gamma=0.5, C=1.0)
# Trained the model
svm.fit(X, y)

# Plot Decision Boundary
DecisionBoundaryDisplay.from_estimator(
        svm,
        X,
        response_method="predict",
        cmap=plt.cm.Spectral,
        alpha=0.8,
        xlabel=cancer.feature_names[0],
        ylabel=cancer.feature_names[1],
    )

# Scatter plot
plt.scatter(X[:, 0], X[:, 1], 
            c=y, 
            s=20, edgecolors="k")
plt.show()
Output:

Breast Cancer Classifications with SVM RBF kernel 
Breast Cancer Classifications with SVM RBF kernel
Advantages of Support Vector Machine (SVM)
High-Dimensional Performance: SVM excels in high-dimensional spaces, making it suitable for image classification and gene expression analysis.
Nonlinear Capability: Utilizing kernel functions like RBF and polynomial SVM effectively handles nonlinear relationships.
Outlier Resilience: The soft margin feature allows SVM to ignore outliers, enhancing robustness in spam detection and anomaly detection.
Binary and Multiclass Support: SVM is effective for both binary classification and multiclass classification suitable for applications in text classification.
Memory Efficiency: It focuses on support vectors making it memory efficient compared to other algorithms.
Disadvantages of Support Vector Machine (SVM)
Slow Training: SVM can be slow for large datasets, affecting performance in SVM in data mining tasks.
Parameter Tuning Difficulty: Selecting the right kernel and adjusting parameters like C requires careful tuning, impacting SVM algorithms.
Noise Sensitivity: SVM struggles with noisy datasets and overlapping classes, limiting effectiveness in real-world scenarios.
Limited Interpretability: The complexity of the hyperplane in higher dimensions makes SVM less interpretable than other models.
Feature Scaling Sensitivity: Proper feature scaling is essential, otherwise SVM models may perform poorly.


What are SVMs?
A support vector machine (SVM) is a supervised machine learning algorithm that classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space.

SVMs were developed in the 1990s by Vladimir N. Vapnik and his colleagues, and they published this work in a paper titled "Support Vector Method for Function Approximation, Regression Estimation, and Signal Processing"1 in 1995.

SVMs are commonly used within classification problems. They distinguish between two classes by finding the optimal hyperplane that maximizes the margin between the closest data points of opposite classes. The number of features in the input data determine if the hyperplane is a line in a 2-D space or a plane in a n-dimensional space. Since multiple hyperplanes can be found to differentiate classes, maximizing the margin between points enables the algorithm to find the best decision boundary between classes. This, in turn, enables it to generalize well to new data and make accurate classification predictions. The lines that are adjacent to the optimal hyperplane are known as support vectors as these vectors run through the data points that determine the maximal margin.

The SVM algorithm is widely used in machine learning as it can handle both linear and nonlinear classification tasks. However, when the data is not linearly separable, kernel functions are used to transform the data higher-dimensional space to enable linear separation. This application of kernel functions can be known as the “kernel trick”, and the choice of kernel function, such as linear kernels, polynomial kernels, radial basis function (RBF) kernels, or sigmoid kernels, depends on data characteristics and the specific use case.

Finding the max margin to generate optimal hyperplane 
Types of SVM classifiers
Linear SVMs
Linear SVMs are used with linearly separable data; this means that the data do not need to undergo any transformations to separate the data into different classes. The decision boundary and support vectors form the appearance of a street, and Professor Patrick Winston from MIT uses the analogy of "fitting the widest possible street"2 (link resides outside ibm.com) to describe this quadratic optimization problem. Mathematically, this separating hyperplane can be represented as:

wx + b = 0

where w is the weight vector, x is the input vector, and b is the bias term.

There are two approaches to calculating the margin, or the maximum distance between classes, which are hard-margin classification and soft-margin classification. If we use a hard-margin SVMs, the data points will be perfectly separated outside of the support vectors, or "off the street" to continue with Professor Hinton’s analogy. This is represented with the formula,

(wxj + b) yj ≥ a,

and then the margin is maximized, which is represented as: max ɣ= a / ||w||, where a is the margin projected onto w.

Soft-margin classification is more flexible, allowing for some misclassification through the use of slack variables (`ξ`). The hyperparameter, C, adjusts the margin; a larger C value narrows the margin for minimal misclassification while a smaller C value widens it, allowing for more misclassified data3.

Nonlinear SVMs
Much of the data in real-world scenarios are not linearly separable, and that’s where nonlinear SVMs come into play. In order to make the data linearly separable, preprocessing methods are applied to the training data to transform it into a higher-dimensional feature space. That said, higher dimensional spaces can create more complexity by increasing the risk of overfitting the data and by becoming computationally taxing. The “kernel trick” helps to reduce some of that complexity, making the computation more efficient, and it does this by replacing dot product calculations with an equivalent kernel function4.

There are a number of different kernel types that can be applied to classify data. Some popular kernel functions include:

Polynomial kernel

Radial basis function kernel (also known as a Gaussian or RBF kernel)

Sigmoid kernel

Support vector regression (SVR)
Support vector regression (SVR) is an extension of SVMs, which is applied to regression problems (i.e. the outcome is continuous). Similar to linear SVMs, SVR finds a hyperplane with the maximum margin between data points, and it is typically used for time series prediction.

SVR differs from linear regression in that you need to specify the relationship that you’re looking to understand between the independent and dependent variables. An understanding of the relationships between variables and their directions is valuable when using linear regression. This is unnecessary for SVRs as they determine these relationships on their own.

]How SVMs work
In this section, we will discuss the process of building a SVM classifier, how it compares to other supervised learning algorithms and its applications within industry today.

Building a SVM classifier
Split your data
As with other machine learning models, start by splitting your data into a training set and testing set. As an aside, this assumes that you’ve already conducted an exploratory data analysis on your data. While this is technically not necessary to build a SVM classifier, it is good practice before using any machine learning model as this will give you an understanding of any missing data or outliers.

Generate and evaluate the model
Import an SVM module from the library of your choosing, like scikit-learn (link resides outside ibm.com). Train your training samples on the classifier and predict the response. You can evaluate performance by comparing accuracy of the test set to the predicted values. You may want to use other evaluation metrics, like f1-score, precision, or recall.

Hyperparameter tuning
Hyperparameters can be tuned to improve the performance of an SVM model. Optimal hyperparameters can be found using grid search and cross-validation methods, which will iterate through different kernel, regularization (C), and gamma values to find the best combination.

SVMs vs. other supervised learning classifiers
Different machine learning classifiers can be used for the same use case. It's important to test out and evaluate different models to understand which ones perform the best. That said, it can be helpful to understand the strengths and weaknesses of each to assess its application for your use case.

SVMs vs naive bayes
Both Naive Bayes and SVM classifies are commonly used for text classification tasks. SVMs tend to perform better than Naive Bayes when the data is not linearly separable. That said, SVMs have to tune for different hyperparameters and can be more computationally expensive.

SVMs vs logistic regression
SVMs typically perform better with high-dimensional and unstructured datasets, such as image and text data, compared to logistic regression. SVMs are also less sensitive to overfitting and easier to interpret. That said, they can be more computationally expensive.

SVMs vs decision trees
SVMs perform better with high-dimensional data and are less prone to overfitting compared to decision trees. That said, decision trees are typically faster to train, particularly with smaller datasets, and they are generally easier to interpret.

SVM vs. neural networks
Similar to other model comparisons, SVMs are more computationally expensive to train and less prone to overfitting, but neural networks are considered more flexible and scalable.

Applications of SVMs
While SVMs can be applied for a number of tasks, these are some of the most popular applications of SVMs across industries.

Text classification
SVMs are commonly used in natural language processing (NLP) for tasks such as sentiment analysis, spam detection, and topic modeling. They lend themselves to these data as they perform well with high-dimensional data.

Image classification
SVMs are applied in image classification tasks such as object detection and image retrieval. It can also be useful in security domains, classifying an image as one that has been tampered with.

Bioinformatics
SVMs are also used for protein classification, gene expression analysis, and disease diagnosis. SVMs are often applied in cancer research (link resides outside ibm.com) because they can detect subtle trends in complex datasets.

Geographic information system (GIS)
SVMs can analyze layered geophysical structures underground, filtering out the 'noise' from electromagnetic data. They have also helped to predict the seismic liquefaction potential of soil, which is relevant to field of civil engineering.




Random Forest Algorithm in Machine Learning
Last Updated : 27 Jun, 2025
Random Forest is a machine learning algorithm that uses many decision trees to make better predictions. Each tree looks at different random parts of the data and their results are combined by voting for classification or averaging for regression. This helps in improving accuracy and reducing errors.

Random-forest-algorithm.webpRandom-forest-algorithm.webp
Working of Random Forest Algorithm
Create Many Decision Trees: The algorithm makes many decision trees each using a random part of the data. So every tree is a bit different.
Pick Random Features: When building each tree it doesn’t look at all the features (columns) at once. It picks a few at random to decide how to split the data. This helps the trees stay different from each other.
Each Tree Makes a Prediction: Every tree gives its own answer or prediction based on what it learned from its part of the data.
Combine the Predictions:
For classification we choose a category as the final answer is the one that most trees agree on i.e majority voting.
For regression we predict a number as the final answer is the average of all the trees predictions.
Why It Works Well: Using random data and features for each tree helps avoid overfitting and makes the overall prediction more accurate and trustworthy.
Random forest is also a ensemble learning technique which you can learn more about from: Ensemble Learning

Key Features of Random Forest
Handles Missing Data: It can work even if some data is missing so you don’t always need to fill in the gaps yourself.
Shows Feature Importance: It tells you which features (columns) are most useful for making predictions which helps you understand your data better.
Works Well with Big and Complex Data: It can handle large datasets with many features without slowing down or losing accuracy.
Used for Different Tasks: You can use it for both classification like predicting types or labels and regression like predicting numbers or amounts.
Assumptions of Random Forest
Each tree makes its own decisions: Every tree in the forest makes its own predictions without relying on others.
Random parts of the data are used: Each tree is built using random samples and features to reduce mistakes.
Enough data is needed: Sufficient data ensures the trees are different and learn unique patterns and variety.
Different predictions improve accuracy: Combining the predictions from different trees leads to a more accurate final result.
Implementing Random Forest for Classification Tasks
Here we will predict survival rate of a person in titanic.

Import libraries and load the Titanic dataset.
Remove rows with missing target values ('Survived').
Select features like class, sex, age, etc and convert 'Sex' to numbers.
Fill missing age values with the median.
Split the data into training and testing sets, then train a Random Forest model.
Predict on test data, check accuracy and print a sample prediction result.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
titanic_data = pd.read_csv(url)

titanic_data = titanic_data.dropna(subset=['Survived'])

X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]
y = titanic_data['Survived']

X.loc[:, 'Sex'] = X['Sex'].map({'female': 0, 'male': 1})

X.loc[:, 'Age'].fillna(X['Age'].median(), inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:\n", classification_rep)

sample = X_test.iloc[0:1]
prediction = rf_classifier.predict(sample)

sample_dict = sample.iloc[0].to_dict()
print(f"\nSample Passenger: {sample_dict}")
print(f"Predicted Survival: {'Survived' if prediction[0] == 1 else 'Did Not Survive'}")
Output:

clf
Random Forest for Classification Tasks
We evaluated model's performance using a classification report to see how well it predicts the outcomes and used a random sample to check model prediction.

Implementing Random Forest for Regression Tasks
We will do house price prediction here.

Load the California housing dataset and create a DataFrame with features and target.
Separate the features and the target variable.
Split the data into training and testing sets (80% train, 20% test).
Initialize and train a Random Forest Regressor using the training data.
Predict house values on test data and evaluate using MSE and R² score.
Print a sample prediction and compare it with the actual value.

import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

california_housing = fetch_california_housing()
california_data = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)
california_data['MEDV'] = california_housing.target

X = california_data.drop('MEDV', axis=1)
y = california_data['MEDV']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

rf_regressor.fit(X_train, y_train)

y_pred = rf_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

single_data = X_test.iloc[0].values.reshape(1, -1)
predicted_value = rf_regressor.predict(single_data)
print(f"Predicted Value: {predicted_value[0]:.2f}")
print(f"Actual Value: {y_test.iloc[0]:.2f}")

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared Score: {r2:.2f}")
Output:

reg
Random Forest for Regression Tasks
We evaluated the model's performance using Mean Squared Error and R-squared Score which show how accurate the predictions are and used a random sample to check model prediction.

Advantages of Random Forest
Random Forest provides very accurate predictions even with large datasets.
Random Forest can handle missing data well without compromising with accuracy.
It doesn’t require normalization or standardization on dataset.
When we combine multiple decision trees it reduces the risk of overfitting of the model.
Limitations of Random Forest
It can be computationally expensive especially with a large number of trees.

It’s harder to interpret the model compared to simpler models like decision trees.


What is random forest?
Random forest is a commonly-used machine learning algorithm, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.

Decision trees
Since the random forest model is made up of multiple decision trees, it would be helpful to start by describing the decision tree algorithm briefly. Decision trees start with a basic question, such as, “Should I surf?” From there, you can ask a series of questions to determine an answer, such as, “Is it a long period swell?” or “Is the wind blowing offshore?”. These questions make up the decision nodes in the tree, acting as a means to split the data. Each question helps an individual to arrive at a final decision, which would be denoted by the leaf node. Observations that fit the criteria will follow the “Yes” branch and those that don’t will follow the alternate path. Decision trees seek to find the best split to subset the data, and they are typically trained through the Classification and Regression Tree (CART) algorithm. Metrics, such as Gini impurity, information gain, or mean square error (MSE), can be used to evaluate the quality of the split.

This decision tree is an example of a classification problem, where the class labels are "surf" and "don't surf."

While decision trees are common supervised learning algorithms, they can be prone to problems, such as bias and overfitting. However, when multiple decision trees form an ensemble in the random forest algorithm, they predict more accurate results, particularly when the individual trees are uncorrelated with each other.

Ensemble methods
Ensemble learning methods are made up of a set of classifiers—e.g. decision trees—and their predictions are aggregated to identify the most popular result. The most well-known ensemble methods are bagging, also known as bootstrap aggregation, and boosting. In 1996, Leo Breiman introduced the bagging method; in this method, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once. After several data samples are generated, these models are then trained independently, and depending on the type of task—i.e. regression or classification—the average or majority of those predictions yield a more accurate estimate. This approach is commonly used to reduce variance within a noisy dataset.

Random forest algorithm
The random forest algorithm is an extension of the bagging method as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. Feature randomness, also known as feature bagging or “the random subspace method”, generates a random subset of features, which ensures low correlation among decision trees. This is a key difference between decision trees and random forests. While decision trees consider all the possible feature splits, random forests only select a subset of those features.

If we go back to the “should I surf?” example, the questions that I may ask to determine the prediction may not be as comprehensive as someone else’s set of questions. By accounting for all the potential variability in the data, we can reduce the risk of overfitting, bias, and overall variance, resulting in more precise predictions.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

How it works
Random forest algorithms have three main hyperparameters, which need to be set before training. These include node size, the number of trees, and the number of features sampled. From there, the random forest classifier can be used to solve for regression or classification problems.

The random forest algorithm is made up of a collection of decision trees, and each tree in the ensemble is comprised of a data sample drawn from a training set with replacement, called the bootstrap sample. Of that training sample, one-third of it is set aside as test data, known as the out-of-bag (oob) sample, which we’ll come back to later. Another instance of randomness is then injected through feature bagging, adding more diversity to the dataset and reducing the correlation among decision trees. Depending on the type of problem, the determination of the prediction will vary. For a regression task, the individual decision trees will be averaged, and for a classification task, a majority vote—i.e. the most frequent categorical variable—will yield the predicted class. Finally, the oob sample is then used for cross-validation, finalizing that prediction.

Diagram of Random Forest
Benefits and challenges of random forest
There are a number of key advantages and challenges that the random forest algorithm presents when used for classification or regression problems. Some of them include:

Key Benefits
Reduced risk of overfitting: Decision trees run the risk of overfitting as they tend to tightly fit all the samples within training data. However, when there’s a robust number of decision trees in a random forest, the classifier won’t overfit the model since the averaging of uncorrelated trees lowers the overall variance and prediction error.
Provides flexibility: Since random forest can handle both regression and classification tasks with a high degree of accuracy, it is a popular method among data scientists. Feature bagging also makes the random forest classifier an effective tool for estimating missing values as it maintains accuracy when a portion of the data is missing.
Easy to determine feature importance: Random forest makes it easy to evaluate variable importance, or contribution, to the model. There are a few ways to evaluate feature importance. Gini importance and mean decrease in impurity (MDI) are usually used to measure how much the model’s accuracy decreases when a given variable is excluded. However, permutation importance, also known as mean decrease accuracy (MDA), is another importance measure. MDA identifies the average decrease in accuracy by randomly permutating the feature values in oob samples.
Key Challenges
Time-consuming process: Since random forest algorithms can handle large data sets, they can be provide more accurate predictions, but can be slow to process data as they are computing data for each individual decision tree.
Requires more resources: Since random forests process larger data sets, they’ll require more resources to store that data.
More complex: The prediction of a single decision tree is easier to interpret when compared to a forest of them.
Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Random forest applications
The random forest algorithm has been applied across a number of industries, allowing them to make better business decisions. Some use cases include:

Finance: It is a preferred algorithm over others as it reduces time spent on data management and pre-processing tasks. It can be used to evaluate customers with high credit risk, to detect fraud, and option pricing problems.
Healthcare: The random forest algorithm has applications within computational biology, allowing doctors to tackle problems such as gene expression classification, biomarker discovery, and sequence annotation. As a result, doctors can make estimates around drug responses to specific medications.
E-commerce: It can be used for recommendation engines for cross-sell purposes.


Naive Bayes Classifiers
Naive Bayes is a classification algorithm that uses probability to predict which category a data point belongs to, assuming that all features are unrelated. This article will give you an overview as well as more advanced use and implementation of Naive Bayes in machine learning.

Key Features of Naive Bayes Classifiers
The main idea behind the Naive Bayes classifier is to use Bayes' Theorem to classify data based on the probabilities of different classes given the features of the data. It is used mostly in high-dimensional text classification

The Naive Bayes Classifier is a simple probabilistic classifier and it has very few number of parameters which are used to build the ML models that can predict at a faster speed than other classification algorithms.
It is a probabilistic classifier because it assumes that one feature in the model is independent of existence of another feature. In other words, each feature contributes to the predictions with no relation between each other.
Naïve Bayes Algorithm is used in spam filtration, Sentimental analysis, classifying articles and many more.
Why it is Called Naive Bayes?
It is named as "Naive" because it assumes the presence of one feature does not affect other features. The "Bayes" part of the name refers to its basis in Bayes’ Theorem.

Consider a fictional dataset that describes the weather conditions for playing a game of golf. Given the weather conditions, each tuple classifies the conditions as fit(“Yes”) or unfit(“No”) for playing golf. Here is a tabular representation of our dataset.

Outlook	Temperature	Humidity	Windy	Play Golf
0	Rainy	Hot	High	False	No
1	Rainy	Hot	High	True	No
2	Overcast	Hot	High	False	Yes
3	Sunny	Mild	High	False	Yes
4	Sunny	Cool	Normal	False	Yes
5	Sunny	Cool	Normal	True	No
6	Overcast	Cool	Normal	True	Yes
7	Rainy	Mild	High	False	No
8	Rainy	Cool	Normal	False	Yes
9	Sunny	Mild	Normal	False	Yes
10	Rainy	Mild	Normal	True	Yes
11	Overcast	Mild	High	True	Yes
12	Overcast	Hot	Normal	False	Yes
13	Sunny	Mild	High	True	No
The dataset is divided into two parts, namely, feature matrix and the response vector.

Feature matrix contains all the vectors(rows) of dataset in which each vector consists of the value of dependent features. In above dataset, features are ‘Outlook’, ‘Temperature’, ‘Humidity’ and ‘Windy’.
Response vector contains the value of class variable(prediction or output) for each row of feature matrix. In above dataset, the class variable name is ‘Play golf’.
Assumption of Naive Bayes
The fundamental Naive Bayes assumption is that each feature makes an:

Feature independence: This means that when we are trying to classify something, we assume that each feature (or piece of information) in the data does not affect any other feature.
Continuous features are normally distributed: If a feature is continuous, then it is assumed to be normally distributed within each class.
Discrete features have multinomial distributions: If a feature is discrete, then it is assumed to have a multinomial distribution within each class.
Features are equally important: All features are assumed to contribute equally to the prediction of the class label.
No missing data: The data should not contain any missing values.
Introduction to Bayes' Theorem
Bayes’ Theorem provides a principled way to reverse conditional probabilities. It is defined as:

P
(
y
∣
X
)
=
P
(
X
∣
y
)
⋅
P
(
y
)
P
(
X
)
P(y∣X)= 
P(X)
P(X∣y)⋅P(y)
​
 

Where:

P
(
y
∣
X
)
P(y∣X): Posterior probability, probability of class 
y
y given features 
X
X
P
(
X
∣
y
)
P(X∣y): Likelihood, probability of features 
X
X given class 
y
y
P
(
y
)
P(y): Prior probability of class 
y
y
P
(
X
)
P(X): Marginal likelihood or evidence
Naive Bayes Working
1. Terminology
Consider a classification problem (like predicting if someone plays golf based on weather). Then:

y
y is the class label (e.g. "Yes" or "No" for playing golf)
X
=
(
x
1
,
x
2
,
.
.
.
,
x
n
)
X=(x 
1
​
 ,x 
2
​
 ,...,x 
n
​
 ) is the feature vector (e.g. Outlook, Temperature, Humidity, Wind)
A sample row from the dataset:

X
=
(Rainy, Hot, High, False)
,
y
=
No
X=(Rainy, Hot, High, False),y=No

This represents:

What is the probability that someone will not play golf given that the weather is Rainy, Hot, High humidity, and No wind?

2. The Naive Assumption
The "naive" in Naive Bayes comes from the assumption that all features are independent given the class. That is:

P
(
x
1
,
x
2
,
.
.
.
,
x
n
∣
y
)
=
P
(
x
1
∣
y
)
⋅
P
(
x
2
∣
y
)
⋯
P
(
x
n
∣
y
)
P(x 
1
​
 ,x 
2
​
 ,...,x 
n
​
 ∣y)=P(x 
1
​
 ∣y)⋅P(x 
2
​
 ∣y)⋯P(x 
n
​
 ∣y)

Thus, Bayes' theorem becomes:

P
(
y
∣
x
1
,
.
.
.
,
x
n
)
=
P
(
y
)
⋅
∏
i
=
1
n
P
(
x
i
∣
y
)
P
(
x
1
)
P
(
x
2
)
.
.
.
P
(
x
n
)
P(y∣x 
1
​
 ,...,x 
n
​
 )= 
P(x 
1
​
 )P(x 
2
​
 )...P(x 
n
​
 )
P(y)⋅∏ 
i=1
n
​
 P(x 
i
​
 ∣y)
​
 

Since the denominator is constant for a given input, we can write:

P
(
y
∣
x
1
,
.
.
.
,
x
n
)
∝
P
(
y
)
⋅
∏
i
=
1
n
P
(
x
i
∣
y
)
P(y∣x 
1
​
 ,...,x 
n
​
 )∝P(y)⋅∏ 
i=1
n
​
 P(x 
i
​
 ∣y)

3. Constructing the Naive Bayes Classifier
We compute the posterior for each class 
y
y and choose the class with the highest probability:

y
^
=
arg
⁡
max
⁡
y
P
(
y
)
⋅
∏
i
=
1
n
P
(
x
i
∣
y
)
y
^
​
 =argmax 
y
​
 P(y)⋅∏ 
i=1
n
​
 P(x 
i
​
 ∣y)

This becomes our Naive Bayes classifier.


Types of Naive Bayes Model
There are three types of Naive Bayes Model :

1. Gaussian Naive Bayes
In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values as shown below:

2. Multinomial Naive Bayes
Multinomial Naive Bayesis used when features represent the frequency of terms (such as word counts) in a document. It is commonly applied in text classification, where term frequencies are important.

3. Bernoulli Naive Bayes
Bernoulli Naive Bayes deals with binary features, where each feature indicates whether a word appears or not in a document. It is suited for scenarios where the presence or absence of terms is more relevant than their frequency. Both models are widely used in document classification tasks

Advantages of Naive Bayes Classifier
Easy to implement and computationally efficient.
Effective in cases with a large number of features.
Performs well even with limited training data.
It performs well in the presence of categorical features.
For numerical features data is assumed to come from normal distributions
Disadvantages of Naive Bayes Classifier
Assumes that features are independent, which may not always hold in real-world data.
Can be influenced by irrelevant attributes.
May assign zero probability to unseen events, leading to poor generalization.
Applications of Naive Bayes Classifier
Spam Email Filtering: Classifies emails as spam or non-spam based on features.
Text Classification: Used in sentiment analysis, document categorization, and topic classification.
Medical Diagnosis: Helps in predicting the likelihood of a disease based on symptoms.
Credit Scoring: Evaluates creditworthiness of individuals for loan approval.
Weather Prediction: Classifies weather conditions based on various factors.


What are Naïve Bayes classifiers?
The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks such as text classification. They use principles of probability to perform classification tasks.

Naïve Bayes is part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category. Unlike discriminative classifiers, like logistic regression, it does not learn which features are most important to differentiate between classes.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

A brief review of Bayesian statistics
Naïve Bayes is also known as a probabilistic classifier since it is based on Bayes’ Theorem. It would be difficult to explain this algorithm without explaining the basics of Bayesian statistics. This theorem, also known as Bayes’ Rule, allows us to “invert” conditional probabilities. As a reminder, conditional probabilities represent the probability of an event given some other event has occurred, which is represented with the following formula:

Conditional probability formula 
Bayes’ Theorem is distinguished by its use of sequential events, where additional information later acquired impacts the initial probability. These probabilities are denoted as the prior probability and the posterior probability. The prior probability is the initial probability of an event before it is contextualized under a certain condition, or the marginal probability. The posterior probability is the probability of an event after observing a piece of data.

A popular example in statistics and machine learning literature (link resides outside ibm.com) to demonstrate this concept is medical testing. For instance, imagine there is an individual, named Jane, who takes a test to determine if she has diabetes. Let’s say that the overall probability having diabetes is 5%; this would be our prior probability. However, if she obtains a positive result from her test, the prior probability is updated to account for this additional information, and it then becomes our posterior probability. This example can be represented with the following equation, using Bayes’ Theorem:

Conditional probability formula for diabetes and testing example
However, since our knowledge of prior probabilities is not likely to exact given other variables, such as diet, age, family history, et cetera, we typically leverage probability distributions from random samples, simplifying the equation to P(Y|X) = P(X|Y)P(Y) / P(X)

The return to Naïve Bayes
Naïve Bayes classifiers work differently in that they operate under a couple of key assumptions, earning it the title of “naïve”. It assumes that predictors in a Naïve Bayes model are conditionally independent, or unrelated to any of the other feature in the model. It also assumes that all features contribute equally to the outcome. While these assumptions are often violated in real-world scenarios (e.g. a subsequent word in an e-mail is dependent upon the word that precedes it), it simplifies a classification problem by making it more computationally tractable. That is, only a single probability will now be required for each variable, which, in turn, makes the model computation easier. Despite this unrealistic independence assumption, the classification algorithm performs well, particularly with small sample sizes.

With that assumption in mind, we can now reexamine the parts of a Naïve Bayes classifier more closely. Similar to Bayes’ Theorem, it’ll use conditional and prior probabilities to calculate the posterior probabilities using the following formula:

Posterior probability formula 
Now, let’s imagine text classification use case to illustrate how the Naïve Bayes algorithm works. Picture an e-mail provider that is looking to improve their spam filter. The training data would consist of words from e-mails that have been classified as either “spam” or “not spam”. From there, the class conditional probabilities and the prior probabilities are calculated to yield the posterior probability. The Naïve Bayes classifier will operate by returning the class, which has the maximum posterior probability out of a group of classes (i.e. “spam” or “not spam”) for a given e-mail. This calculation is represented with the following formula:

Formula to calculate maximum posterior probability
Since each class is referring to the same piece of text, we can actually eliminate the denominator from this equation, simplifying it to:

Simplified formula to calculate maximum posterior probability 
The accuracy of the learning algorithm based on the training dataset is then evaluated based on the performance of the test dataset.

Class-conditional probabilities
To unpack this a little more, we’ll go a level deeper to the individual parts, which comprise this formula. The class-conditional probabilities are the individual likelihoods of each word in an e-mail. These are calculated by determining the frequency of each word for each category—i.e. “spam” or “not spam”, which is also known as the maximum likelihood estimation (MLE). In this example, if we were examining if the phrase, “Dear Sir”, we’d just calculate how often those words occur within all spam and non-spam e-mails. This can be represented by the formula below, where y is “Dear Sir” and x is “spam”.

Conditional probability formula for spam example
Prior probabilities
The prior probabilities are exactly what we described earlier with Bayes’ Theorem. Based on the training set, we can calculate the overall probability that an e-mail is “spam” or “not spam”. The prior probability for class label, “spam”, would be represented within the following formula:

Formula to calculate the probability of spam
The prior probability acts as a “weight” to the class-conditional probability when the two values are multiplied together, yielding the individual posterior probabilities. From there, the maximum a posteriori (MAP) estimate is calculated to assign a class label of either spam or not spam. The final equation for the Naïve Bayesian equation can be represented in the following ways:

Naïve Bayesian equation 
Alternatively, it can be represented in the log space as naïve bayes is commonly used in this form:

Alternative way to represent the Naïve Bayesian equation 
Evaluating your Naïve Bayes classifier
One way to evaluate your classifier is to plot a confusion matrix, which will plot the actual and predicted values within a matrix. Rows generally represent the actual values while columns represent the predicted values. Many guides will illustrate this figure as a 2 x 2 plot, such as the below:

Visualization on how to interpret a confusion matrix
However, if you were predicting images from zero through 9, you’d have a 10 x 10 plot. If you wanted to know the number of times that classifier “confused” images of 4s with 9s, you’d only need to check the 4th row and the 9th column.

Confusion matrix example
Types of Naïve Bayes classifiers
There isn’t just one type of Naïve Bayes classifier. The most popular types differ based on the distributions of the feature values. Some of these include:

Gaussian Naïve Bayes (GaussianNB): This is a variant of the Naïve Bayes classifier, which is used with Gaussian distributions—i.e. normal distributions—and continuous variables. This model is fitted by finding the mean and standard deviation of each class.
Multinomial Naïve Bayes (MultinomialNB): This type of Naïve Bayes classifier assumes that the features are from multinomial distributions. This variant is useful when using discrete data, such as frequency counts, and it is typically applied within natural language processing use cases, like spam classification.
Bernoulli Naïve Bayes (BernoulliNB): This is another variant of the Naïve Bayes classifier, which is used with Boolean variables—that is, variables with two values, such as True and False or 1 and 0.
All of these can be implemented through the Scikit Learn (link resides outside ibm.com) Python library (also known as sklearn).

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Advantages and disadvantages of the Naïve Bayes classifier
Advantages
Less complex: Compared to other classifiers, Naïve Bayes is considered a simpler classifier since the parameters are easier to estimate. As a result, it’s one of the first algorithms learned within data science and machine learning courses.
Scales well: Compared to logistic regression, Naïve Bayes is considered a fast and efficient classifier that is fairly accurate when the conditional independence assumption holds. It also has low storage requirements.
Can handle high-dimensional data: Use cases, such document classification, can have a high number of dimensions, which can be difficult for other classifiers to manage.
Disadvantages:
Subject to Zero frequency: Zero frequency occurs when a categorical variable does not exist within the training set. For example, imagine that we’re trying to find the maximum likelihood estimator for the word, “sir” given class “spam”, but the word, “sir” doesn’t exist in the training data. The probability in this case would zero, and since this classifier multiplies all the conditional probabilities together, this also means that posterior probability will be zero. To avoid this issue, laplace smoothing can be leveraged.
Unrealistic core assumption: While the conditional independence assumption overall performs well, the assumption does not always hold, leading to incorrect classifications.
Applications of the Naïve Bayes classifier
Along with a number of other algorithms, Naïve Bayes belongs to a family of data mining algorithms which turn large volumes of data into useful information. Some applications of Naïve Bayes include:

Spam filtering: Spam classification is one of the most popular applications of Naïve Bayes cited in literature. For a deeper read on this use case, check out this chapter from O'Reilly (link resides outside ibm.com).
Document classification: Document and text classification go hand in hand. Another popular use case of Naïve Bayes is content classification. Imagine the content categories of a News media website. All the content categories can be classified under a topic taxonomy based on the each article on the site. Federick Mosteller and David Wallace are credited with the first application of Bayesian inference within their 1963 paper (link resides outside ibm.com).
Sentiment analysis: While this is another form of text classification, sentiment analysis is commonly leveraged within marketing to better understand and quantify opinions and attitudes around specific products and brands. 
Mental state predictions: Using fMRI data, naïve bayes has been leveraged to predict different cognitive states among humans. The goal of this research (link resides outside ibm.com) was to assist in better understanding hidden cognitive states, particularly among brain injury patients.


Bagging vs Boosting in Machine Learning
Last Updated : 03 Apr, 2025
As we know, Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. Basic idea is to learn a set of classifiers (experts) and to allow them to vote. Bagging and Boosting are two types of Ensemble Learning. These two decrease the variance of a single estimate as they combine several estimates from different models. So the result may be a model with higher stability. Let's understand these two terms in a glimpse.

Bagging: It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.
Boosting: It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.
Let’s look at both of them in detail and understand the Difference between Bagging and Boosting.

Bagging
Bootstrap Aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods. Bagging is a special case of the model averaging approach. 

Description of the Technique

Suppose a set D of d tuples, at each iteration i, a training set Di of d tuples is selected via row sampling with a replacement method (i.e., there can be repetitive elements from different d tuples) from D (i.e., bootstrap). Then a classifier model Mi is learned for each training set D < i. Each classifier Mi returns its class prediction. The bagged classifier M* counts the votes and assigns the class with the most votes to X (unknown sample).

Implementation Steps of Bagging

Step 1: Multiple subsets are created from the original data set with equal tuples, selecting observations with replacement.
Step 2: A base model is created on each of these subsets.
Step 3: Each model is learned in parallel with each training set and independent of each other.
Step 4: The final predictions are determined by combining the predictions from all the models.

An illustration for the concept of bootstrap aggregating (Bagging)
Example of Bagging

The Random Forest model uses Bagging, where decision tree models with higher variance are present. It makes random feature selection to grow trees. Several random trees make a Random Forest.

To read more refer to this article: Bagging classifier

Boosting
Boosting is an ensemble modeling technique designed to create a strong classifier by combining multiple weak classifiers. The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones.

Initially, a model is built using the training data.
Subsequent models are then trained to address the mistakes of their predecessors.
boosting assigns weights to the data points in the original dataset.
Higher weights: Instances that were misclassified by the previous model receive higher weights.
Lower weights: Instances that were correctly classified receive lower weights.
Training on weighted data: The subsequent model learns from the weighted dataset, focusing its attention on harder-to-learn examples (those with higher weights).
This iterative process continues until:
The entire training dataset is accurately predicted, or
A predefined maximum number of models is reached.
Boosting Algorithms

There are several boosting algorithms. The original ones, proposed by Robert Schapire and Yoav Freund were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize. AdaBoost was the first really successful boosting algorithm developed for the purpose of binary classification. AdaBoost is short for Adaptive Boosting and is a very popular boosting technique that combines multiple “weak classifiers” into a single “strong classifier”.

Algorithm:

Initialise the dataset and assign equal weight to each of the data point.
Provide this as input to the model and identify the wrongly classified data points.
Increase the weight of the wrongly classified data points and decrease the weights of correctly classified data points. And then normalize the weights of all data points.
if (got required results)
  Goto step 5
else
  Goto step 2
End

An illustration presenting the intuition behind the boosting algorithm, consisting of the parallel learners and weighted dataset.
To read more refer to this article: Boosting and AdaBoost in ML

Similarities Between Bagging and Boosting
Bagging and Boosting, both being the commonly used methods, have a universal similarity of being classified as ensemble methods. Here we will explain the similarities between them.

Both are ensemble methods to get N learners from 1 learner.
Both generate several training data sets by random sampling.
In Bagging, the final decision is made by averaging predictions or majority voting. In Boosting, new models focus on correcting previous errors, and the final decision is made by a weighted majority vote.
Both are good at reducing variance and provide higher stability.
Differences Between Bagging and Boosting
S.NO

Bagging

Boosting

1.	The simplest way of combining predictions that 
belong to the same type.	A way of combining predictions that 
belong to the different types.
2.	Aim to decrease variance, not bias.	Aim to decrease bias, not variance.
3.	Each model receives equal weight.	Models are weighted according to their performance.
4.	Each model is built independently.	New models are influenced 
by the performance of previously built models.
5.	Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset.	Iteratively train models, with each new model focusing on correcting the errors (misclassifications or high residuals) of the previous models
6.	Bagging tries to solve the over-fitting problem.	Boosting tries to reduce bias.
7.	If the classifier is unstable (high variance), then apply bagging.	If the classifier is stable and simple (high bias) the apply boosting.
8. 	In this base classifiers are trained parallelly.	In this base classifiers are trained sequentially.
9	Example: The Random forest model uses Bagging.	Example: The AdaBoost uses Boosting techniques

Ensemble Learning
Last Updated : 30 May, 2025
Ensemble learning is a method where we use many small models instead of just one. Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer.

Ensemble-learning.webpEnsemble-learning.webp
Types of Ensembles Learning in Machine Learning
There are three main types of ensemble methods:

Bagging (Bootstrap Aggregating):
Models are trained independently on different random subsets of the training data. Their results are then combined—usually by averaging (for regression) or voting (for classification). This helps reduce variance and prevents overfitting.
Boosting:
Models are trained one after another. Each new model focuses on fixing the errors made by the previous ones. The final prediction is a weighted combination of all models, which helps reduce bias and improve accuracy.
Stacking (Stacked Generalization):
Multiple different models (often of different types) are trained, and their predictions are used as inputs to a final model, called a meta-model. The meta-model learns how to best combine the predictions of the base models, aiming for better performance than any individual model.
1. Bagging Algorithm
Bagging classifier can be used for both regression and classification tasks. Here is an overview of Bagging classifier algorithm:

Bootstrap Sampling: Divides the original training data into ‘N’ subsets and randomly selects a subset with replacement in some rows from other subsets. This step ensures that the base models are trained on diverse subsets of the data and there is no class imbalance.
Base Model Training: For each bootstrapped sample we train a base model independently on that subset of data. These weak models are trained in parallel to increase computational efficiency and reduce time consumption. We can use different base learners i.e. different ML models as base learners to bring variety and robustness.
Prediction Aggregation: To make a prediction on testing data combine the predictions of all base models. For classification tasks it can include majority voting or weighted majority while for regression it involves averaging the predictions.
Out-of-Bag (OOB) Evaluation: Some samples are excluded from the training subset of particular base models during the bootstrapping method. These “out-of-bag” samples can be used to estimate the model’s performance without the need for cross-validation.
Final Prediction: After aggregating the predictions from all the base models, Bagging produces a final prediction for each instance.
Python pseudo code for Bagging Estimator implementing libraries:
1. Importing Libraries and Loading Data
BaggingClassifier: for creating an ensemble of classifiers trained on different subsets of data.
DecisionTreeClassifier: the base classifier used in the bagging ensemble.
load_iris: to load the Iris dataset for classification.
train_test_split: to split the dataset into training and testing subsets.
accuracy_score: to evaluate the model’s prediction accuracy.



from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
2. Loading and Splitting the Iris Dataset
data = load_iris(): loads the Iris dataset, which includes features and target labels.
X = data.data: extracts the feature matrix (input variables).
y = data.target: extracts the target vector (class labels).
train_test_split(...): splits the data into training (80%) and testing (20%) sets, with random_state=42 to ensure reproducibility.



data = load_iris()
X = data.data
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
3. Creating a Base Classifier
Decision tree is chosen as the base model. They are prone to overfitting when trained on small datasets making them good candidates for bagging.

base_classifier = DecisionTreeClassifier(): initializes a Decision Tree classifier, which will serve as the base estimator in the Bagging ensemble.



base_classifier = DecisionTreeClassifier()
4. Creating and Training the Bagging Classifier
A BaggingClassifier is created using the decision tree as the base classifier.
n_estimators = 10 specifies that 10 decision trees will be trained on different bootstrapped subsets of the training data.



bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
bagging_classifier.fit(X_train, y_train)
5. Making Predictions and Evaluating Accuracy
The trained bagging model predicts labels for test data.
The accuracy of the predictions is calculated by comparing the predicted labels (y_pred) to the actual labels (y_test).



y_pred = bagging_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
Output:

Accuracy: 1.0

2. Boosting Algorithm
Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. Weak models are trained in series such that each next model tries to correct errors of the previous model until the entire training dataset is predicted correctly. One of the most well-known boosting algorithms is AdaBoost (Adaptive Boosting). Here is an overview of Boosting algorithm:

Initialize Model Weights: Begin with a single weak learner and assign equal weights to all training examples.
Train Weak Learner: Train weak learners on these dataset.
Sequential Learning: Boosting works by training models sequentially where each model focuses on correcting the errors of its predecessor. Boosting typically uses a single type of weak learner like decision trees.
Weight Adjustment: Boosting assigns weights to training datapoints. Misclassified examples receive higher weights in the next iteration so that next models pay more attention to them.
Python pseudo code for boosting Estimator implementing libraries:
1. Importing Libraries and Modules
AdaBoostClassifier from sklearn.ensemble: for building the AdaBoost ensemble model.
DecisionTreeClassifier from sklearn.tree: as the base weak learner for AdaBoost.
load_iris from sklearn.datasets: to load the Iris dataset.
train_test_split from sklearn.model_selection: to split the dataset into training and testing sets.
accuracy_score from sklearn.metrics: to evaluate the model’s accuracy.



from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
2. Loading and Splitting the Dataset
data = load_iris(): loads the Iris dataset, which includes features and target labels.
X = data.data: extracts the feature matrix (input variables).
y = data.target: extracts the target vector (class labels).
train_test_split(...): splits the data into training (80%) and testing (20%) sets, with random_state=42 to ensure reproducibility.



data = load_iris()
X = data.data
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
3. Defining the Weak Learner
We are creating the base classifier as a decision tree with maximum depth 1 (a decision stump). This simple tree will act as a weak learner for the AdaBoost algorithm, which iteratively improves by combining many such weak learners.




base_classifier = DecisionTreeClassifier(max_depth=1)
4. Creating and Training the AdaBoost Classifier
base_classifier: The weak learner used in boosting.
n_estimators = 50: Number of weak learners to train sequentially.
learning_rate = 1.0: Controls the contribution of each weak learner to the final model.
random_state = 42: Ensures reproducibility.



adaboost_classifier = AdaBoostClassifier(
    base_classifier, n_estimators=50, learning_rate=1.0, random_state=42
)
adaboost_classifier.fit(X_train, y_train)
5. Making Predictions and Calculating Accuracy
We are calculating the accuracy of the model by comparing the true labels y_test with the predicted labels y_pred. The accuracy_score function returns the proportion of correctly predicted samples. Then, we print the accuracy value.




accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
Output:

Accuracy: 1.0

Benefits of Ensemble Learning in Machine Learning
Ensemble learning is a versatile approach that can be applied to machine learning model for: -

Reduction in Overfitting: By aggregating predictions of multiple model's ensembles can reduce overfitting that individual complex models might exhibit.
Improved Generalization: It generalizes better to unseen data by minimizing variance and bias.
Increased Accuracy: Combining multiple models gives higher predictive accuracy.
Robustness to Noise: It mitigates the effect of noisy or incorrect data points by averaging out predictions from diverse models.
Flexibility: It can work with diverse models including decision trees, neural networks and support vector machines making them highly adaptable.
Bias-Variance Tradeoff: Techniques like bagging reduce variance, while boosting reduces bias leading to better overall performance.
There are various ensemble learning techniques we can use as each one of them has their own pros and cons.

Ensemble Learning Techniques
Technique

Category

Description

Random Forest

Bagging

Random forest constructs multiple decision trees on bootstrapped subsets of the data and aggregates their predictions for final output, reducing overfitting and variance.

Random Subspace Method

Bagging

Trains models on random subsets of input features to enhance diversity and improve generalization while reducing overfitting.

Gradient Boosting Machines (GBM)

Boosting

Gradient Boosting Machines sequentially builds decision trees, with each tree correcting errors of the previous ones, enhancing predictive accuracy iteratively.

Extreme Gradient Boosting (XGBoost)

Boosting

XGBoost do optimizations like tree pruning, regularization, and parallel processing for robust and efficient predictive models.

AdaBoost (Adaptive Boosting)

Boosting

AdaBoost focuses on challenging examples by assigning weights to data points. Combines weak classifiers with weighted voting for final predictions.

CatBoost

Boosting

CatBoost specialize in handling categorical features natively without extensive preprocessing with high predictive accuracy and automatic overfitting handling.

What is Unsupervised Learning?
Last Updated : 15 Jan, 2025
Unsupervised learning is a branch of machine learning that deals with unlabeled data. Unlike supervised learning, where the data is labeled with a specific category or outcome, unsupervised learning algorithms are tasked with finding patterns and relationships within the data without any prior knowledge of the data's meaning. Unsupervised machine learning algorithms find hidden patterns and data without any human intervention, i.e., we don't give output to our model. The training model has only input parameter values and discovers the groups or patterns on its own.

Unsupervised-learning
Unsupervised Learning
The image shows set of animals: elephants, camels, and cows that represents raw data that the unsupervised learning algorithm will process.

The "Interpretation" stage signifies that the algorithm doesn't have predefined labels or categories for the data. It needs to figure out how to group or organize the data based on inherent patterns.
Algorithm represents the core of unsupervised learning process using techniques like clustering, dimensionality reduction, or anomaly detection to identify patterns and structures in the data.
Processing stage shows the algorithm working on the data.
The output shows the results of the unsupervised learning process. In this case, the algorithm might have grouped the animals into clusters based on their species (elephants, camels, cows).

How does unsupervised learning work?
Unsupervised learning works by analyzing unlabeled data to identify patterns and relationships. The data is not labeled with any predefined categories or outcomes, so the algorithm must find these patterns and relationships on its own. This can be a challenging task, but it can also be very rewarding, as it can reveal insights into the data that would not be apparent from a labeled dataset.

Data-set in Figure A is Mall data that contains information about its clients that subscribe to them. Once subscribed they are provided a membership card and the mall has complete information about the customer and his/her every purchase. Now using this data and unsupervised learning techniques, the mall can easily group clients based on the parameters we are feeding in. 



The input to the unsupervised learning models is as follows: 

Unstructured data: May contain noisy(meaningless) data, missing values, or unknown data
Unlabeled data: Data only contains a value for input parameters, there is no targeted value(output). It is easy to collect as compared to the labeled one in the Supervised approach.
Unsupervised Learning Algorithms
There are mainly 3 types of Algorithms which are used for Unsupervised dataset.

Clustering
Association Rule Learning
Dimensionality Reduction
1. Clustering Algorithms
Clustering in unsupervised machine learning is the process of grouping unlabeled data into clusters based on their similarities. The goal of clustering is to identify patterns and relationships in the data without any prior knowledge of the data's meaning.

Broadly this technique is applied to group data based on different patterns, such as similarities or differences, our machine model finds. These algorithms are used to process raw, unclassified data objects into groups. For example, in the above figure, we have not given output parameter values, so this technique will be used to group clients based on the input parameters provided by our data.

Some common clustering algorithms: 

K-means Clustering: Groups data into K clusters based on how close the points are to each other.
Hierarchical Clustering: Creates clusters by building a tree step-by-step, either merging or splitting groups.
Density-Based Clustering (DBSCAN): Finds clusters in dense areas and treats scattered points as noise.
Mean-Shift Clustering: Discovers clusters by moving points toward the most crowded areas.
Spectral Clustering: Groups data by analyzing connections between points using graphs.
2. Association Rule Learning
Association rule learning is also known as association rule mining is a common technique used to discover associations in unsupervised machine learning. This technique is a rule-based ML technique that finds out some very useful relations between parameters of a large data set. This technique is basically used for market basket analysis that helps to better understand the relationship between different products.

For e.g. shopping stores use algorithms based on this technique to find out the relationship between the sale of one product w.r.t to another's sales based on customer behavior. Like if a customer buys milk, then he may also buy bread, eggs, or butter. Once trained well, such models can be used to increase their sales by planning different offers.

Some common Association Rule Learning algorithms: 

Apriori Algorithm: Finds patterns by exploring frequent item combinations step-by-step.
FP-Growth Algorithm: An Efficient Alternative to Apriori. It quickly identifies frequent patterns without generating candidate sets.
Eclat Algorithm: Uses intersections of itemsets to efficiently find frequent patterns.
Efficient Tree-based Algorithms: Scales to handle large datasets by organizing data in tree structures.
3. Dimensionality Reduction
Dimensionality reduction is the process of reducing the number of features in a dataset while preserving as much information as possible. This technique is useful for improving the performance of machine learning algorithms and for data visualization.

Imagine a dataset of 100 features about students (height, weight, grades, etc.). To focus on key traits, you reduce it to just 2 features: height and grades, making it easier to visualize or analyze the data.

Here are some popular Dimensionality Reduction algorithms:

Principal Component Analysis (PCA): Reduces dimensions by transforming data into uncorrelated principal components.
Linear Discriminant Analysis (LDA): Reduces dimensions while maximizing class separability for classification tasks.
Non-negative Matrix Factorization (NMF): Breaks data into non-negative parts to simplify representation.
Locally Linear Embedding (LLE): Reduces dimensions while preserving the relationships between nearby points.
Isomap: Captures global data structure by preserving distances along a manifold.
Challenges of Unsupervised Learning
Here are the key challenges of unsupervised learning:

Noisy Data: Outliers and noise can distort patterns and reduce the effectiveness of algorithms.
Assumption Dependence: Algorithms often rely on assumptions (e.g., cluster shapes), which may not match the actual data structure.
Overfitting Risk: Overfitting can occur when models capture noise instead of meaningful patterns in the data.
Limited Guidance: The absence of labels restricts the ability to guide the algorithm toward specific outcomes.
Cluster Interpretability: Results, such as clusters, may lack clear meaning or alignment with real-world categories.
Sensitivity to Parameters: Many algorithms require careful tuning of hyperparameters, such as the number of clusters in k-means.
Lack of Ground Truth: Unsupervised learning lacks labeled data, making it difficult to evaluate the accuracy of results.
Applications of Unsupervised learning
Unsupervised learning has diverse applications across industries and domains. Key applications include:

Customer Segmentation: Algorithms cluster customers based on purchasing behavior or demographics, enabling targeted marketing strategies.
Anomaly Detection: Identifies unusual patterns in data, aiding fraud detection, cybersecurity, and equipment failure prevention.
Recommendation Systems: Suggests products, movies, or music by analyzing user behavior and preferences.
Image and Text Clustering: Groups similar images or documents for tasks like organization, classification, or content recommendation.
Social Network Analysis: Detects communities or trends in user interactions on social media platforms.
Astronomy and Climate Science: Classifies galaxies or groups weather patterns to support scientific research

What is unsupervised learning?
Unsupervised learning, also known as unsupervised machine learning, uses machine learning (ML) algorithms to analyze and cluster unlabeled data sets. These algorithms discover hidden patterns or data groupings without the need for human intervention.

Unsupervised learning's ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation and image recognition.

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Common unsupervised learning approaches
Unsupervised learning models are utilized for three main tasks—clustering, association, and dimensionality reduction. Below we’ll define each learning method and highlight common algorithms and approaches to conduct them effectively.

Clustering
Clustering is a data mining technique which groups unlabeled data based on their similarities or differences. Clustering algorithms are used to process raw, unclassified data objects into groups represented by structures or patterns in the information. Clustering algorithms can be categorized into a few types, specifically exclusive, overlapping, hierarchical, and probabilistic.

Exclusive and Overlapping Clustering
Exclusive clustering is a form of grouping that stipulates a data point can exist only in one cluster. This can also be referred to as “hard” clustering. K-means clustering is a common example of an exclusive clustering method where data points are assigned into K groups, where K represents the number of clusters based on the distance from each group’s centroid. The data points closest to a given centroid will be clustered under the same category. A larger K value will be indicative of smaller groupings with more granularity whereas a smaller K value will have larger groupings and less granularity. K-means clustering is commonly used in market segmentation, document clustering, image segmentation, and image compression.

Overlapping clusters differs from exclusive clustering in that it allows data points to belong to multiple clusters with separate degrees of membership. “Soft” or fuzzy k-means clustering is an example of overlapping clustering.

Hierarchical clustering
Hierarchical clustering, also known as hierarchical cluster analysis (HCA), is an unsupervised clustering algorithm that can be categorized in two ways: agglomerative or divisive.

Agglomerative clustering is considered a “bottoms-up approach.” Its data points are isolated as separate groupings initially, and then they are merged together iteratively on the basis of similarity until one cluster has been achieved. Four different methods are commonly used to measure similarity:

Ward’s linkage: This method states that the distance between two clusters is defined by the increase in the sum of squared after the clusters are merged.

Average linkage: This method is defined by the mean distance between two points in each cluster.

Complete (or maximum) linkage: This method is defined by the maximum distance between two points in each cluster.

Single (or minimum) linkage: This method is defined by the minimum distance between two points in each cluster.
Euclidean distance is the most common metric used to calculate these distances; however, other metrics, such as Manhattan distance, are also cited in clustering literature.

Divisive clustering can be defined as the opposite of agglomerative clustering; instead it takes a “top-down” approach. In this case, a single data cluster is divided based on the differences between data points. Divisive clustering is not commonly used, but it is still worth noting in the context of hierarchical clustering. These clustering processes are usually visualized using a dendrogram, a tree-like diagram that documents the merging or splitting of data points at each iteration.

A dendogram or tree-like diagram
Probabilistic clustering
A probabilistic model is an unsupervised technique that helps us solve density estimation or “soft” clustering problems. In probabilistic clustering, data points are clustered based on the likelihood that they belong to a particular distribution. The Gaussian Mixture Model (GMM) is the one of the most commonly used probabilistic clustering methods.

Gaussian Mixture Models are classified as mixture models, which means that they are made up of an unspecified number of probability distribution functions. GMMs are primarily leveraged to determine which Gaussian, or normal, probability distribution a given data point belongs to. If the mean or variance are known, then we can determine which distribution a given data point belongs to. However, in GMMs, these variables are not known, so we assume that a latent, or hidden, variable exists to cluster data points appropriately. While it is not required to use the Expectation-Maximization (EM) algorithm, it is a commonly used to estimate the assignment probabilities for a given data point to a particular data cluster.
Diagram of normal distributions within a gaussian mixture model
Association Rules
An association rule is a rule-based method for finding relationships between variables in a given dataset. These methods are frequently used for market basket analysis, allowing companies to better understand relationships between different products. Understanding consumption habits of customers enables businesses to develop better cross-selling strategies and recommendation engines. Examples of this can be seen in Amazon’s “Customers Who Bought This Item Also Bought” or Spotify’s "Discover Weekly" playlist. While there are a few different algorithms used to generate association rules, such as Apriori, Eclat, and FP-Growth, the Apriori algorithm is most widely used.

Apriori algorithms
Apriori algorithms have been popularized through market basket analyses, leading to different recommendation engines for music platforms and online retailers. They are used within transactional datasets to identify frequent itemsets, or collections of items, to identify the likelihood of consuming a product given the consumption of another product. For example, if I play Black Sabbath’s radio on Spotify, starting with their song “Orchid”, one of the other songs on this channel will likely be a Led Zeppelin song, such as “Over the Hills and Far Away.” This is based on my prior listening habits as well as the ones of others. Apriori algorithms use a hash tree to count itemsets, navigating through the dataset in a breadth-first manner.

Dimensionality reduction
While more data generally yields more accurate results, it can also impact the performance of machine learning algorithms (e.g. overfitting) and it can also make it difficult to visualize datasets. Dimensionality reduction is a technique used when the number of features, or dimensions, in a given dataset is too high. It reduces the number of data inputs to a manageable size while also preserving the integrity of the dataset as much as possible. It is commonly used in the preprocessing data stage, and there are a few different dimensionality reduction methods that can be used, such as:

Principal component analysis
Principal component analysis (PCA) is a type of dimensionality reduction algorithm which is used to reduce redundancies and to compress datasets through feature extraction. This method uses a linear transformation to create a new data representation, yielding a set of "principal components." The first principal component is the direction which maximizes the variance of the dataset. While the second principal component also finds the maximum variance in the data, it is completely uncorrelated to the first principal component, yielding a direction that is perpendicular, or orthogonal, to the first component. This process repeats based on the number of dimensions, where a next principal component is the direction orthogonal to the prior components with the most variance.

Singular value decomposition
Singular value decomposition (SVD) is another dimensionality reduction approach which factorizes a matrix, A, into three, low-rank matrices. SVD is denoted by the formula, A = USVT, where U and V are orthogonal matrices. S is a diagonal matrix, and S values are considered singular values of matrix A. Similar to PCA, it is commonly used to reduce noise and compress data, such as image files.

Autoencoders
Autoencoders leverage neural networks to compress data and then recreate a new representation of the original data’s input. Looking at the image below, you can see that the hidden layer specifically acts as a bottleneck to compress the input layer prior to reconstructing within the output layer. The stage from the input layer to the hidden layer is referred to as “encoding” while the stage from the hidden layer to the output layer is known as “decoding.”

Neural network layers diagram
Applications of unsupervised learning
Machine learning techniques have become a common method to improve a product user experience and to test systems for quality assurance. Unsupervised learning provides an exploratory path to view data, allowing businesses to identify patterns in large volumes of data more quickly when compared to manual observation. Some of the most common real-world applications of unsupervised learning are:

News Sections: Google News uses unsupervised learning to categorize articles on the same story from various online news outlets. For example, the results of a presidential election could be categorized under their label for “US” news.

Computer vision: Unsupervised learning algorithms are used for visual perception tasks, such as object recognition.

Medical imaging: Unsupervised machine learning provides essential features to medical imaging devices, such as image detection, classification and segmentation, used in radiology and pathology to diagnose patients quickly and accurately.

Anomaly detection: Unsupervised learning models can comb through large amounts of data and discover atypical data points within a dataset. These anomalies can raise awareness around faulty equipment, human error, or breaches in security.

Customer personas: Defining customer personas makes it easier to understand common traits and business clients' purchasing habits. Unsupervised learning allows businesses to build better buyer persona profiles, enabling organizations to align their product messaging more appropriately.

Recommendation Engines: Using past purchase behavior data, unsupervised learning can help to discover data trends that can be used to develop more effective cross-selling strategies. This is used to make relevant add-on recommendations to customers during the checkout process for online retailers.
Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Unsupervised vs. supervised  and semi-supervised learning
Unsupervised learning and supervised learning are frequently discussed together. Unlike unsupervised learning algorithms, supervised learning algorithms use labeled data. From that data, it either predicts future outcomes or assigns data to specific categories based on the regression or classification problem that it is trying to solve.

While supervised learning algorithms tend to be more accurate than unsupervised learning models, they require upfront human intervention to label the data appropriately. However, these labelled datasets allow supervised learning algorithms to avoid computational complexity as they don’t need a large training set to produce intended outcomes. Common regression and classification techniques are linear and logistic regression, naïve bayes, KNN algorithm, and random forest.

Semi-supervised learning occurs when only part of the given input data has been labelled. Unsupervised and semi-supervised learning can be more appealing alternatives as it can be time-consuming and costly to rely on domain expertise to label data appropriately for supervised learning.

For a deep dive into the differences between these approaches, check out "Supervised vs. Unsupervised Learning: What's the Difference?"

Challenges of unsupervised learning
While unsupervised learning has many benefits, some challenges can occur when it allows machine learning models to execute without any human intervention. Some of these challenges can include:

Computational complexity due to a high volume of training data

Longer training times

Higher risk of inaccurate results

Human intervention to validate output variables

Lack of transparency into the basis on which data was clustered

In reinforcement learning, an agent learns to make decisions by interacting with an environment. It is used in robotics and other decision-making settings.

Reinforcement learning (RL) is a type of machine learning process that focuses on decision making by autonomous agents. An autonomous agent is any system that can make decisions and act in response to its environment independent of direct instruction by a human user. Robots and self-driving cars are examples of autonomous agents. In reinforcement learning, an autonomous agent learns to perform a task by trial and error in the absence of any guidance from a human user.1 It particularly addresses sequential decision-making problems in uncertain environments, and shows promise in artificial intelligence development.

Supervised and unsupervised learning
Literature often contrasts reinforcement learning with supervised and unsupervised learning. Supervised learning uses manually labeled data to produce predictions or classifications. Unsupervised learning aims to uncover and learn hidden patterns from unlabeled data. In contrast to supervised learning, reinforcement learning does not use labeled examples of correct or incorrect behavior. But reinforcement learning also differs from unsupervised learning in that reinforcement learning learns by trial-and-error and reward function rather than by extracting information of hidden patterns.2

Supervised and unsupervised learning methods assume each record of input data is independent of other records in the dataset but that each record actualizes a common underlying data distribution model. These methods learn to predict with model performance measured according to prediction accuracy maximization.

By contrast, reinforcement learning learns to act. It assumes input data to be interdependent tuples—i.e. an ordered sequence of data—organized as state-action-reward. Many applications of reinforcement learning algorithms aim to mimic real-world biological learning methods through positive reinforcement.

Note that, although the two are not often compared in literature, reinforcement learning is distinct from self-supervised learning as well. The latter is a form of unsupervised learning that uses pseudo labels derived from unlabeled training data as a ground truth to measure model accuracy. Reinforcement learning, however, does not produce pseudo labels or measure against a ground truth—it is not a classification method but an action learner. The two have been combined however with promising results.3

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Reinforcement learning process
Reinforcement learning essentially consists of the relationship between an agent, environment, and goal. Literature widely formulates this relationship in terms of the Markov decision process (MDP).

Markov decision process
The reinforcement learning agent learns about a problem by interacting with its environment. The environment provides information on its current state. The agent then uses that information to determine which actions(s) to take. If that action obtains a reward signal from the surrounding environment, the agent is encouraged to take that action again when in a similar future state. This process repeats for every new state thereafter. Over time, the agent learns from rewards and punishments to take actions within the environment that meet a specified goal.4

Diagram for reinforcement learning topic page
In Markov decision processes, state space refers to all of the information provided by an environment’s state. Action space denotes all possible actions the agent may take within a state.5

Exploration-exploitation trade-off
Because an RL agent has no manually labeled input data guiding its behavior, it must explore its environment, attempting new actions to discover those that receive rewards. From these reward signals, the agent learns to prefer actions for which it was rewarded in order to maximize its gain. But the agent must continue exploring new states and actions as well. In doing so, it can then use that experience to improve its decision-making.

RL algorithms thus require an agent to both exploit knowledge of previously rewarded state-actions and explore other state-actions. The agent cannot exclusively pursue exploration or exploitation. It must continuously try new actions while also preferring single (or chains of) actions that produce the largest cumulative reward.6

Components of reinforcement learning
Beyond the agent-environment-goal triumvirate, four principal sub-elements characterize reinforcement learning problems.

- Policy. This defines the RL agent’s behavior by mapping perceived environmental states to specific actions the agent must take when in those states. It can take the form of a rudimentary function or more involved computational process. For instance, a policy guiding an autonomous vehicle may map pedestrian detection to a stop action.

- Reward signal. This designates the RL problem’s goal. Each of the RL agent’s actions either receives a reward from the environment or not. The agent’s only objective is to maximize its cumulative rewards from the environment. For self-driving vehicles, the reward signal can be reduced travel time, decreased collisions, remaining on the road and in the proper lane, avoiding extreme de- or accelerations, and so forth. This example shows RL may incorporate multiple reward signals to guide an agent.

- Value function. Reward signal differs from value function in that the former denotes immediate benefit while the latter specifies long-term benefit. Value refers to a state’s desirability per all of the states (with their incumbent rewards) that are likely to follow. An autonomous vehicle may be able to reduce travel time by exiting its lane, driving on the sidewalk, and accelerating quickly, but these latter three actions may reduce its overall value function. Thus, the vehicle as an RL agent may exchange marginally longer travel time to increase its reward in the latter three areas.

- Model. This is an optional sub-element of reinforcement learning systems. Models allow agents to predict environment behavior for possible actions. Agents then use model predictions to determine possible courses of action based on potential outcomes. This can be the model guiding the autonomous vehicle and that helps it predict best routes, what to expect from surrounding vehicles given their position and speed, and so forth.7 Some model-based approaches use direct human feedback in initial learning and then shift to autonomous leanring.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Online versus offline learning
There are two general methods by which an agent collects data for learning policies:

- Online. Here, an agent collects data directly from interacting with its surrounding environment. This data is processed and collected iteratively as the agent continues interacting with that environment.

- Offline. When an agent does not have direct access to an environment, it can learn through logged data of that environment. This is offline learning. A large subset of research has turned to offline learning given practical difficulties in training models through direct interaction with environments.8

Diagram for reinforcement learning topic page
Types of reinforcement learning
Reinforcement learning is a vibrant, ongoing area of research, and as such, developers have produced a myriad approaches to reinforcement learning. Nevertheless, three widely discussed and foundational reinforcement learning methods are dynamic programming, monte carlo, and temporal difference learning.

Dynamic programming
Dynamic programming breaks down larger tasks into smaller tasks. Thus, it models problems as workflows of sequential decision made at discrete time steps. Each decision is made in terms of the resulting possible next state. An agent’s reward (r) for a given action is defined as a function of that action (a), the current environmental state (s), and the potential next state (s’):

Dynamic programming formula
This reward function can be used as (part of) the policy governing an agent’s actions. Determining the optimal policy for agent behavior is a chief component of dynamic programming methods for reinforcement learning. Enter the Bellman equation.

The Bellman equation is:

Bellman equation formula
In short, this equation defines vt(s) as the total expected reward starting at time t until the end of a decision workflow. It assumes that the agent begins by occupying state s at time t. The equation ultimately divides the reward at time t into the immediate reward rt(s,a) (i.e. the reward formula) and the agent’s total expected reward. An agent thus maximizes its value function—being the total value of the Bellman equation—by consistently choosing that action which receives a reward signal in each state.9

Monte Carlo method
Dynamic programming is model-based, meaning it constructs a model of its environment to perceive rewards, identify patterns, and navigate the environment. Monte Carlo, however, assumes a black-box environment, making it model-free.

While dynamic programming predicts potential future states and reward signals in making decisions, Monte Carlo methods are exclusively experience-based, meaning they sample sequences of states, actions, and rewards solely through interaction with the environment. Monte Carlo methods thus learn through trial and error rather than probabilistic distributions.

Monte Carlo further differs from dynamic programming in value function determination. Dynamic programming seeks the largest cumulative reward by consistently selecting rewarded actions in successive states. Monte Carlo, by contrast, averages the returns for each state–action pair. This, in turn, means that the Monte Carlo method must wait until all actions in a given episode (or planning horizon) have been completed before calculating its value function, and then updating its policy.10

Temporal difference learning
Literature widely describes temporal difference (TD) learning as a combination of dynamic programming and Monte Carlo. As in the former, TD updates its policy, and so estimates for future states, after each step without waiting for a final value. As in Monte Carlo, however, TD learns through raw interaction with its environment rather than using a model thereof.11

Per its name, the TD learning agent revises its policy according to the difference between predicted and actual received rewards in each state. That is, while dynamic programming and Monte Carlo only consider the reward received, TD further weighs the difference between its expectation and received reward. Using this difference, the agent updates its estimates for the next step without waiting until the event planning horizon, contra Monte Carlo.12

TD has many variations. Two prominent variations are State–action–reward–state–action (SARSA) and Q-learning. SARSA is an on-policy TD method, meaning it evaluates and attempts to improve its decision-governing policy. Q-learning is off-policy. Off-policy methods are those that use two policies: one for exploitation (target policy) and one for exploration to generate behavior (behavior policy).13

Additional methods
There is a myriad of additional reinforcement learning methods. Dynamic programming is a value-based method, meaning it selects actions based on their estimated values according to a policy that aims to maximize its value function. By contrast, policy gradient methods learn a parameterized policy that can select actions without consulting a value function. These are called policy-based and are considered more effective in high-dimensional environments.14

Actor-critic methods use both value-based and policy-based. The so-called “actor” is a policy gradient determining which actions to take, while the “critic” is a value function to evaluate actions. Actor-critic methods are, essentially, a form of TD. More specifically, actor-critic evaluates a given action’s value based not only on its own reward but the possible value of the following state, which it adds to the action’s reward. Actor-critic’s advantage is that, due to its implementation of a value function and policy in decision-making, it effectively requires less environment interaction.15

Examples of reinforcement learning
Robotics
Given reinforcement learning is centrally concerned with decision-making in unpredictable environments, it has been a core area of interest in robotics. For accomplishing simple and repetitive tasks, decision-making may be straightforward. But more complicated tasks, such as attempts to simulate human behavior or automate driving, involve interaction with high-variable and mutable real-world environments. Research shows deep reinforcement learning with deep neural networks aids such tasks, especially with respect to generalization and mapping high-dimensionally sensory input to controlled systems outputs.16 Studies suggest that deep reinforcement learning with robots relies heavily on collected datasets, and so recent work explores avenues for collecting real-world data17 and repurposing prior data18 to improve reinforcement learning systems.

Natural language processing
Recent research suggests leveraging natural language processing techniques and tools—e.g. large language models (LLMs)—may improve generalization in reinforcement learning systems through textual representation of real-world environments.19 Many studies show how interactive textual environments provide cost-effective alternatives to three-dimensional environments when instructing learning agents in successive decision-making tasks.20 Deep reinforcement learning also undergirds textual decision-making in chatbots. In fact, reinforcement learning outperforms other methods for improving chatbot dialogue response.21

Reinforcement Learning
Last Updated : 24 Feb, 2025
Reinforcement Learning (RL) is a branch of machine learning that focuses on how agents can learn to make decisions through trial and error to maximize cumulative rewards. RL allows machines to learn by interacting with an environment and receiving feedback based on their actions. This feedback comes in the form of rewards or penalties.

Reinforcement-Learning_
Reinforcement Learning revolves around the idea that an agent (the learner or decision-maker) interacts with an environment to achieve a goal. The agent performs actions and receives feedback to optimize its decision-making over time.

Agent: The decision-maker that performs actions.
Environment: The world or system in which the agent operates.
State: The situation or condition the agent is currently in.
Action: The possible moves or decisions the agent can make.
Reward: The feedback or result from the environment based on the agent’s action.
How Reinforcement Learning Works?
The RL process involves an agent performing actions in an environment, receiving rewards or penalties based on those actions, and adjusting its behavior accordingly. This loop helps the agent improve its decision-making over time to maximize the cumulative reward.

Here’s a breakdown of RL components:

Policy: A strategy that the agent uses to determine the next action based on the current state.
Reward Function: A function that provides feedback on the actions taken, guiding the agent towards its goal.
Value Function: Estimates the future cumulative rewards the agent will receive from a given state.
Model of the Environment: A representation of the environment that predicts future states and rewards, aiding in planning.
Reinforcement Learning Example: Navigating a Maze
Imagine a robot navigating a maze to reach a diamond while avoiding fire hazards. The goal is to find the optimal path with the least number of hazards while maximizing the reward:

Each time the robot moves correctly, it receives a reward.
If the robot takes the wrong path, it loses points.
The robot learns by exploring different paths in the maze. By trying various moves, it evaluates the rewards and penalties for each path. Over time, the robot determines the best route by selecting the actions that lead to the highest cumulative reward.




The robot's learning process can be summarized as follows:

Exploration: The robot starts by exploring all possible paths in the maze, taking different actions at each step (e.g., move left, right, up, or down).
Feedback: After each move, the robot receives feedback from the environment:
A positive reward for moving closer to the diamond.
A penalty for moving into a fire hazard.
Adjusting Behavior: Based on this feedback, the robot adjusts its behavior to maximize the cumulative reward, favoring paths that avoid hazards and bring it closer to the diamond.
Optimal Path: Eventually, the robot discovers the optimal path with the least number of hazards and the highest reward by selecting the right actions based on past experiences.
Types of Reinforcements in RL
1. Positive Reinforcement
Positive Reinforcement is defined as when an event, occurs due to a particular behavior, increases the strength and the frequency of the behavior. In other words, it has a positive effect on behavior. 

Advantages: Maximizes performance, helps sustain change over time.
Disadvantages: Overuse can lead to excess states that may reduce effectiveness.
2. Negative Reinforcement
Negative Reinforcement is defined as strengthening of behavior because a negative condition is stopped or avoided. 

Advantages: Increases behavior frequency, ensures a minimum performance standard.
Disadvantages: It may only encourage just enough action to avoid penalties.
CartPole in OpenAI Gym
One of the classic RL problems is the CartPole environment in OpenAI Gym, where the goal is to balance a pole on a cart. The agent can either push the cart left or right to prevent the pole from falling over.

State space: Describes the four key variables (position, velocity, angle, angular velocity) of the cart-pole system.
Action space: Discrete actions—either move the cart left or right.
Reward: The agent earns 1 point for each step the pole remains balanced.



import gym
import numpy as np
import warnings
​
# Suppress specific deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
​
# Load the environment with render mode specified
env = gym.make('CartPole-v1', render_mode="human")
​
# Initialize the environment to get the initial state
state = env.reset()
​
# Print the state space and action space
print("State space:", env.observation_space)
print("Action space:", env.action_space)
​
# Run a few steps in the environment with random actions
for _ in range(10):
    env.render()  # Render the environment for visualization
    action = env.action_space.sample()  # Take a random action
    
    # Take a step in the environment
    step_result = env.step(action)
    
    # Check the number of values returned and unpack accordingly
    if len(step_result) == 4:
        next_state, reward, done, info = step_result
        terminated = False
    else:
        next_state, reward, done, truncated, info = step_result
        terminated = done or truncated
    
    print(f"Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}, Info: {info}")
    
    if terminated:
        state = env.reset()  # Reset the environment if the episode is finished
​
env.close()  # Close the environment when done
Output:

reinforcement-learning
Application of Reinforcement Learning
Robotics: RL is used to automate tasks in structured environments such as manufacturing, where robots learn to optimize movements and improve efficiency.
Game Playing: Advanced RL algorithms have been used to develop strategies for complex games like chess, Go, and video games, outperforming human players in many instances.
Industrial Control: RL helps in real-time adjustments and optimization of industrial operations, such as refining processes in the oil and gas industry.
Personalized Training Systems: RL enables the customization of instructional content based on an individual's learning patterns, improving engagement and effectiveness.
Advantages of Reinforcement Learning
Solving Complex Problems: RL is capable of solving highly complex problems that cannot be addressed by conventional techniques.
Error Correction: The model continuously learns from its environment and can correct errors that occur during the training process.
Direct Interaction with the Environment: RL agents learn from real-time interactions with their environment, allowing adaptive learning.
Handling Non-Deterministic Environments: RL is effective in environments where outcomes are uncertain or change over time, making it highly useful for real-world applications.
Disadvantages of Reinforcement Learning
Not Suitable for Simple Problems: RL is often an overkill for straightforward tasks where simpler algorithms would be more efficient.
High Computational Requirements: Training RL models requires a significant amount of data and computational power, making it resource-intensive.
Dependency on Reward Function: The effectiveness of RL depends heavily on the design of the reward function. Poorly designed rewards can lead to suboptimal or undesired behaviors.
Difficulty in Debugging and Interpretation: Understanding why an RL agent makes certain decisions can be challenging, making debugging and troubleshooting complex
Reinforcement Learning is a powerful technique for decision-making and optimization in dynamic environments. However, the complexity of RL necessitates careful design of reward functions and substantial computational resources. By understanding its principles and applications, RL can be leveraged to solve intricate real-world problems and drive advancements across various industries.

Evaluation Metrics in Machine Learning
Last Updated : 02 Jul, 2025
When building machine learning models, it’s important to understand how well they perform. Evaluation metrics help us to measure the effectiveness of our models. Whether we are solving a classification problem, predicting continuous values or clustering data, selecting the right evaluation metric allows us to assess how well the model meets our goals. In this article, we will see commonly used evaluation metrics and discuss how to choose the right metric for our model.

Classification Metrics
Classification problems aim to predict discrete categories. To evaluate the performance of classification models, we use the following metrics:

1. Accuracy
Accuracy is a fundamental metric used for evaluating the performance of a classification model. It tells us the proportion of correct predictions made by the model out of all predictions.

A
c
c
u
r
a
c
y
=
N
u
m
b
e
r
o
f
C
o
r
r
e
c
t
P
r
e
d
i
c
t
i
o
n
s
T
o
t
a
l
N
u
m
b
e
r
o
f
P
r
e
d
i
c
t
i
o
n
s
Accuracy= 
TotalNumberofPredictions
NumberofCorrectPredictions
​
 

While accuracy provides a quick snapshot, it can be misleading in cases of imbalanced datasets. For example, in a dataset with 90% class A and 10% class B, a model predicting only class A will still achieve 90% accuracy but it will fail to identify any class B instances.

Accuracy is good but it gives a False Positive sense of achieving high accuracy. The problem arises due to the possibility of misclassification of minor class samples being very high.

2. Precision
It measures how many of the positive predictions made by the model are actually correct. It's useful when the cost of false positives is high such as in medical diagnoses where predicting a disease when it’s not present can have serious consequences.

P
r
e
c
i
s
i
o
n
=
T
P
T
P
+
F
P
Precision= 
TP+FP
TP
​
 

Where:

TP = True Positives
FP = False Positives
Precision helps ensure that when the model predicts a positive outcome, it’s likely to be correct.

3. Recall
Recall or Sensitivity measures how many of the actual positive cases were correctly identified by the model. It is important when missing a positive case (false negative) is more costly than false positives.

R
e
c
a
l
l
=
T
P
T
P
+
F
N
Recall= 
TP+FN
TP
​
 

Where:

FN = False Negatives
In scenarios where catching all positive cases is important (like disease detection), recall is a key metric.

4. F1 Score
The F1 Score is the harmonic mean of precision and recall. It is useful when we need a balance between precision and recall as it combines both into a single number. A high F1 score means the model performs well on both metrics. Its range is [0,1].

Lower recall and higher precision gives us great accuracy but then it misses a large number of instances. More the F1 score better will be performance. It can be expressed mathematically in this way:

F1 Score
=
2
×
Precision
×
Recall
Precision
+
Recall
F1 Score=2× 
Precision+Recall
Precision×Recall
​
 

5. Logarithmic Loss (Log Loss)
Log loss measures the uncertainty of the model’s predictions. It is calculated by penalizing the model for assigning low probabilities to the correct classes. This metric is used in multi-class classification and is helpful when we want to assess a model’s confidence in its predictions. If there are N  samples belonging to the M class, then we calculate the Log loss in this way:

Logarithmic Loss
=
−
1
N
∑
i
=
1
N
∑
j
=
1
M
y
i
j
⋅
log
⁡
(
p
i
j
)
Logarithmic Loss=− 
N
1
​
 ∑ 
i=1
N
​
 ∑ 
j=1
M
​
 y 
ij
​
 ⋅log(p 
ij
​
 ) 

Where:

y
i
j
y 
ij
​
 =Actual class (0 or 1) for sample 
i
i and class 
j
j
p
i
j
p 
ij
​
  =Predicted probability for sample 
i
i and class 
j
j
The goal is to minimize Log Loss, as a lower Log Loss shows higher prediction accuracy.

6. Area Under Curve (AUC) and ROC Curve
It is useful for binary classification tasks. The AUC value represents the probability that the model will rank a randomly chosen positive example higher than a randomly chosen negative example. AUC ranges from 0 to 1 with higher values showing better model performance.

1. True Positive Rate(TPR)

Also known as sensitivity or recall, the True Positive Rate measures how many actual positive instances were correctly identified by the model. It answers the question: "Out of all the actual positive cases, how many did the model correctly identify?"

Formula:

T
P
R
=
T
P
T
P
+
F
N
    
TPR= 
TP+FN
TP
​
      

Where:

TP = True Positives (correctly predicted positive cases)
FN = False Negatives (actual positive cases incorrectly predicted as negative)
2. True Negative Rate(TNR)

Also called specificity, the True Negative Rate measures how many actual negative instances were correctly identified by the model. It answers the question: "Out of all the actual negative cases, how many did the model correctly identify as negative?"

Formula:

T
N
R
=
T
N
T
N
+
F
P
 
TNR= 
TN+FP
TN
​
   

Where:

TN = True Negatives (correctly predicted negative cases)
FP = False Positives (actual negative cases incorrectly predicted as positive)
3. False Positive Rate(FPR)

It measures how many actual negative instances were incorrectly classified as positive. It’s a key metric when the cost of false positives is high such as in fraud detection.

Formula:

F
P
R
=
F
P
F
P
+
T
N
FPR= 
FP+TN
FP
​
 

Where:

FP = False Positives (incorrectly predicted positive cases)
TN = True Negatives (correctly predicted negative cases)
4. False Negative Rate(FNR)

It measures how many actual positive instances were incorrectly classified as negative. It answers: "Out of all the actual positive cases, how many were misclassified as negative?"

Formula:

F
N
R
=
F
N
F
N
+
T
P
FNR= 
FN+TP
FN
​
 

Where:

FN = False Negatives (incorrectly predicted negative cases)
TP = True Positives (correctly predicted positive cases)
ROC Curve

It is a graphical representation of the True Positive Rate (TPR) vs the False Positive Rate (FPR) at different classification thresholds. The curve helps us visualize the trade-offs between sensitivity (TPR) and specificity (1 - FPR) across various thresholds. Area Under Curve (AUC) quantifies the overall ability of the model to distinguish between positive and negative classes.

AUC = 1: Perfect model (always correctly classifies positives and negatives).
AUC = 0.5: Model performs no better than random guessing.
AUC < 0.5: Model performs worse than random guessing (showing that the model is inverted).
ROC Curve for Evaluation of Classification Models
ROC Curve for Evaluation of Classification Models
7. Confusion Matrix
Confusion matrix creates a N X N matrix, where N is the number of classes or categories that are to be predicted. Here we have N = 2, so we get a 2 X 2 matrix. Suppose there is a problem with our practice which is a binary classification. Samples of that classification belong to either Yes or No. So, we build our classifier which will predict the class for the new input sample. After that, we tested our model with 165 samples and we get the following result.

n=165

Predicted No

Predited Yes

Actual No

50

10

Actual Yes

5

100

There are 4 terms we should keep in mind: 

True Positives: It is the case where we predicted Yes and the real output was also Yes.
True Negatives: It is the case where we predicted No and the real output was also No.
False Positives: It is the case where we predicted Yes but it was actually No.
False Negatives: It is the case where we predicted No but it was actually Yes. 
Regression Metrics
In the regression task, we are supposed to predict the target variable which is in the form of continuous values. To evaluate the performance of such a model below metrics are used:

1. Mean Absolute Error (MAE)
MAE calculates the average of the absolute differences between the predicted and actual values. It gives a clear view of the model’s prediction accuracy but it doesn't shows whether the errors are due to over- or under-prediction. It is simple to calculate and interpret helps in making it a good starting point for model evaluation.

M
A
E
=
1
N
∑
j
=
1
N
∣
y
j
−
y
^
j
∣
MAE= 
N
1
​
 ∑ 
j=1
N
​
 ∣y 
j
​
 − 
y
^
​
  
j
​
 ∣

Where:

y
j
y 
j
​
  = Actual value
y
^
j
y
^
​
  
j
​
  = Predicted value
2. Mean Squared Error (MSE)
MSE calculates the average of the squared differences between the predicted and actual values. Squaring the differences ensures that larger errors are penalized more heavily helps in making it sensitive to outliers. This is useful when large errors are undesirable but it can be problematic when outliers are not relevant to the model’s purpose.

Formula:

M
S
E
=
1
N
∑
j
=
1
N
(
y
j
−
y
^
j
)
2
MSE= 
N
1
​
 ∑ 
j=1
N
​
 (y 
j
​
 − 
y
^
​
  
j
​
 ) 
2
 

Where:

y
j
y 
j
​
  = Actual value
y
^
j
y
^
​
  
j
​
  = Predicted value
3. Root Mean Squared Error (RMSE)
RMSE is the square root of MSE, bringing the metric back to the original scale of the data. Like MSE, it heavily penalizes larger errors but is easier to interpret as it’s in the same units as the target variable. It’s useful when we want to know how much our predictions deviate from the actual values in terms of the same scale.

Formula:

R
M
S
E
=
∑
j
=
1
N
(
y
j
−
y
^
j
)
2
N
RMSE= 
N
∑ 
j=1
N
​
 (y 
j
​
 − 
y
^
​
  
j
​
 ) 
2
 
​
 
​
 

Where:

y
j
y 
j
​
  = Actual value
y
^
j
y
^
​
  
j
​
  = Predicted value
4. Root Mean Squared Logarithmic Error (RMSLE)
RMSLE is useful when the target variable spans a wide range of values. Unlike RMSE, it penalizes underestimations more than overestimations helps in making it ideal for situations where the model is predicting quantities that vary greatly in scale like predicting prices or population.

Formula:

R
M
S
L
E
=
∑
j
=
1
N
(
log
⁡
(
y
j
+
1
)
−
log
⁡
(
y
^
j
+
1
)
)
2
N
RMSLE= 
N
∑ 
j=1
N
​
 (log(y 
j
​
 +1)−log( 
y
^
​
  
j
​
 +1)) 
2
 
​
 
​
 

Where:

y
j
y 
j
​
  = Actual value
y
^
j
y
^
​
  
j
​
  = Predicted value
5. R² (R-squared)
R2 score represents the proportion of the variance in the dependent variable that is predictable from the independent variables. An R² value close to 1 shows a model that explains most of the variance while a value close to 0 shows that the model does not explain much of the variability in the data. R² is used to assess the goodness-of-fit of regression models.

Formula:

R
2
=
1
−
∑
j
=
1
n
(
y
j
−
y
^
j
)
2
∑
j
=
1
n
(
y
j
−
y
ˉ
)
2
R 
2
 =1− 
∑ 
j=1
n
​
 (y 
j
​
 − 
y
ˉ
​
 ) 
2
 
∑ 
j=1
n
​
 (y 
j
​
 − 
y
^
​
  
j
​
 ) 
2
 
​
 

Where:

y
j
y 
j
​
  = Actual value
y
^
j
y
^
​
  
j
​
  = Predicted value
y
ˉ
y
ˉ
​
 = Mean of the actual values
Clustering Metrics
In unsupervised learning tasks such as clustering, the goal is to group similar data points together. Evaluating clustering performance is often more challenging than supervised learning since there is no explicit ground truth. However, clustering metrics provide a way to measure how well the model is grouping similar data points.

1. Silhouette Score
Silhouette Score evaluates how well a data point fits within its assigned cluster considering how close it is to points in its own cluster (cohesion) and how far it is from points in other clusters (separation). A higher silhouette score (close to +1) shows well-clustered data while a score near -1 suggests that the data point is in the wrong cluster.

Formula:

Silhouette Score
=
b
−
a
max
⁡
(
a
,
b
)
Silhouette Score= 
max(a,b)
b−a
​
 

Where:

a = Average distance between a sample and all other points in the same cluster
b = Average distance between a sample and all points in the nearest cluster
2. Davies-Bouldin Index
Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster. A lower Davies-Bouldin index shows better clustering as it suggests the clusters are well-separated and compact. The goal is to minimize the Davies-Bouldin index to achieve optimal clustering.

Formula:

Davies-Bouldin Index
=
1
N
∑
i
=
1
N
max
⁡
i
≠
j
(
σ
i
+
σ
j
d
(
c
i
,
c
j
)
)
Davies-Bouldin Index= 
N
1
​
 ∑ 
i=1
N
​
 max 
i

=j
​
 ( 
d(c 
i
​
 ,c 
j
​
 )
σ 
i
​
 +σ 
j
​
 
​
 )

Where:

σ
i
σ 
i
​
  = Average distance of points in cluster i from the cluster centroid
d
(
c
i
,
c
j
)
d(c 
i
​
 ,c 
j
​
 ) = Distance between centroids of clusters i and j
By mastering the appropriate evaluation metrics, we upgrade ourselves to fine-tune machine learning models which helps in ensuring they meet the needs of diverse applications and deliver optimal performance.


Introduction to bias-variance tradeoff
In machine learning (ML) and artificial intelligence (AI), bias-variance tradeoff is a concept that governs the performance of a predictive machine learning model and a fundamental tenant in data science.

When we decide on building an ML model for a specific business problem, we want to choose a model architecture that minimizes errors and capture underlying signals. Bias and variance represent two sources of prediction error. Bias measures how far off predictions are from the true values due to overly simplistic assumptions; variance, however, captures how much predictions fluctuate based on different training data.

Understanding and managing this tradeoff is crucial for building models that generalize well to unseen data. Models with high bias are prone to underfitting, missing important patterns, while models with high variance are prone to overfitting, capturing noise as if it were signal. Striking the right balance is at the heart of effective machine learning design and helps explain why models that perform well on training data might still fail in the real world.

In this explainer, we dive into technical details of bias-variance tradeoff and prediction error, painting a picture of how to build the right model for a dataset. 

Tradeoff illustrated
In predictive models such as linear regression or K-nearest neighbor (KNN), bias and variance are interdependent:

Bias measures how far off, on average, a model’s predictions are from the ground truth values. High-bias models tend to make strong assumptions about the form of the data and cause underfitting. An overly simplistic model tends to have high bias and low variance—a model like this tends to have high training errors and high prediction errors.  
Variance measures how much a model’s predictions change with different training datasets. High-variance models are sensitive to noise in the training data and cause overfitting. A model with complex architecture and more parameters tends to have high variance and low bias.
Bias variance diagram
In this explainer, we use linear regression as an example to illustrate how the model complexity affects the bias and variance in predicted results. Recall that in linear regression, the evaluation metric is defined by mean square error (MSE): the average squared error from ground truth and predicted value. A large MSE indicates a poorly fit model on the training data, whereas a low MSE indicates a well-fitted model on the training data.

MSE is defined as:

 
M
S
E
=
(
y
p
r
e
d
-
y
a
c
t
u
a
l
)
2
  

Or expressed as a residual sum of squares:

 
R
S
S
=
∑
i
=
1
n
(
y
i
-
y
i
^
)
2





Let’s say we’re given a set of input values X and corresponding output values Y. The true relationship between X and Y is nonlinear—think of a smooth, curved U-shape like a sine wave. But we don’t know that underlying function. Instead, we observe noisy data points that approximate it.

Graphic of noisy data
We now want to build a model to predict Y by using X.

To illustrate how model complexity affects performance, we can try fitting three models of increasing complexity: a linear model, a moderately complex polynomial model and a very complex polynomial model.

This noise component introduces randomness, mimicking real-world data. A polynomial is a mathematical expression involving a sum of powers of X multiplied by coefficients.

For example, a degree 1 polynomial is:

 
y
^
=
β
0
+
β
1
x

The model is represented as a straight line:

Polynomial degree 1
This model is very simple and makes a strong assumption that the relationship between X and Y is linear. But the data clearly has a curved pattern. As a result:

Bias is high: The model cannot capture the nonlinear pattern in the data.
Variance is low: It is stable and doesn’t change much with different datasets.
MSE (mean squared error): 0.2929. This is relatively high.
This is an example of underfitting—the model is too simple to learn the true structure.

A degree 4 polynomial is:

 
y
^
=
β
0
+
β
1
x
+
β
2
x
2
+
β
3
x
3
+
β
4
x
4

Polynomial degree 4
Now we use a polynomial that includes powers of x up to  
x
4
 :

 
y
^
=
β
0
+
β
1
x
+
β
2
x
2
+
β
3
x
3
+
β
4
x
4

This model is complex enough to capture the curve of the data without being too sensitive to noise.

Bias is moderate: The model can represent the true function fairly well.
Variance is moderate: It doesn’t overreact to small fluctuations in the data.
MSE: About 0.0714, lower than degree 1.
This is the best-performing model in our example—it generalizes well.

A degree 25 polynomial is:

 
y
^
=
∑
i
=
0
25
β
i
x
i

Polynomial degree 25
With 26 parameters, the model has high flexibility and fits the training data very closely—even the random noise. The curve looks very squiggly and overfits the data.

Bias is low: It’s flexible enough to follow the signal.
Variance is high: It reacts strongly to noise and would change significantly with a new sample of data.
 MSE: About 0.059—lower than degree 4 because it overmemorized the pattern of the training data and over.
This is an example of overfitting—the model learns the noise along with the signal and doesn’t generalize well to the unseen data.

The higher the degree, the more "wiggly" the curve becomes, and the more it can adapt to the training data—including both signal and noise.

In the example above, we can see that model complexity and the number of parameters directly affect bias-variance tradeoff. As the model becomes more complex and has more parameters, the variability in predicted values in the testing set increases, leading to high variance. However, as the model simplifies and the numbers of parameters decrease, the  
b
i
a
s
2
  in prediction increases.

Therefore, when we construct a machine learning model, we aim to simultaneously bias and variance to achieve optimum model performance. This optimization not only generates good results from the training, but also generalize well to unseen testing data. In the next section, we dive into the mathematical details of how bias and variance calculation is derived and why machine learning model contains uncertainties that are made up of bias, variance and irreducible error.

Bias variance tradeoff
3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Bias and variance in practice
Understanding how bias and variance manifest in real-world machine learning models is essential for diagnosing and improving performance. In the following section, we dive into details on how high bias and high variance model lead to potentially poor performances in an AI system.

High-bias models

High-bias models are typically too simplistic to capture the true patterns in the data. They underfit the training set, leading to poor training and test accuracy. A classic example is linear regression applied to the nonlinear data shown before. If the true relationship between features and target is quadratic or sinusoidal and we fit a straight line, the model lacks the capacity to capture the underlying structure.

Symptoms: High error on both training and test sets. The bias becomes large and leading to poor performance on both training set and testing set.

High-variance models

High-variance models are overly flexible and fit the training data too closely, including the noise. They overfit the training set and fail to generalize to unseen data, leading to overfitting and leading to predictions with abnormally high variability.

Common examples include:

Decision trees with no pruning.
Polynomial regression with high degrees.
KNN with very low k.
Symptoms: Low training error but high test error. The predictions vary significantly across different datasets. The variance term dominates the error, indicating the model is unstable regarding training data changes.

Diagnosing bias and variance
Some practical tools to diagnose these errors include:

Learning curves (shown before in section I):

Plot training and validation error versus training set size.
If both errors are high and converge, it indicates a high bias.
If training error is low and validation error is high, with a gap that doesn't close, it suggests high variance. Cross-validation can be applied to diagnose the performance of the model and averaging out errors from the selected training set.

Helps estimate generalization error.
Useful for comparing models or hyperparameters in a variance-aware way.
Real-world consideration
In practice, controlling the bias-variance tradeoff is less about picking the "perfect" model and more about managing complexity through various strategies. We can apply several techniques to control the variability in prediction errors by applying some of the following strategies:

Regularization

Regularization refers to a set of techniques used to constrain or penalize a model's complexity to improve generalization—that is, performance on unseen data. In mathematical terms, regularization modifies the original loss function by adding a penalty term that discourages complexity (usually in the form of large weights or overly flexible models).

The goal is to prevent overfitting, especially when dealing with high-dimensional or limited data. When training a machine learning model, we typically minimize a loss function like Mean Squared Error (MSE)

 
R
S
S
=
∑
i
=
1
n
(
y
i
-
y
i
^
)
2

With regularization, we add a penalty to this objective. 

L2 regularization (ridge regression)

 
L
o
s
s
R
i
d
g
e
=
∑
i
=
1
n
(
y
i
-
y
i
^
)
2
+
λ
*
P
e
n
a
l
t
y

Here,

 
λ
  is a hyperparameter that controls the tradeoff between fitting the training data and keeping the model simple.

It adds a penalty proportional to the square of the magnitude of coefficients. This discourages overly large weights, reducing variance. The penalty term ensures the features that have low predictive power to have low values, effectively reducing the coefficients of the parameters.

L1 regularization (lasso)

Encourages sparsity:

 
L
o
s
s
l
a
s
s
o
=
∑
i
=
1
n
(
y
i
-
ŷ
i
)
2
+
λ
∑
j
=
1
p
|
β
j
|

It can eliminate irrelevant features entirely, simplifying the model and thus reducing the variance. The penalty term USD{\sum_{j=1}^{p} |\beta_j}USD ensures the insignificant features to be reduced to zero, effectively completely eliminating the features.

Ensemble methods

Ensemble methods combine multiple models to reduce error by averaging out individual prediction deviation. It involves combining, or stacking multiple high-variance models together to get the best prediction accuracy. Some examples include:

- Bagging (for example, Random forests) reduces variance by averaging multiple high-variance estimators trained on different data subsets.

- Boosting (for example, xgBoost, AdaBoost) builds a strong learner by sequentially correcting the errors of previous models, often balancing reduction of bias or variance with careful tuning.

Hyperparameter tuning and model selection

Model complexity and regularization strength are often controlled through hyperparameters. Techniques like grid search or random search with cross-validation or Bayesian optimization can help find a model that balances bias and variance on held-out data.






AI Academy

AI Academy 2024 Teaser Trailer
Become an AI expert
Gain the knowledge to prioritize AI investments that drive business growth. Get started with our free AI Academy today and lead the future of AI in your organization.

Watch the series 
Applications to modern AI
The bias-variance tradeoff is not just theoretical. It plays a critical role in deep learning and large-scale AI systems. In the modern era of AI, the choice of neural network architecture plays a critical role in managing the tradeoff between bias and variance. Here's how two foundational architectures—CNNs and RNNs—navigate this balance in practice.

1. Convolutional neural networks (CNNs): CNNs are designed specifically for data with a spatial structure—most commonly, images. Their architectural features allow them to reduce variance while maintaining sufficient expressiveness to keep bias low.

Local receptive fields (Convolutions): Instead of connecting every input pixel to every output neuron (as in fully connected networks), CNNs use small filters (kernels) that slide across the input. This enforces the assumption that local features are useful—a bias toward spatial locality.
Weight sharing: Each filter (or kernel) is reused across the entire image, drastically reducing the number of trainable parameters. This limits overfitting, lowering variance, but introduces some bias by constraining the model’s flexibility.
Pooling layers (for example, max pooling): These layers summarize feature maps and introduce translation invariance. While this reduces variance by ignoring minor fluctuations, it might increase bias by discarding some potentially useful details.
Hierarchical feature learning: CNNs learn from low-level edges to high-level shapes layer by layer. This layered inductive bias allows generalization with fewer examples—helpful in data-scarce domains.
2. Recurrent neural networks (RNNs): RNNs are tailored to sequential data such as text, speech or time series, where current outputs depend on previous elements. Their design tries to balance long-term dependencies (which reduce bias) and training stability (which controls variance).

Weight sharing over time: RNNs use the same parameters at every time step, introducing a bias toward stationarity in sequences (assuming the same kind of patterns recur), but significantly reducing variance by limiting parameter growth.
Memory of past inputs: RNNs maintain a hidden state h_t that summarizes past information. In theory, this state allows the model to reduce bias by modeling long-range dependencies. However, in practice, vanishing gradients often prevent them from learning long-term relationships effectively, increasing bias.
Variants like long short-term memory (LSTM) and gated recurrent unit (GRU): These architectures mitigate vanishing gradients by using gates, allowing better memory retention over time. As a result, they can lower bias further without a large increase in variance.
Training stability and overfitting: Deep RNNs (many layers or long sequences) are prone to high variance—overfitting noise in training sequences. Techniques like dropout, gradient clipping and sequence bucketing are often used to control this.
Techniques that control the tradeoff
Dropout: Randomly turning off neurons during training adds noise, forcing the network to learn redundant representations—reducing overfitting and thus variance.
Batch normalization: Helps stabilize and accelerate training, and often reduces variance by smoothing optimization.
Early stopping: Prevents overfitting by halting training when validation loss starts increasing.
Transfer learning: Pretrained models on large datasets often generalize better with fewer parameters to train, reducing variance on small datasets.
Scaling laws and modern observations: Recent findings in large models (like transformers) show that increasing data, compute and model size reduces test error—suggesting bias decreases faster than variance increases in high-capacity models. However, poor regularization or insufficient data can still lead to overfitting.
Theoretical foundations
Let's dive into the mathematical foundations of the bias-variance tradeoff. Recall from the previous example, we aim to reduce the total error of predicted values and actual values. This error is composed of three components: bias, variance and irreducible error. We can analyze the expected squared prediction error of a model:

 
f
^
(
x
)

compared to the true function:  
f
(
x
)
,

where  
f
^
(
x
)
  is learned from a training dataset  
D
 , and  
x
   is the true (unknown) function.

Let:

 
y
=
f
(
x
)
+
ε
,
ε
∼
N
(
0
,
σ
2
)

this means for the function  
y
=
f
(
x
)
+
ε
 , the error (denoted by  
ε
 ) is normally distributed with a mean of 0 and a variance of  
σ
2
 ,  
σ
 denotes the standard deviation of the distribution

 
f
^
(
x
)
  is the model’s predicted value at input  
x

The expectation (or mean) is taken over different training datasets  
D
  and noise  
ε
 . The symbol  
E
  is used to express "expectation," or "expected value," which is a true value of the mean of the distribution

We are interested in the expected prediction error at a single point  
x
 :

 
E
D
,
ε
[
(
y
-
f
^
(
x
)
)
2
]

Substitute:

 
y
=
f
(
x
)
+
ε

So the expression becomes:

 
=
E
D
,
ε
[
(
f
(
x
)
+
ε
-
f
^
(
x
)
)
2
]

Expanding the square:

 
$
=
E
D
,
ε
[
(
f
(
x
)
-
f
^
(
x
)
)
2
+
2
(
f
(
x
)
-
f
^
(
x
)
)
ε
+
ε
2
]
$

Split the expectation by using linearity (linearity is a simple algebraic concept, for example,  
E
[
A
+
B
]
=
E
[
A
]
+
E
[
B
]
):

 
=
E
D
[
(
f
(
x
)
-
f
^
(
x
)
)
2
]
+
2
E
D
,
ε
[
(
f
(
x
)
-
f
^
(
x
)
)
ε
]
+
E
ε
[
ε
2
]

Now, since:

 
E
[
ε
]
=
0
⇒
E
[
(
f
(
x
)
-
f
^
(
x
)
)
ε
]
=
0

 
E
[
ε
2
]
=
σ
2

We get:

 
E
D
[
(
f
(
x
)
-
f
^
(
x
)
)
2
]
+
σ
2

Decomposing the first term:

Add and subtract 
E
D
[
f
^
(
x
)
]
 :

 
E
D
[
(
f
(
x
)
-
f
^
(
x
)
)
2
]
=
E
D
[
(
f
(
x
)
-
E
D
[
f
^
(
x
)
]
+
E
D
[
f
^
(
x
)
]
-
f
^
(
x
)
)
2
]

Let:

 
a
=
f
(
x
)
-
E
D
[
f
^
(
x
)
]

 
b
=
E
D
[
f
^
(
x
)
]
-
f
^
(
x
)

Then:

 
E
D
[
(
a
+
b
)
2
]
=
a
2
+
E
D
[
b
2
]
+
2
a
E
D
[
b
]

Since  
E
D
[
b
]
=
0
 , the cross term vanishes, and we get:

 
=
(
f
(
x
)
-
E
D
[
f
^
(
x
)
]
)
2
+
E
D
[
(
f
^
(
x
)
-
E
D
[
f
^
(
x
)
]
)
2
]

Final bias-variance decomposition:

 
E
D
,
ε
[
(
y
-
f
^
(
x
)
)
2
]
=
$
$
(
f
(
x
)
-
E
D
[
f
^
(
x
)
]
)
2
+
E
D
[
(
f
^
(
x
)
-
E
D
[
f
^
(
x
)
]
)
2
]
+
σ
2

Here, the first term is  
b
i
a
s
2
 , second term is  
v
a
r
i
a
n
c
e
 , and the third term is irreducible error

This shows that the total expected prediction error can be decomposed into:

- Bias²: Error from erroneous assumptions in the model (for example, underfitted, overly simple model)

- Variance: Error from sensitivity to training data (for example, overfitted, overly complex model)

- Irreducible noise: Unavoidable randomness and error in the observations

 

Conclusion and further reading
In summary, bias and variance are two fundamental sources of prediction error in machine learning. Understanding this tradeoff is not just a theoretical exercise—it directly shapes how we design, train and deploy ML models in practice.

Whether you're choosing between a simple linear model or a complex deep neural network, recognizing the balance between underfitting and overfitting is essential to building robust AI systems. While we focused on mean squared error (MSE) as our loss function, this tradeoff applies to a wide range of distributions and error metrics—making it a universal consideration across supervised learning.

In recent years, researchers have observed intriguing behavior in large, overparameterized models like deep neural networks. Despite their high capacity, these models often generalize well, even when they perfectly fit the training data—seemingly defying the traditional bias-variance framework.

This puzzling behavior is explored in works like "Reconciling modern machine learning and the bias-variance trade-off" by Belkin et al. (2019), which introduces the concept of double descent, and "A universal law of robustness via isoperimetry" by Bubeck et al., which proposes a geometric interpretation of generalization.

As we build more powerful AI systems, a deeper understanding of these dynamics becomes essential—not only for optimizing performance, but also for interpreting model behavior, ensuring fairness, and advancing responsible AI practices.

Bias-Variance Trade Off - Machine Learning
Last Updated : 05 Jun, 2023
It is important to understand prediction errors (bias and variance) when it comes to accuracy in any machine-learning algorithm. There is a tradeoff between a model’s ability to minimize bias and variance which is referred to as the best solution for selecting a value of Regularization constant. A proper understanding of these errors would help to avoid the overfitting and underfitting of a data set while training the algorithm. 

What is Bias?
The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data. This happens when the hypothesis is too simple or linear in nature. Refer to the graph given below for an example of such a situation.

HighBias
High Bias in the Model
In such a problem, a hypothesis looks like follows.

h
θ
(
x
)
=
g
(
θ
0
+
θ
1
x
1
+
θ
2
x
2
)
 
h 
θ
​
 (x)=g(θ 
0
​
 +θ 
1
​
 x 
1
​
 +θ 
2
​
 x 
2
​
 )  

What is Variance?
The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data. When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high. While training a data model variance should be kept low. The high variance data looks as follows.


High Variance in the Model
In such a problem, a hypothesis looks like follows.

h
θ
(
x
)
=
g
(
θ
0
+
θ
1
x
+
θ
2
x
2
+
θ
3
x
3
+
θ
4
x
4
)
h 
θ
​
 (x)=g(θ 
0
​
 +θ 
1
​
 x+θ 
2
​
 x 
2
 +θ 
3
​
 x 
3
 +θ 
4
​
 x 
4
 )

Bias Variance Tradeoff
If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time. For the graph, the perfect tradeoff will be like this.


 
 We try to optimize the value of the total error for the model by using the Bias-Variance Tradeoff.

T
o
t
a
l
E
r
r
o
r
=
B
i
a
s
2
+
V
a
r
i
a
n
c
e
+
I
r
r
e
d
u
c
i
b
l
e
E
r
r
o
r
TotalError=Bias 
2
 +Variance+IrreducibleError

The best fit will be given by the hypothesis on the tradeoff point. The error to complexity graph to show trade-off is given as - 

Region for the Least Value of Total Error
Region for the Least Value of Total Error
 This is referred to as the best point chosen for the training of the algorithm which gives low error in training as well as testing data.
ML | Underfitting and Overfitting
Last Updated : 27 Jan, 2025
Machine learning models aim to perform well on both training data and new, unseen data and is considered "good" if:

It learns patterns effectively from the training data.
It generalizes well to new, unseen data.
It avoids memorizing the training data (overfitting) or failing to capture relevant patterns (underfitting).
To evaluate how well a model learns and generalizes, we monitor its performance on both the training data and a separate validation or test dataset which is often measured by its accuracy or prediction errors. However, achieving this balance can be challenging. Two common issues that affect a model's performance and generalization ability are overfitting and underfitting. These problems are major contributors to poor performance in machine learning models. Let's us understand what they are and how they contribute to ML models.

Bias and Variance in Machine Learning
Bias and variance are two key sources of error in machine learning models that directly impact their performance and generalization ability.

Bias: is the error that happens when a machine learning model is too simple and doesn't learn enough details from the data. It's like assuming all birds can only be small and fly, so the model fails to recognize big birds like ostriches or penguins that can't fly and get biased with predictions.

These assumptions make the model easier to train but may prevent it from capturing the underlying complexities of the data.
High bias typically leads to underfitting, where the model performs poorly on both training and testing data because it fails to learn enough from the data.
Example: A linear regression model applied to a dataset with a non-linear relationship.
Variance: Error that happens when a machine learning model learns too much from the data, including random noise.

A high-variance model learns not only the patterns but also the noise in the training data, which leads to poor generalization on unseen data.
High variance typically leads to overfitting, where the model performs well on training data but poorly on testing data.
Overfitting and Underfitting: The Core Issues
1. Overfitting in Machine Learning
Overfitting happens when a model learns too much from the training data, including details that don’t matter (like noise or outliers).

For example, imagine fitting a very complicated curve to a set of points. The curve will go through every point, but it won’t represent the actual pattern.
As a result, the model works great on training data but fails when tested on new data.
Overfitting models are like students who memorize answers instead of understanding the topic. They do well in practice tests (training) but struggle in real exams (testing).

Reasons for Overfitting:

 High variance and low bias.
The model is too complex.
The size of the training data.
2. Underfitting in Machine Learning
Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what’s going on in the data.

For example, imagine drawing a straight line to fit points that actually follow a curve. The line misses most of the pattern.
In this case, the model doesn’t work well on either the training or testing data.
Underfitting models are like students who don’t study enough. They don’t do well in practice tests or real exams. Note: The underfitting model has High bias and low variance.

Reasons for Underfitting:

The model is too simple, So it may be not capable to represent the complexities in the data.
The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.
The size of the training dataset used is not enough.
Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.
Features are not scaled.
Let's visually understand the concept of underfitting, proper fitting, and overfitting.

Bias and Variance-Geeksforgeeks
: Bias and Variance
Underfitting : Straight line trying to fit a curved dataset but cannot capture the data's patterns, leading to poor performance on both training and test sets.
Overfitting: A squiggly curve passing through all training points, failing to generalize performing well on training data but poorly on test data.
Appropriate Fitting: Curve that follows the data trend without overcomplicating to capture the true patterns in the data.
Balance Between Bias and Variance
The relationship between bias and variance is often referred to as the bias-variance tradeoff, which highlights the need for balance:

Increasing model complexity reduces bias but increases variance (risk of overfitting).
Simplifying the model reduces variance but increases bias (risk of underfitting).
The goal is to find an optimal balance where both bias and variance are minimized, resulting in good generalization performance.

Imagine you're trying to predict the price of houses based on their size, and you decide to draw a line or curve that best fits the data points on a graph. How well this line captures the trend in the data depends on the complexity of the model you use.

Underfitting and Overfitting in Machine Learning-Geeksforgeeks
Underfitting and Overfitting
When a model is too simple, like fitting a straight line to curved data, it has high bias and fails to capture the true relationship, leading to underfitting. For example, a linear model cannot represent a non-linear increase in house prices with size.
However, if the model becomes too complex, like a fourth-degree polynomial that adjusts to every point, it develops high variance, overfits the training data, and struggles to generalize to new data. This is overfitting, where the model performs well on training but poorly on testing.
An ideal model strikes a balance with low bias and low variance, capturing the overall pattern without overreacting to noise. For instance, a smooth second-degree polynomial fits the data well without being overly complex.
How to Address Overfitting and Underfitting?
Techniques to Reduce Underfitting
Increase model complexity.
Increase the number of features, performing feature engineering.
Remove noise from the data.
Increase the number of epochs or increase the duration of training to get better results.
Techniques to Reduce Overfitting

Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.
Increase the training data can improve the model's ability to generalize to unseen data and reduce the likelihood of overfitting.
Reduce model complexity.
Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).
Ridge Regularization and Lasso Regularization.
Use dropout for neural networks to tackle overfitting.

What is deep learning?
Deep learning is a subset of machine learning that uses multilayered neural networks, called deep neural networks, to simulate the complex decision-making power of the human brain. Some form of deep learning powers most of the artificial intelligence (AI) applications in our lives today.

The chief difference between deep learning and machine learning is the structure of the underlying neural network architecture. “Nondeep,” traditional machine learning models use simple neural networks with one or two computational layers. Deep learning models use three or more layers, but typically hundreds or thousands of layers to train the models.

While supervised learning models require structured, labeled input data to make accurate outputs, deep learning models can use unsupervised learning. With unsupervised learning, deep learning models can extract the characteristics, features and relationships they need to make accurate outputs from raw, unstructured data. Additionally, these models can even evaluate and refine their outputs for increased precision.

Deep learning is an aspect of data science that drives many applications and services that improve automation, performing analytical and physical tasks without human intervention. This enables many everyday products and services, such as digital assistants, voice-enabled TV remotes, credit card fraud detection, self-driving cars and generative AI.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

How deep learning works
Neural networks, or artificial neural networks, attempt to mimic the human brain through a combination of data inputs, weights and bias, all acting as silicon neurons. These elements work together to accurately recognize, classify and describe objects within the data.

Deep neural networks consist of multiple layers of interconnected nodes, each building on the previous layer to refine and optimize the prediction or categorization. This progression of computations through the network is called forward propagation. The input and output layers of a deep neural network are called visible layers. The input layer is where the deep learning model ingests the data for processing, and the output layer is where the final prediction or classification is made.

Another process called backpropagation uses algorithms, such as gradient descent, to calculate errors in predictions, and then adjusts the weights and biases of the function by moving backwards through the layers to train the model. Together, forward propagation and backpropagation enable a neural network to make predictions and correct for any errors. Over time, the algorithm becomes gradually more accurate.

Deep learning requires a tremendous amount of computing power. High-performance graphical processing units (GPUs) are ideal because they can handle a large volume of calculations in multiple cores with copious memory available. Distributed cloud computing might also assist. This level of computing power is necessary to train deep algorithms through deep learning. However, managing multiple GPUs on premises can create a large demand on internal resources and be incredibly costly to scale. For software requirements, most deep learning apps are coded with one of these three learning frameworks: JAX, PyTorch or TensorFlow.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Types of deep learning models
Deep learning algorithms are incredibly complex, and there are different types of neural networks to address specific problems or datasets. Here are six. Each has its own advantages and they are presented here roughly in the order of their development, with each successive model adjusting to overcome a weakness in a previous model.

One potential weakness across them all is that deep learning models are often “black boxes,” making it difficult to understand their inner workings and posing interpretability challenges. But this can be balanced against the overall benefits of high accuracy and scalability.

CNNs
Convolutional neural networks (CNNs or ConvNets) are used primarily in computer vision and image classification applications. They can detect features and patterns within images and videos, enabling tasks such as object detection, image recognition, pattern recognition and face recognition. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.

CNNs are a specific type of neural network, which is composed of node layers, containing an input layer, one or more hidden layers and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.

At least three main types of layers make up a CNN: a convolutional layer, pooling layer and fully connected (FC) layer. For complex uses, a CNN might contain up to thousands of layers, each layer building on the previous layers. By “convolution” working and reworking the original input detailed patterns can be discovered. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.

CNNs are distinguished from other neural networks by their superior performance with image, speech or audio signal inputs. Before CNNs, manual and time-consuming feature extraction methods were used to identify objects in images. However, CNNs now provide a more scalable approach to image classification and object recognition tasks, and process high-dimensional data. And CNNs can exchange data between layers, to deliver more efficient data processing. While information might be lost in the pooling layer, this might be outweighed by the benefits of CNNs, which can help to reduce complexity, improve efficiency and limit risk of overfitting.

There are other disadvantages to CNNs, which are computationally demanding costing time and budget, requiring many graphical processing units (GPUs). They also require highly trained experts with cross-domain knowledge, and careful testing of configurations, hyperparameters and configurations.

RNNs
Recurrent neural networks (RNNs) are typically used in natural language and speech recognition applications as they use sequential or time-series data. RNNs can be identified by their feedback loops. These learning algorithms are primarily used when using time-series data to make predictions about future outcomes. Use cases include stock market predictions or sales forecasting, or ordinal or temporal problems, such as language translation, natural language processing (NLP), speech recognition and image captioning. These functions are often incorporated into popular applications such as Siri, voice search and Google Translate.

RNNs use their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of RNNs depends on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions.

RNNs share parameters across each layer of the network and share the same weight parameter within each layer of the network, with the weights adjusted through the processes of backpropagation and gradient descent to facilitate reinforcement learning.

RNNs use a backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. BPTT differs from the traditional approach in that BPTT sums errors at each time step, whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.

An advantage over other neural network types is that RNNs use both binary data processing and memory. RNNs can plan out multiple inputs and productions so that rather than delivering only one result for a single input, RNNs can produce one-to-many, many-to-one or many-to-many outputs.

There are also options within RNNs. For example, the long short-term memory (LSTM) network is superior to simple RNNs by learning and acting on longer-term dependencies.

However, RNNs tend to run into two basic problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve.

When the gradient is vanishing and is too small, it continues to become smaller, updating the weight parameters until they become insignificant, that is: zero (0). When that occurs, the algorithm is no longer learning.
Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights grow too large, and they will eventually be represented as NaN (not a number). One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN models.
Some final disadvantages: RNNs might also require long training time and be difficult to use on large datasets. Optimizing RNNs add complexity when they have many layers and parameters.

Autoencoders and variational autoencoders
Deep learning made it possible to move beyond the analysis of numerical data, by adding the analysis of images, speech and other complex data types. Among the first class of models to achieve this were variational autoencoders (VAEs). They were the first deep-learning models to be widely used for generating realistic images and speech, which empowered deep generative modeling by making models easier to scale, which is the cornerstone of what we think of as generative AI.

Autoencoders work by encoding unlabeled data into a compressed representation, and then decoding the data back into its original form. Plain autoencoders were used for a variety of purposes, including reconstructing corrupted or blurry images. Variational autoencoders added the critical ability not just to reconstruct data, but also to output variations on the original data.

This ability to generate novel data ignited a rapid-fire succession of new technologies, from generative adversarial networks (GANs) to diffusion models, capable of producing ever more realistic but fake images. In this way, VAEs set the stage for today’s generative AI.

Autoencoders are built out of blocks of encoders and decoders, an architecture that also underpins today’s large language models. Encoders compress a dataset into a dense representation, arranging similar data points closer together in an abstract space. Decoders sample from this space to create something new while preserving the dataset’s most important features.

The biggest advantage to autoencoders is the ability to handle large batches of data and show input data in a compressed form, so the most significant aspects stand out, enabling anomaly detection and classification tasks. This also speeds transmission and reduces storage requirements. Autoencoders can be trained on unlabeled data so they might be used where labeled data is not available. When unsupervised training is used, there is a time savings advantage: deep learning algorithms learn automatically and gain accuracy without needing manual feature engineering. In addition, VAEs can generate new sample data for text or image generation.

There are disadvantages to autoencoders. The training of deep or intricate structures can be a drain on computational resources. And during unsupervised training, the model might overlook the needed properties and instead simply replicate the input data. Autoencoders might also overlook complex data linkages in structured data so that it does not correctly identify complex relationships.

GANs
Generative adversarial networks (GANs) are neural networks that are used both in and outside of artificial intelligence (AI) to create new data resembling the original training data. These can include images appearing to be human faces but are generated, not taken of real people. The “adversarial” part of the name comes from the back-and-forth between the two portions of the GAN: a generator and a discriminator.

The generator creates something: images, video or audio and then producing an output with a twist. For example, a horse can be transformed into a zebra with some degree of accuracy. The result depends on the input and how well-trained the layers are in the generative model for this use case.
The discriminator is the adversary, where the generative result (fake image) is compared against the real images in the dataset. The discriminator tries to distinguish between the real and fake images, video or audio.
GANs train themselves. The generator creates fakes while the discriminator learns to spot the differences between the generator's fakes and the true examples. When the discriminator is able to flag the fake, then the generator is penalized. The feedback loop continues until the generator succeeds in producing output that the discriminator cannot distinguish.

The prime GAN benefit is creating realistic output that can be difficult to distinguish from the originals, which in turn may be used to further train machine learning models. Setting up a GAN to learn is straightforward, since they are trained by using unlabeled data or with minor labeling. However, the potential disadvantage is that the generator and discriminator might go back-and-forth in competition for a long time, creating a large system drain. One training limitation is that a huge amount of input data might be required to obtain a satisfactory output. Another potential problem is “mode collapse,” when the generator produces a limited set of outputs rather than a wider variety.

Diffusion models
Diffusion models are generative models that are trained using the forward and reverse diffusion process of progressive noise-addition and denoising. Diffusion models generate data, most often images similar to the data on which they are trained, but then overwrite the data used to train them. They gradually add Gaussian noise to the training data until it’s unrecognizable, then learn a reversed “denoising” process that can synthesize output (usually images) from random noise input.

A diffusion model learns to minimize the differences of the generated samples versus the desired target. Any discrepancy is quantified and the model's parameters are updated to minimize the loss training the model to produce samples closely resembling the authentic training data.

Beyond image quality, diffusion models have the advantage of not requiring adversarial training, which speeds the learning process and also offering close process control. Training is more stable than with GANs and diffusion models are not as prone to mode collapse.

But, compared to GANs, diffusion models can require more computing resources to train, including more fine-tuning. IBM Research® has also discovered that this form of generative AI can be hijacked with hidden backdoors, giving attackers control over the image creation process so that AI diffusion models can be tricked into generating manipulated images.

Transformer models
Transformer models combine an encoder-decoder architecture with a text-processing mechanism and have revolutionized how language models are trained. An encoder converts raw, unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence.

Using fill-in-the-blank guessing, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without having to label parts of speech and other grammatical features. Transformers, in fact, can be pretrained at the outset without a particular task in mind. After these powerful representations are learned, the models can later be specialized with much less data to perform a requested task.

Several innovations make this possible. Transformers process words in a sentence simultaneously, enabling text processing in parallel, speeding up training. Earlier techniques including recurrent neural networks (RNNs) processed words one by one. Transformers also learned the positions of words and their relationships, this context enables them to infer meaning and disambiguate words such as “it” in long sentences.

By eliminating the need to define a task upfront, transformers made it practical to pretrain language models on vast amounts of raw text, enabling them to grow dramatically in size. Previously, labeled data was gathered to train one model on a specific task. With transformers, one model trained on a massive amount of data can be adapted to multiple tasks by fine-tuning it on a small amount of labeled task-specific data.

Language transformers today are used for nongenerative tasks such as classification and entity extraction as well as generative tasks including machine translation, summarization and question answering. Transformers have surprised many people with their ability to generate convincing dialog, essays and other content.

Natural language processing (NLP) transformers provide remarkable power since they can run in parallel, processing multiple portions of a sequence simultaneously, which then greatly speeds training. Transformers also track long-term dependencies in text, which enables them to understand the overall context more clearly and create superior output. In addition, transformers are more scalable and flexible in order to be customized by task.

As to limitations, because of their complexity, transformers require huge computational resources and a long training time. Also, the training data must be accurately on-target, unbiased and plentiful to produce accurate results.

Deep learning use cases
The number of uses for deep learning grows every day. Here are just a few of the ways that it is now helping businesses become more efficient and better serve their customers.

Application modernization
Generative AI can enhance the capabilities of developers and reduce the ever-widening skills gap in the domains of application modernization and IT automation. Generative AI for coding is possible because of recent breakthroughs in large language model (LLM) technologies and natural language processing (NLP). It uses deep learning algorithms and large neural networks trained on vast datasets of existing source code. Training code generally comes from publicly available code produced by open-source projects.

Programmers can enter plain text prompts describing what they want the code to do. Generative AI tools suggest code snippets or full functions, streamlining the coding process by handling repetitive tasks and reducing manual coding. Generative AI can also translate code from one language to another, streamlining code conversion or modernization projects, such as updating legacy applications by translating COBOL to Java.

Computer vision
Computer vision is a field of artificial intelligence (AI) that includes image classification, object detection and semantic segmentation. It uses machine learning and neural networks to teach computers and learning systems to derive meaningful information from digital images, videos and other visual inputs and to make recommendations or take actions when the system sees defects or issues. If AI enables computers to think, computer vision enables them to see, observe and understand.

Because a computer vision system is often trained to inspect products or watch production assets, it usually can analyze thousands of products or processes per minute, noticing imperceptible defects or issues. Computer vision is used in industries that range from energy and utilities to manufacturing and automotive.

Computer vision needs lots of data, and then it runs analyses of that data over and over until it discerns and ultimately recognizes images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects.

Computer vision uses algorithmic models to enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will “look” at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than with someone programming it to recognize an image.

Computer vision enables systems to derive meaningful information from digital images, videos and other visual inputs, and based on those inputs, to take action. This ability to provide recommendations distinguishes it from simple image recognition tasks. Some common applications of computer vision today can be seen in:

Automotive: While the age of driverless cars hasn’t entirely arrived, the underlying technology has started to make its way into automobiles, improving driver and passenger safety through features such as lane line detection.

Healthcare: Computer vision has been incorporated into radiology technology, enabling doctors to better identify cancerous tumors in healthy anatomy.

Marketing: Social media platforms provide suggestions on who might be in a photograph that has been posted on a profile, making it easier to tag friends in photo albums.

Retail: Visual search has been incorporated into some e-commerce platforms, enabling brands to recommend items that would complement an existing wardrobe.
Customer care
AI is helping businesses to better understand and cater to increasing consumer demands. With the rise of highly personalized online shopping, direct-to-consumer models, and delivery services, generative AI can help further unlock a host of benefits that can improve customer care, talent transformation and the performance of applications.

AI empowers businesses to adopt a customer-centric approach by harnessing valuable insights from customer feedback and buying habits. This data-driven approach can help improve product design and packaging and can help drive high customer satisfaction and increased sales.

Generative AI can also serve as a cognitive assistant for customer care, providing contextual guidance based on conversation history, sentiment analysis and call center transcripts. Also, generative AI can enable personalized shopping experiences, foster customer loyalty and provide a competitive advantage.

Digital labor
Organizations can augment their workforce by building and deploying robotic process automation (RPA) and digital labor to collaborate with humans to increase productivity, or assist whenever backup is needed. For example, this can help developers speed the updating of legacy software.

Digital labor uses foundation models to automate and improve the productivity of knowledge workers by enabling self-service automation in a fast and reliable way, without technical barriers. To automate task performance or calling APIs, an enterprise-grade LLM-based slot filling model can identify information in a conversation and gather all the information required for completing an action or calling an API without much manual effort.

Instead of having technical experts record and encode repetitive action flows for knowledge workers, digital labor automations built with a foundation of model-powered conversational instructions and demonstrations can be used by the knowledge worker for self-service automation. For example, to speed app creation, no-code digital apprentices can help end-users, who lack programming expertise, by effectively teaching, supervising and validating code.

Generative AI
Generative AI (also called gen AI) is a category of AI that autonomously creates text, images, video, data or other content in response to a user’s prompt or request.

Generative AI relies on deep learning models that can learn from patterns in existing content and generate new, similar content based on that training. It has applications in many fields including customer service, marketing, software development and research offers enormous potential to streamline enterprise workflows through fast, automated content creation and augmentation.

Generative AI excels at handling diverse data sources such as emails, images, videos, audio files and social media content. This unstructured data forms the backbone for creating models and the ongoing training of generative AI, so it can stay effective over time. Using this unstructured data can enhance customer service through chatbots and facilitate more effective email routing. In practice, this might mean guiding users to appropriate resources, whether that’s connecting them with the right agent or directing them to user guides and FAQs.

Despite its much-discussed limitations and risks, many businesses are forging ahead, cautiously exploring how their organizations can harness generative AI to improve their internal workflows, and enhance their products and services. This is the new frontier: How to make the workplace more efficient without creating legal or ethical issues.

AI agents and agentic AI
An AI agent is an autonomous AI program—it can perform tasks and accomplish goals on behalf of a user or another system without human intervention, by designing its own workflow and using available tools (other applications or services).

Agentic AI is a system of multiple AI agents, the efforts of which are coordinated, or orchestrated, to accomplish a more complex task or a greater goal than any single agent in the system could accomplish.

Unlike chatbots and other AI models which operate within predefined constraints and require human intervention, AI agents and agentic AI exhibit autonomy, goal-driven behavior and adaptability to changing circumstances. The terms “agent” and “agentic” refer to these models’ agency, or their capacity to act independently and purposefully.

One way to think of agents is as a natural next step after generative AI. Gen AI models focus on creating content based on learned patterns; agents use that content to interact with each other and other tools to make decisions, solve problems and complete tasks. For example, a gen AI app might be able to tell you the best time to climb Mt. Everest given your work schedule, but an agent can tell you this, and then use an online travel service to book you the best flight and reserve a room in the most convenient hotel in Nepal.

Explore our 2025 guide to AI agents 
Natural language processing and speech recognition
NLP combines computational linguistics rule-based modeling of human language with statistical and machine learning models to enable computers and digital devices to recognize, understand and generate text and speech. NLP powers applications and devices that can translate text from one language to another, respond to typed or spoken commands, recognize or authenticate users based on voice. It helps summarize large volumes of text, assess the intent or sentiment of text or speech and generate text or graphics or other content on demand.

A subset of NLP is statistical NLP, which combines computer algorithms with machine learning and deep learning models. This approach helps to automatically extract, classify and label elements of text and voice data and then assign a statistical likelihood to each possible meaning of those elements. Today, deep learning models and learning techniques based on RNNs enable NLP systems that “learn” as they work and extract ever more accurate meaning from huge volumes of raw, unstructured and unlabeled text and voice datasets.

Speech recognition: also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text is a capability that enables a program to process human speech into a written format.

While speech recognition is commonly confused with voice recognition, speech recognition focuses on the translation of speech from a verbal format to a text one whereas voice recognition just seeks to identify an individual user’s voice.

Industry applications
Real-world deep learning applications are all around us, and so well integrated into products and services that users are unaware of the complex data processing that is taking place in the background. Some of these examples include:

Customer service deep learning
Many organizations incorporate deep learning technology into their customer service processes. Chatbots are often used in various applications, services and customer service portals. Traditional chatbots use natural language and even visual recognition, commonly found in call center-like menus. However, more sophisticated chatbot solutions attempt to determine, through learning, if there are multiple responses to ambiguous questions in real time. Based on the responses it receives, the chatbot then tries to answer these questions directly or routes the conversation to a human user.

Virtual assistants such as Apple's Siri, Amazon Alexa or Google Assistant extend the idea of a chatbot by enabling speech recognition functionality. This creates a new method to engage users in a personalized way.

Financial services analytics
Financial institutions regularly use predictive analytics to drive algorithmic trading of stocks, assess business risks for loan approvals, detect fraud, and help manage credit and investment portfolios for clients.

Healthcare record-keeping
The healthcare industry has benefited greatly from deep learning capabilities ever since the digitization of hospital records and images. Image recognition applications can support medical imaging specialists and radiologists, helping them analyze and assess more images in less time.

Law enforcement uses deep learning
Deep learning algorithms can analyze and learn from transactional data to identify dangerous patterns that indicate possible fraudulent or criminal activity. Speech recognition, computer vision and other deep learning applications can improve the efficiency and effectiveness of investigative analysis by extracting patterns and evidence from sound and video recordings, images and documents. This capability helps law enforcement analyze large amounts of data more quickly and accurately.


Artificial Neural Networks and its Applications
Last Updated : 04 Jul, 2025
Artificial Neural Networks (ANNs) are computer systems designed to mimic how the human brain processes information. Just like the brain uses neurons to process data and make decisions, ANNs use artificial neurons to analyze data, identify patterns and make predictions. These networks consist of layers of interconnected neurons that work together to solve complex problems. The key idea is that ANNs can "learn" from the data they process, just as our brain learns from experience. They are used in various applications from recognizing images to making personalized recommendations. In this article, we will see more about ANNs, how they function and other core concepts.

1-.webp1-.webp
Key Components of an ANN
Input Layer: This is where the network receives information. For example, in an image recognition task, the input could be an image.
Hidden Layers: These layers process the data received from the input layer. The more hidden layers there are, the more complex patterns the network can learn and understand. Each hidden layer transforms the data into more abstract information.
Output Layer: This is where the final decision or prediction is made. For example, after processing an image, the output layer might decide whether it’s a cat or a dog.
Neural Networks Architecture
Neural Networks Architecture
Working of Artificial Neural Networks
ANNs work by learning patterns in data through a process called training. During training, the network adjusts itself to improve its accuracy by comparing its predictions with the actual results.

Lets see how the learning process works:

Input Layer: Data such as an image, text or number is fed into the network through the input layer.
Hidden Layers: Each neuron in the hidden layers performs some calculation on the input, passing the result to the next layer. The data is transformed and abstracted at each layer.
Output Layer: After passing through all the layers, the network gives its final prediction like classifying an image as a cat or a dog.
The process of backpropagation is used to adjust the weights between neurons. When the network makes a mistake, the weights are updated to reduce the error and improve the next prediction.

Training and Testing:
During training, the network is shown examples like images of cats and learns to recognize patterns in them.
After training, the network is tested on new data to check its performance. The better the network is trained, the more accurately it will predict new data.
How do Artificial Neural Networks learn?
Artificial Neural Networks (ANNs) learn by training on a set of data. For example, to teach an ANN to recognize a cat, we show it thousands of images of cats. The network processes these images and learns to identify the features that define a cat.
Once the network has been trained, we test it by providing new images to see if it can correctly identify cats. The network’s prediction is then compared to the actual label (whether it's a cat or not). If it makes an incorrect prediction, the network adjusts by fine-tuning the weights of the connections between neurons using a process called backpropagation. This involves correcting the weights based on the difference between the predicted and actual result.
This process repeats until the network can accurately recognize a cat in an image with minimal error. Essentially, through constant training and feedback, the network becomes better at identifying patterns and making predictions.
Artificial neurons vs Biological neurons
Aspect	Biological Neurons	Artificial Neurons
Structure	Dendrites: Receive signals from other neurons.	Input Nodes: Receive data and pass it on to the next layer.
Cell Body (Soma): Processes the signals.	Hidden Layer Nodes: Process and transform the data.
Axon: Transmits processed signals to other neurons.	Output Nodes: Produce the final result after processing.
Connections	Synapses: Links between neurons that transmit signals.	Weights: Connections between neurons that control the influence of one neuron on another.
Learning Mechanism	Synaptic Plasticity: Changes in synaptic strength based on activity over time.	Backpropagation: Adjusts the weights based on errors in predictions to improve future performance.
Activation	Activation: Neurons fire when signals are strong enough to reach a threshold.	Activation Function: Maps input to output, deciding if the neuron should fire based on the processed data.
Biological neurons to Artificial neurons - Geeksforgeeks
Biological neurons to Artificial neurons
Common Activation Functions in ANNs
Activation functions are important in neural networks because they introduce non-linearity and helps the network to learn complex patterns. Lets see some common activation functions used in ANNs:

Sigmoid Function: Outputs values between 0 and 1. It is used in binary classification tasks like deciding if an image is a cat or not.
ReLU (Rectified Linear Unit): A popular choice for hidden layers, it returns the input if positive and zero otherwise. It helps to solve the vanishing gradient problem.
Tanh (Hyperbolic Tangent): Similar to sigmoid but outputs values between -1 and 1. It is used in hidden layers when a broader range of outputs is needed.
Softmax: Converts raw outputs into probabilities used in the final layer of a network for multi-class classification tasks.
Leaky ReLU: A variant of ReLU that allows small negative values for inputs helps in preventing “dead neurons” during training.
These functions help the network decide whether to activate a neuron helps it to recognize patterns and make predictions.

For more details refer to Types of Activation Functions

Types of Artificial Neural Networks
1. Feedforward Neural Network (FNN)
Feedforward Neural Networks are one of the simplest types of ANNs. In this network, data flows in one direction from the input layer to the output layer, passing through one or more hidden layers. There are no loops or cycles means the data doesn’t return to any earlier layers. This type of network does not use backpropagation and is mainly used for basic classification and regression tasks.

2. Convolutional Neural Network (CNN)
Convolutional Neural Networks (CNNs) are designed to process data that has a grid-like structure such as images. It include convolutional layers that apply filters to extract important features from the data such as edges or textures. This makes CNNs effective in image and speech recognition as they can identify patterns and structures in complex data.

3. Radial Basis Function Network (RBFN)
Radial Basis Function Networks are designed to work with data that can be modeled in a radial or circular way. These networks consist of two layers: one that maps input to radial basis functions and another that finds the output. They are used for classification and regression tasks especially when the data represents an underlying pattern or trend.

4. Recurrent Neural Network (RNN)
Recurrent Neural Networks are designed to handle sequential data such as time-series or text. Unlike other networks, RNNs have feedback loops that allow information to be passed back into previous layers, giving the network memory. This feature helps RNNs to make predictions based on the context provided by previous data helps in making them ideal for tasks like speech recognition, language modeling and forecasting.

Optimization Algorithms in ANN Training
Optimization algorithms adjust the weights of a neural network during training to minimize errors. The goal is to make the network’s predictions more accurate. Lets see key algorithms:

Gradient Descent: Most basic optimization algorithm that updates weights by calculating the gradient of the loss function.
Adam (Adaptive Moment Estimation): An efficient version of gradient descent that adapts learning rates for each weight used in deep learning.
RMSprop: A variation of gradient descent that adjusts the learning rate based on the average of recent gradients, it is useful in training recurrent neural networks (RNNs).
Stochastic Gradient Descent (SGD): Updates weights using one sample at a time helps in making it faster but more noisy.
For more details refer to Optimization Algorithms in ANN

Applications of Artificial Neural Networks
Social Media: ANNs help social media platforms suggest friends and relevant content by analyzing user profiles, interests and interactions. They also assist in targeted advertising which ensures users to see ads tailored to their preferences.
Marketing and Sales: E-commerce sites like Amazon use ANNs to recommend products based on browsing history. They also personalize offers, predict customer behavior and segment customers for more effective marketing campaigns.
Healthcare: ANNs are used in medical imaging for detecting diseases like cancer and they assist in diagnosing conditions with accuracy similar to doctors. Additionally, they predict health risks and recommend personalized treatment plans.
Personal Assistants: Virtual assistants like Siri and Alexa use ANNs to process natural language, understand voice commands and respond accordingly. They help manage tasks like setting reminders helps in making calls and answering queries.
Customer Support: ANNs power chatbots and automated customer service systems that analyze customer queries and provide accurate responses helps in improving efficiency in handling customer inquiries.
Finance: In the financial industry, they are used for fraud detection, credit scoring and predicting market trends by analyzing large sets of transaction data and spotting anomalies.
Challenges in Artificial Neural Networks
Data Dependency: ANNs require large amounts of high-quality data to train effectively. Gathering and cleaning sufficient data can be time-consuming, expensive and often impractical especially in industries with limited access to quality data.
Computational Power: Training deep neural networks with many layers, demands significant computational resources. High-performance hardware (e.g GPUs) is often required which makes it expensive and resource-intensive.
Overfitting: It can easily overfit to the training data which means they perform well on the training set but poorly on new, unseen data. This challenge arises when the model learns to memorize rather than generalize, reducing its real-world applicability.
Interpretability: They are often referred to as "black boxes." It is difficult to understand how they make decisions which is a problem in fields like healthcare and finance where explainability and transparency are important.
Training Time: Training ANNs can take a long time, especially for deep learning models with many layers and vast datasets. This lengthy training process can delay the deployment of models and hinder their use in time-sensitive applications.


Convolutional Neural Network (CNN) in Machine Learning
Last Updated : 29 May, 2025
Convolutional Neural Networks (CNNs) are deep learning models designed to process data with a grid-like topology such as images. They are the foundation for most modern computer vision applications to detect features within visual data.

Introduction-to-Convolutional-Neural-Networks_.webpIntroduction-to-Convolutional-Neural-Networks_.webp
Key Components of a Convolutional Neural Network
Convolutional Layers: These layers apply convolutional operations to input images using filters or kernels to detect features such as edges, textures and more complex patterns. Convolutional operations help preserve the spatial relationships between pixels.
Pooling Layers: They downsample the spatial dimensions of the input, reducing the computational complexity and the number of parameters in the network. Max pooling is a common pooling operation where we select a maximum value from a group of neighboring pixels.
Activation Functions: They introduce non-linearity to the model by allowing it to learn more complex relationships in the data.
Fully Connected Layers: These layers are responsible for making predictions based on the high-level features learned by the previous layers. They connect every neuron in one layer to every neuron in the next layer.
How CNNs Work?
Input Image: CNN receives an input image which is preprocessed to ensure uniformity in size and format.
Convolutional Layers: Filters are applied to the input image to extract features like edges, textures and shapes.
Pooling Layers: The feature maps generated by the convolutional layers are downsampled to reduce dimensionality.
Fully Connected Layers: The downsampled feature maps are passed through fully connected layers to produce the final output, such as a classification label.
Output: The CNN outputs a prediction, such as the class of the image.
Working-of-CNN_
Working of CNN Models
How to Train a Convolutional Neural Network?
CNNs are trained using a supervised learning approach. This means that the CNN is given a set of labeled training images. The CNN learns to map the input images to their correct labels.

The training process for a CNN involves the following steps:

Data Preparation: The training images are preprocessed to ensure that they are all in the same format and size.
Loss Function: A loss function is used to measure how well the CNN is performing on the training data. The loss function is typically calculated by taking the difference between the predicted labels and the actual labels of the training images.
Optimizer: An optimizer is used to update the weights of the CNN in order to minimize the loss function.
Backpropagation: Backpropagation is a technique used to calculate the gradients of the loss function with respect to the weights of the CNN. The gradients are then used to update the weights of the CNN using the optimizer.
How to Evaluate CNN Models
Efficiency of CNN can be evaluated using a variety of criteria. Among the most popular metrics are:

Accuracy: Accuracy is the percentage of test images that the CNN correctly classifies.
Precision: Precision is the percentage of test images that the CNN predicts as a particular class and that are actually of that class.
Recall: Recall is the percentage of test images that are of a particular class and that the CNN predicts as that class.
F1 Score: The F1 Score is a harmonic mean of precision and recall. It is a good metric for evaluating the performance of a CNN on classes that are imbalanced.
Case Study of CNN for Diabetic retinopathy
Diabetic retinopathy is a severe eye condition caused by damage to the retina's blood vessels due to prolonged diabetes. It is a leading cause of blindness among adults aged 20 to 64. CNNs have successfully used to detect diabetic retinopathy by analyzing retinal images. By training on labeled datasets of healthy and affected retina images CNNs can accurately identify signs of the disease helping in early diagnosis and treatment.

Different Types of CNN Models
1. LeNet
LeNet developed by Yann LeCun and his colleagues in the late 1990s was one of the first successful CNNs designed for handwritten digit recognition. It laid the foundation for modern CNNs and achieved high accuracy on the MNIST dataset which contains 70,000 images of handwritten digits (0-9).

2. AlexNet
AlexNet is a CNN architecture that was developed by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton in 2012. It was the first CNN to win the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) a major image recognition competition. It consists of several layers of convolutional and pooling layers followed by fully connected layers. The architecture includes five convolutional layers, three pooling layers and three fully connected layers.

3. Resnet
ResNets (Residual Networks) are designed for image recognition and processing tasks. They are renowned for their ability to train very deep networks without overfitting making them highly effective for complex tasks. It introduces skip connections that allow the network to learn residual functions making it easier to train deep architecture.

4. GoogleNet
GoogleNet also known as InceptionNet is renowned for achieving high accuracy in image classification while using fewer parameters and computational resources compared to other state-of-the-art CNNs. The core component of GoogleNet allows the network to learn features at different scales simultaneously to enhance performance.

5. VGG
VGGs are developed by the Visual Geometry Group at Oxford, it uses small 3x3 convolutional filters stacked in multiple layers, creating a deep and uniform structure. Popular variants like VGG-16 and VGG-19 achieved state-of-the-art performance on the ImageNet dataset demonstrating the power of depth in CNNs.

Applications of CNN
Image classification: CNNs are the state-of-the-art models for image classification. They can be used to classify images into different categories such as cats and dogs.
Object detection: It can be used to detect objects in images such as people, cars and buildings. They can also be used to localize objects in images which means that they can identify the location of an object in an image.
Image segmentation: It can be used to segment images which means that they can identify and label different objects in an image. This is useful for applications such as medical imaging and robotics.
Video analysis: It can be used to analyze videos such as tracking objects in a video or detecting events in a video. This is useful for applications such as video surveillance and traffic monitoring.
Advantages of CNN
High Accuracy: They can achieve high accuracy in various image recognition tasks.
Efficiency: They are efficient, especially when implemented on GPUs.
Robustness: They are robust to noise and variations in input data.
Adaptability: It can be adapted to different tasks by modifying their architecture.
Disadvantages of CNN
Complexity: It can be complex and difficult to train, especially for large datasets.
Resource-Intensive: It require significant computational resources for training and deployment.
Data Requirements: They need large amounts of labeled data for training.
Interpretability: They can be difficult to interpret making it challenging to understand their predictions.
Introduction to Recurrent Neural Networks
Last Updated : 01 Jul, 2025
Recurrent Neural Networks (RNNs) differ from regular neural networks in how they process information. While standard neural networks pass information in one direction i.e from input to output, RNNs feed information back into the network at each step.

introduction_to_recurrent_neural_network.webpintroduction_to_recurrent_neural_network.webp
Lets understand RNN with a example:

Imagine reading a sentence and you try to predict the next word, you don’t rely only on the current word but also remember the words that came before. RNNs work similarly by “remembering” past information and passing the output from one step as input to the next i.e it considers all the earlier words to choose the most likely next word. This memory of previous steps helps the network understand context and make better predictions.

Key Components of RNNs
There are mainly two components of RNNs that we will discuss.

1. Recurrent Neurons
The fundamental processing unit in RNN is a Recurrent Unit. They hold a hidden state that maintains information about previous inputs in a sequence. Recurrent units can "remember" information from prior steps by feeding back their hidden state, allowing them to capture dependencies across time.

recurrent-neuron
Recurrent Neuron
2. RNN Unfolding
RNN unfolding or unrolling is the process of expanding the recurrent structure over time steps. During unfolding each step of the sequence is represented as a separate layer in a series illustrating how information flows across each time step.

This unrolling enables backpropagation through time (BPTT) a learning process where errors are propagated across time steps to adjust the network’s weights enhancing the RNN’s ability to learn dependencies within sequential data.

Unfolding-660
RNN Unfolding
Recurrent Neural Network Architecture
RNNs share similarities in input and output structures with other deep learning architectures but differ significantly in how information flows from input to output. Unlike traditional deep neural networks where each dense layer has distinct weight matrices. RNNs use shared weights across time steps, allowing them to remember information over sequences.

In RNNs the hidden state 
H
i
H 
i
​
 ​ is calculated for every input 
X
i
X 
i
​
 ​ to retain sequential dependencies. The computations follow these core formulas:

1. Hidden State Calculation:

h
=
σ
(
U
⋅
X
+
W
⋅
h
t
−
1
+
B
)
h=σ(U⋅X+W⋅h 
t−1
​
 +B) 

Here:

h
h represents the current hidden state.
U
U and 
W
W are weight matrices.
B
B is the bias.
2. Output Calculation:

Y
=
O
(
V
⋅
h
+
C
)
Y=O(V⋅h+C) 

The output 
Y
Y is calculated by applying 
O
O an activation function to the weighted hidden state where 
V
V and 
C
C represent weights and bias.

3. Overall Function:

Y
=
f
(
X
,
h
,
W
,
U
,
V
,
B
,
C
)
Y=f(X,h,W,U,V,B,C) 

This function defines the entire RNN operation where the state matrix 
S
S holds each element 
s
i
s 
i
​
  representing the network's state at each time step 
i
i.

recurrent_neural_networks
Recurrent Neural Architecture
How does RNN work?
At each time step RNNs process units with a fixed activation function. These units have an internal hidden state that acts as memory that retains information from previous time steps. This memory allows the network to store past knowledge and adapt based on new inputs.

Updating the Hidden State in RNNs
The current hidden state 
h
t
h 
t
​
 ​ depends on the previous state 
h
t
−
1
h 
t−1
​
 ​ and the current input 
x
t
x 
t
​
 ​ and is calculated using the following relations:

1. State Update:

h
t
=
f
(
h
t
−
1
,
x
t
)
h 
t
​
 =f(h 
t−1
​
 ,x 
t
​
 ) 

where:

h
t
h 
t
​
 ​ is the current state
h
t
−
1
h 
t−1
​
 ​ is the previous state
x
t
x 
t
​
  is the input at the current time step
2. Activation Function Application:

h
t
=
tanh
⁡
(
W
h
h
⋅
h
t
−
1
+
W
x
h
⋅
x
t
)
h 
t
​
 =tanh(W 
hh
​
 ⋅h 
t−1
​
 +W 
xh
​
 ⋅x 
t
​
 )

Here, 
W
h
h
W 
hh
​
 ​ is the weight matrix for the recurrent neuron and 
W
x
h
W 
xh
​
 ​ is the weight matrix for the input neuron.

3. Output Calculation:

y
t
=
W
h
y
⋅
h
t
y 
t
​
 =W 
hy
​
 ⋅h 
t
​
  

where 
y
t
y 
t
​
 ​ is the output and 
W
h
y
W 
hy
​
 ​ is the weight at the output layer.

These parameters are updated using backpropagation. However, since RNN works on sequential data here we use an updated backpropagation which is known as backpropagation through time. 

Backpropagation Through Time (BPTT) in RNNs
Since RNNs process sequential data Backpropagation Through Time (BPTT) is used to update the network's parameters. The loss function L(θ) depends on the final hidden state 
h
3
h 
3
​
  and each hidden state relies on preceding ones forming a sequential dependency chain:

h
3
h 
3
​
  depends on 
 depends on 
h
2
,
h
2
 depends on 
h
1
,
…
,
h
1
 depends on 
h
0
 depends on h 
2
​
 ,h 
2
​
  depends on h 
1
​
 ,…,h 
1
​
  depends on h 
0
​
 ​.

Backpropagation-Through-Time-(BPTT)
Backpropagation Through Time (BPTT) In RNN
In BPTT, gradients are backpropagated through each time step. This is essential for updating network parameters based on temporal dependencies.

1. Simplified Gradient Calculation:

∂
L
(
θ
)
∂
W
=
∂
L
(
θ
)
∂
h
3
⋅
∂
h
3
∂
W
∂W
∂L(θ)
​
 = 
∂h 
3
​
 
∂L(θ)
​
 ⋅ 
∂W
∂h 
3
​
 
​
  

2. Handling Dependencies in Layers: Each hidden state is updated based on its dependencies:

h
3
=
σ
(
W
⋅
h
2
+
b
)
h 
3
​
 =σ(W⋅h 
2
​
 +b) 

The gradient is then calculated for each state, considering dependencies from previous hidden states.

3. Gradient Calculation with Explicit and Implicit Parts: The gradient is broken down into explicit and implicit parts summing up the indirect paths from each hidden state to the weights.

∂
h
3
∂
W
=
∂
h
3
+
∂
W
+
∂
h
3
∂
h
2
⋅
∂
h
2
+
∂
W
∂W
∂h 
3
​
 
​
 = 
∂W
∂h 
3
+
​
 
​
 + 
∂h 
2
​
 
∂h 
3
​
 
​
 ⋅ 
∂W
∂h 
2
+
​
 
​
 

4. Final Gradient Expression: The final derivative of the loss function with respect to the weight matrix W is computed:

∂
L
(
θ
)
∂
W
=
∂
L
(
θ
)
∂
h
3
⋅
∑
k
=
1
3
∂
h
3
∂
h
k
⋅
∂
h
k
∂
W
∂W
∂L(θ)
​
 = 
∂h 
3
​
 
∂L(θ)
​
 ⋅∑ 
k=1
3
​
  
∂h 
k
​
 
∂h 
3
​
 
​
 ⋅ 
∂W
∂h 
k
​
 
​
 

This iterative process is the essence of backpropagation through time.

Types Of Recurrent Neural Networks
There are four types of RNNs based on the number of inputs and outputs in the network:

1. One-to-One RNN
This is the simplest type of neural network architecture where there is a single input and a single output. It is used for straightforward classification tasks such as binary classification where no sequential data is involved.

One-to-One
One to One RNN
2. One-to-Many RNN
In a One-to-Many RNN the network processes a single input to produce multiple outputs over time. This is useful in tasks where one input triggers a sequence of predictions (outputs). For example in image captioning a single image can be used as input to generate a sequence of words as a caption.

One-to-Many
One to Many RNN
3. Many-to-One RNN
The Many-to-One RNN receives a sequence of inputs and generates a single output. This type is useful when the overall context of the input sequence is needed to make one prediction. In sentiment analysis the model receives a sequence of words (like a sentence) and produces a single output like positive, negative or neutral.

Many-to-One
Many to One RNN
4. Many-to-Many RNN
The Many-to-Many RNN type processes a sequence of inputs and generates a sequence of outputs. In language translation task a sequence of words in one language is given as input and a corresponding sequence in another language is generated as output.

Many-to-Many
Many to Many RNN
Variants of Recurrent Neural Networks (RNNs)
There are several variations of RNNs, each designed to address specific challenges or optimize for certain tasks:

1. Vanilla RNN
This simplest form of RNN consists of a single hidden layer where weights are shared across time steps. Vanilla RNNs are suitable for learning short-term dependencies but are limited by the vanishing gradient problem, which hampers long-sequence learning.

2. Bidirectional RNNs
Bidirectional RNNs process inputs in both forward and backward directions, capturing both past and future context for each time step. This architecture is ideal for tasks where the entire sequence is available, such as named entity recognition and question answering.

3. Long Short-Term Memory Networks (LSTMs)
Long Short-Term Memory Networks (LSTMs) introduce a memory mechanism to overcome the vanishing gradient problem. Each LSTM cell has three gates:

Input Gate: Controls how much new information should be added to the cell state.
Forget Gate: Decides what past information should be discarded.
Output Gate: Regulates what information should be output at the current step. This selective memory enables LSTMs to handle long-term dependencies, making them ideal for tasks where earlier context is critical.
4. Gated Recurrent Units (GRUs)
Gated Recurrent Units (GRUs) simplify LSTMs by combining the input and forget gates into a single update gate and streamlining the output mechanism. This design is computationally efficient, often performing similarly to LSTMs and is useful in tasks where simplicity and faster training are beneficial.

How RNN Differs from Feedforward Neural Networks?
Feedforward Neural Networks (FNNs) process data in one direction from input to output without retaining information from previous inputs. This makes them suitable for tasks with independent inputs like image classification. However FNNs struggle with sequential data since they lack memory.

Recurrent Neural Networks (RNNs) solve this by incorporating loops that allow information from previous steps to be fed back into the network. This feedback enables RNNs to remember prior inputs making them ideal for tasks where context is important.

RNN-vs-FNN
Recurrent Vs Feedforward networks
Implementing a Text Generator Using Recurrent Neural Networks (RNNs)
In this section, we create a character-based text generator using Recurrent Neural Network (RNN) in TensorFlow and Keras. We'll implement an RNN that learns patterns from a text sequence to generate new text character-by-character.

1. Importing Necessary Libraries
We start by importing essential libraries for data handling and building the neural network.


import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
2. Defining the Input Text and Prepare Character Set
We define the input text and identify unique characters in the text which we’ll encode for our model.


text = "This is GeeksforGeeks a software training institute"
chars = sorted(list(set(text)))
char_to_index = {char: i for i, char in enumerate(chars)}
index_to_char = {i: char for i, char in enumerate(chars)}
3. Creating Sequences and Labels
To train the RNN, we need sequences of fixed length (seq_length) and the character following each sequence as the label.


seq_length = 3
sequences = []
labels = []

for i in range(len(text) - seq_length):
    seq = text[i:i + seq_length]
    label = text[i + seq_length]
    sequences.append([char_to_index[char] for char in seq])
    labels.append(char_to_index[label])

X = np.array(sequences)
y = np.array(labels)
4. Converting Sequences and Labels to One-Hot Encoding
For training we convert X and y into one-hot encoded tensors.


X_one_hot = tf.one_hot(X, len(chars))
y_one_hot = tf.one_hot(y, len(chars))

5. Building the RNN Model
We create a simple RNN model with a hidden layer of 50 units and a Dense output layer with softmax activation.


model = Sequential()
model.add(SimpleRNN(50, input_shape=(seq_length, len(chars)), activation='relu'))
model.add(Dense(len(chars), activation='softmax'))
6. Compiling and Training the Model
We compile the model using the categorical_crossentropy loss and train it for 100 epochs.


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_one_hot, y_one_hot, epochs=100)
Output:

training
Training the RNN model
7. Generating New Text Using the Trained Model
After training we use a starting sequence to generate new text character by character.


start_seq = "This is G"
generated_text = start_seq

for i in range(50):
    x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])
    x_one_hot = tf.one_hot(x, len(chars))
    prediction = model.predict(x_one_hot)
    next_index = np.argmax(prediction)
    next_char = index_to_char[next_index]
    generated_text += next_char

print("Generated Text:")
print(generated_text)
Output:

prediction
Predicting the next word
Advantages of Recurrent Neural Networks
Sequential Memory: RNNs retain information from previous inputs making them ideal for time-series predictions where past data is crucial.
Enhanced Pixel Neighborhoods: RNNs can be combined with convolutional layers to capture extended pixel neighborhoods improving performance in image and video data processing.
Limitations of Recurrent Neural Networks (RNNs)
While RNNs excel at handling sequential data they face two main training challenges i.e vanishing gradient and exploding gradient problem:

Vanishing Gradient: During backpropagation gradients diminish as they pass through each time step leading to minimal weight updates. This limits the RNN’s ability to learn long-term dependencies which is crucial for tasks like language translation.
Exploding Gradient: Sometimes gradients grow uncontrollably causing excessively large weight updates that de-stabilize training.
These challenges can hinder the performance of standard RNNs on complex, long-sequence tasks.

Applications of Recurrent Neural Networks
RNNs are used in various applications where data is sequential or time-based:

Time-Series Prediction: RNNs excel in forecasting tasks, such as stock market predictions and weather forecasting.
Natural Language Processing (NLP): RNNs are fundamental in NLP tasks like language modeling, sentiment analysis and machine translation.
Speech Recognition: RNNs capture temporal patterns in speech data, aiding in speech-to-text and other audio-related applications.
Image and Video Processing: When combined with convolutional layers, RNNs help analyze video sequences, facial expressions and gesture recognition.


What is a recurrent neural network?
A recurrent neural network or RNN is a deep neural network trained on sequential or time series data to create a machine learning (ML) model that can make sequential predictions or conclusions based on sequential inputs.

An RNN might be used to predict daily flood levels based on past daily flood, tide and meteorological data. But RNNs can also be used to solve ordinal or temporal problems such as language translation, natural language processing (NLP), sentiment analysis, speech recognition and image captioning.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

How RNNs work
Like traditional neural networks, such as feedforward neural networks and convolutional neural networks (CNNs), recurrent neural networks use training data to learn. They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output.

While traditional deep learning networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions.

Let’s take an idiom, such as “feeling under the weather,” which is commonly used when someone is ill to aid us in the explanation of RNNs. For the idiom to make sense, it needs to be expressed in that specific order. As a result, recurrent networks need to account for the position of each word in the idiom, and they use that information to predict the next word in the sequence.

Each word in the phrase "feeling under the weather" is part of a sequence, where the order matters. The RNN tracks the context by maintaining a hidden state at each time step. A feedback loop is created by passing the hidden state from one-time step to the next. The hidden state acts as a memory that stores information about previous inputs. At each time step, the RNN processes the current input (for example, a word in a sentence) along with the hidden state from the previous time step. This allows the RNN to "remember" previous data points and use that information to influence the current output.

Another distinguishing characteristic of recurrent networks is that they share parameters across each layer of the network. While feedforward networks have different weights across each node, recurrent neural networks share the same weight parameter within each layer of the network. That said, these weights are still adjusted through the processes of backpropagation and gradient descent to facilitate reinforcement learning.

Recurrent neural networks use forward propagation and backpropagation through time (BPTT) algorithms to determine the gradients (or derivatives), which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. These calculations allow us to adjust and fit the parameters of the model appropriately. BPTT differs from the traditional approach in that BPTT sums errors at each time step whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.

Three diagrams quickly explaining what Recurrent Neural Networks are
Feedforward vs recurrent neural networks

Common activation functions
An activation function is a mathematical function applied to the output of each layer of neurons in the network to introduce nonlinearity and allow the network to learn more complex patterns in the data. Without activation functions, the RNN would simply compute linear transformations of the input, making it incapable of handling nonlinear problems. Nonlinearity is crucial for learning and modeling complex patterns, particularly in tasks such as NLP, time-series analysis and sequential data prediction.

The activation function controls the magnitude of the neuron’s output, keeping values within a specified range (for example, between 0 and 1 or -1 and 1), which helps prevent values from growing too large or too small during the forward and backward passes. In RNNs, activation functions are applied at each time step to the hidden states, controlling how the network updates its internal memory (hidden state) based on current input and past hidden states.

Common activation functions (pictured after this) include:

The Sigmoid Function is to interpret the output as probabilities or to control gates that decide how much information to retain or forget. However, the sigmoid function is prone to the vanishing gradient problem (explained after this), which makes it less ideal for deeper networks.

The Tanh (Hyperbolic Tangent) Function, which is often used because it outputs values centered around zero, which helps with better gradient flow and easier learning of long-term dependencies.

The ReLU (Rectified Linear Unit) might cause issues with exploding gradients due to its unbounded nature. However, variants such as Leaky ReLU and Parametric ReLU have been used to mitigate some of these issues.

For a closer look at how RNNs work, view our recurrent neural networks deep dive.

Diagram of different activation functions used in RNNs 
Sigmoid, tanh and ReLU

Types of RNNs
Feedforward networks map inputs and outputs one-to-one, and while we’ve visualized recurrent neural networks in this way in the diagrams before this, they do not have this constraint. Instead, their inputs and outputs can vary in length, and different types of RNNs are used for different use cases, such as music generation, sentiment classification and machine translation. Popular recurrent neural network architecture variants include:

Standard RNNs
Bidirectional recurrent neural networks (BRRNs)
Long short-term memory (LSTM)
Gated recurrent units (GNUs)
Encoder-decoder RNN
Y and X values in a graph
Different types of RNNs

Standard RNNs
The most basic version of an RNN, where the output at each time step depends on both the current input and the hidden state from the previous time step, suffers from problems such as vanishing gradients, making it difficult for them to learn long-term dependencies. They excel in simple tasks with short-term dependencies, such as predicting the next word in a sentence (for short, simple sentences) or the next value in a simple time series.

RNNs are good for tasks that process data sequentially in real time, such as processing sensor data to detect anomalies in short time frames, where inputs are received one at a time and predictions need to be made immediately based on the most recent inputs.

Bidirectional recurrent neural networks (BRNNs)
While unidirectional RNNs can only be drawn from previous inputs to make predictions about the current state, bidirectional RNNs or BRNNs, pull in future data to improve the accuracy of it. Returning to the example of “feeling under the weather”, a model based on a BRNN can better predict that the second word in that phrase is “under” if it knows that the last word in the sequence is “weather.”

Long short-term memory (LSTM)
LSTM is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to the vanishing gradient problem. This work addressed the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model might not be able to accurately predict the current state.

As an example, let’s say we wanted to predict the italicized words in, “Alice is allergic to nuts. She can’t eat peanut butter.” The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult or even impossible for the RNN to connect the information.

To remedy this, LSTM networks have “cells” in the hidden layers of the artificial neural network, which have 3 gates: an input gate, an output gate and a forget gate. These gates control the flow of information that is needed to predict the output in the network. For example, if gender pronouns, such as “she”, was repeated multiple times in prior sentences, you might exclude that from the cell state.

Gated recurrent units (GRUs)
A GRU is similar to an LSTM as it also works to address the short-term memory problem of RNN models. Instead of using a “cell state” to regulate information, it uses hidden states, and instead of 3 gates, it has 2: a reset gate and an update gate. Similar to the gates within LSTMs, the reset and update gates control how much and which information to retain.

Because of its simpler architecture, GRUs are computationally more efficient and require fewer parameters compared to LSTMs. This makes them faster to train and often more suitable for certain real-time or resource-constrained applications.

Encoder-decoder RNNs
These are commonly used for sequence-to-sequence tasks, such as machine translation. The encoder processes the input sequence into a fixed-length vector (context), and the decoder uses that context to generate the output sequence. However, the fixed-length context vector can be a bottleneck, especially for long input sequences.

Limitations of RNNs
RNN use has declined in artificial intelligence, especially in favor of architectures such as transformer models, but RNNs are not obsolete. RNNs were traditionally popular for sequential data processing (for example, time series and language modeling) because of their ability to handle temporal dependencies.

However, RNNs’ weakness to the vanishing and exploding gradient problems, along with the rise of transformer models such as BERT and GPT have resulted in this decline. Transformers can capture long-range dependencies much more effectively, are easier to parallelize and perform better on tasks such as NLP, speech recognition and time-series forecasting.

That said, RNNs are still used in specific contexts where their sequential nature and memory mechanism can be useful, especially in smaller, resource-constrained environments or for tasks where data processing benefits from step-by-step recurrence.

For those who want to experiment with such use cases, Keras is a popular open source library, now integrated into the TensorFlow library, providing a Python interface for RNNs. The API is designed for ease of use and customization, enabling users to define their own RNN cell layer with custom behavior.

What is LSTM - Long Short Term Memory?
Last Updated : 28 May, 2025
Long Short-Term Memory (LSTM) is an enhanced version of the Recurrent Neural Network (RNN) designed by Hochreiter and Schmidhuber. LSTMs can capture long-term dependencies in sequential data making them ideal for tasks like language translation, speech recognition and time series forecasting.

Unlike traditional RNNs which use a single hidden state passed through time LSTMs introduce a memory cell that holds information over extended periods addressing the challenge of learning long-term dependencies.

introduction_to_lstms.webpintroduction_to_lstms.webp
Problem with Long-Term Dependencies in RNN
Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous time steps. However they often face challenges in learning long-term dependencies where information from distant time steps becomes crucial for making accurate predictions for current state. This problem is known as the vanishing gradient or exploding gradient problem.

Vanishing Gradient: When training a model over time, the gradients which help the model learn can shrink as they pass through many steps. This makes it hard for the model to learn long-term patterns since earlier information becomes almost irrelevant.
Exploding Gradient: Sometimes gradients can grow too large causing instability. This makes it difficult for the model to learn properly as the updates to the model become erratic and unpredictable.
Both of these issues make it challenging for standard RNNs to effectively capture long-term dependencies in sequential data.

LSTM Architecture
LSTM architectures involves the memory cell which is controlled by three gates:

Input gate: Controls what information is added to the memory cell.
Forget gate: Determines what information is removed from the memory cell.
Output gate: Controls what information is output from the memory cell.
This allows LSTM networks to selectively retain or discard information as it flows through the network which allows them to learn long-term dependencies. The network has a hidden state which is like its short-term memory. This memory is updated using the current input, the previous hidden state and the current state of the memory cell.

Working of LSTM
LSTM architecture has a chain structure that contains four neural networks and different memory blocks called cells.

gate_of_lstm
LSTM Model
Information is retained by the cells and the memory manipulations are done by the gates. There are three gates - 

1. Forget Gate
The information that is no longer useful in the cell state is removed with the forget gate. Two inputs 
x
t
x 
t
​
  (input at the particular time) and 
h
t
−
1
h 
t−1
​
  (previous cell output) are fed to the gate and multiplied with weight matrices followed by the addition of bias. The resultant is passed through an activation function which gives a binary output. If for a particular cell state the output is 0, the piece of information is forgotten and for output 1, the information is retained for future use. 

tanh is activation function
input_gate
Input Gate
3. Output gate
The task of extracting useful information from the current cell state to be presented as output is done by the output gate. First, a vector is generated by applying tanh function on the cell. Then, the information is regulated using the sigmoid function and filter by the values to be remembered using inputs
h
t
−
1
h 
t−1
​
 and 
x
t
x 
t
​
 . At last the values of the vector and the regulated values are multiplied to be sent as an output and input to the next cell. The equation for the output gate is:

o
t
=
σ
(
W
o
⋅
[
h
t
−
1
,
x
t
]
+
b
o
)
o 
t
​
 =σ(W 
o
​
 ⋅[h 
t−1
​
 ,x 
t
​
 ]+b 
o
​
 )

output_gate
Output Gate
Applications of LSTM
Some of the famous applications of LSTM includes:

Language Modeling: Used in tasks like language modeling, machine translation and text summarization. These networks learn the dependencies between words in a sentence to generate coherent and grammatically correct sentences.
Speech Recognition: Used in transcribing speech to text and recognizing spoken commands. By learning speech patterns they can match spoken words to corresponding text.
Time Series Forecasting: Used for predicting stock prices, weather and energy consumption. They learn patterns in time series data to predict future events.
Anomaly Detection: Used for detecting fraud or network intrusions. These networks can identify patterns in data that deviate drastically and flag them as potential anomalies.
Recommender Systems: In recommendation tasks like suggesting movies, music and books. They learn user behavior patterns to provide personalized suggestions.
Video Analysis: Applied in tasks such as object detection, activity recognition and action classification. When combined with Convolutional Neural Networks (CNNs) they help analyze video data and extract useful information.


Understanding of LSTM Networks
Last Updated : 05 Jun, 2023
This article talks about the problems of conventional RNNs, namely, the vanishing and exploding gradients, and provides a convenient solution to these problems in the form of Long Short Term Memory (LSTM). Long Short-Term Memory is an advanced version of recurrent neural network (RNN) architecture that was designed to model chronological sequences and their long-range dependencies more precisely than conventional RNNs. 

Introduction to LSTM
LSTM networks are an extension of recurrent neural networks (RNNs) mainly introduced to handle situations where RNNs fail. 

It fails to store information for a longer period of time. At times, a reference to certain information stored quite a long time ago is required to predict the current output. But RNNs are absolutely incapable of handling such “long-term dependencies”.
There is no finer control over which part of the context needs to be carried forward and how much of the past needs to be ‘forgotten’. 
Other issues with RNNs are exploding and vanishing gradients (explained later) which occur during the training process of a network through backtracking. 
Thus, Long Short-Term Memory (LSTM) was brought into the picture. It has been so designed that the vanishing gradient problem is almost completely removed, while the training model is left unaltered. Long-time lags in certain problems are bridged using LSTMs which also handle noise, distributed representations, and continuous values. With LSTMs, there is no need to keep a finite number of states from beforehand as required in the hidden Markov model (HMM). LSTMs provide us with a large range of parameters such as learning rates, and input and output biases.

Structure of LSTM
The basic difference between the architectures of RNNs and LSTMs is that the hidden layer of LSTM is a gated unit or gated cell. It consists of four layers that interact with one another in a way to produce the output of that cell along with the cell state. These two things are then passed onto the next hidden layer. Unlike RNNs which have got only a single neural net layer of tanh, LSTMs comprise three logistic sigmoid gates and one tanh layer. Gates have been introduced in order to limit the information that is passed through the cell. They determine which part of the information will be needed by the next cell and which part is to be discarded. The output is usually in the range of 0-1 where ‘0’ means ‘reject all’ and ‘1’ means ‘include all’.  

Structure of an LSTM Network
Structure of an LSTM Network
Information is retained by the cells and the memory manipulations are done by the gates. There are three gates which are explained below:

Forget Gate
The information that is no longer useful in the cell state is removed with the forget gate. Two inputs x_t (input at the particular time) and h_t-1 (previous cell output) are fed to the gate and multiplied with weight matrices followed by the addition of bias. The resultant is passed through an activation function which gives a binary output. If for a particular cell state, the output is 0, the piece of information is forgotten and for output 1, the information is retained for future use.

Forget Gate in LSTM Cell
Forget Gate in LSTM Cell
Input gate
The addition of useful information to the cell state is done by the input gate. First, the information is regulated using the sigmoid function and filter the values to be remembered similar to the forget gate using inputs h_t-1 and x_t. Then, a vector is created using the tanh function that gives an output from -1 to +1, which contains all the possible values from h_t-1 and x_t. At last, the values of the vector and the regulated values are multiplied to obtain useful information.

Input gate in the LSTM cell
Input gate in the LSTM cell
Output gate
The task of extracting useful information from the current cell state to be presented as output is done by the output gate. First, a vector is generated by applying the tanh function on the cell. Then, the information is regulated using the sigmoid function and filtered by the values to be remembered using inputs h_t-1 and x_t. At last, the values of the vector and the regulated values are multiplied to be sent as an output and input to the next cell.

Output gate in the LSTM cell
Output gate in the LSTM cell
Variations in LSTM Networks 
With the increasing popularity of LSTMs, various alterations have been tried on the conventional LSTM architecture to simplify the internal design of cells to make them work in a more efficient way and to reduce computational complexity. Gers and Schmidhuber introduced peephole connections which allowed gate layers to have knowledge about the cell state at every instant. Some LSTMs also made use of a coupled input and forget gate instead of two separate gates which helped in making both decisions simultaneously. Another variation was the use of the Gated Recurrent Unit(GRU) which improved the design complexity by reducing the number of gates. It uses a combination of the cell state and hidden state and also an update gate which has forgotten and input gates merged into it.

Applications of LSTM Networks 
LSTM models need to be trained with a training dataset prior to their employment in real-world applications. Some of the most demanding applications are discussed below:

Language modeling or text generation, involves the computation of words when a sequence of words is fed as input. Language models can be operated at the character level, n-gram level, sentence level, or even paragraph level.
Image processing involves performing an analysis of a picture and concluding its result into a sentence. For this, it’s required to have a dataset comprising a good amount of pictures with their corresponding descriptive captions. A model that has already been trained is used to predict features of images present in the dataset. This is photo data. The dataset is then processed in such a way that only the words that are most suggestive are present in it. This is text data. Using these two types of data, we try to fit the model. The work of the model is to generate a descriptive sentence for the picture one word at a time by taking input words that were predicted previously by the model and also the image.
Speech and Handwriting Recognition.
Music generation is quite similar to that of text generation where LSTMs predict musical notes instead of text by analyzing a combination of given notes fed as input.
Language Translation involves mapping a sequence in one language to a sequence in another language. Similar to image processing, a dataset, containing phrases and their translations, is first cleaned and only a part of it is used to train the model. An encoder-decoder LSTM model is used which first converts the input sequence to its vector representation (encoding) and then outputs it to its translated version.
Drawbacks of Using LSTM Networks
As it is said, everything in this world comes with its own advantages and disadvantages, LSTMs too, have a few drawbacks which are discussed below: 

LSTMs became popular because they could solve the problem of vanishing gradients. But it turns out, they fail to remove it completely. The problem lies in the fact that the data still has to move from cell to cell for its evaluation. Moreover, the cell has become quite complex now with additional features (such as forget gates) being brought into the picture.
They require a lot of resources and time to get trained and become ready for real-world applications. In technical terms, they need high memory bandwidth because of the linear layers present in each cell which the system usually fails to provide. Thus, hardware-wise, LSTMs become quite inefficient.
With the rise of data mining, developers are looking for a model that can remember past information for a longer time than LSTMs. The source of inspiration for such kind of model is the human habit of dividing a given piece of information into small parts for easy remembrance.
LSTMs get affected by different random weight initialization and hence behave quite similarly to that of a feed-forward neural net. They prefer small-weight initialization instead.
LSTMs are prone to overfitting and it is difficult to apply the dropout algorithm to curb this issue. Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network.

Gated Recurrent Unit Networks
Last Updated : 04 Jun, 2025
In machine learning Recurrent Neural Networks (RNNs) are essential for tasks involving sequential data such as text, speech and time-series analysis. While traditional RNNs struggle with capturing long-term dependencies due to the vanishing gradient problem architectures like Long Short-Term Memory (LSTM) networks were developed to overcome this limitation.

However LSTMs are very complex structure with higher computational cost. To overcome this Gated Recurrent Unit (GRU) where introduced which uses LSTM architecture by merging its gating mechanisms offering a more efficient solution for many sequential tasks without sacrificing performance. In this article we'll learn more about them.

What-are-GRUs_.webpWhat-are-GRUs_.webp
What are Gated Recurrent Units (GRU) ?
Gated Recurrent Units (GRUs) are a type of RNN introduced by Cho et al. in 2014. The core idea behind GRUs is to use gating mechanisms to selectively update the hidden state at each time step allowing them to remember important information while discarding irrelevant details. GRUs aim to simplify the LSTM architecture by merging some of its components and focusing on just two main gates: the update gate and the reset gate.

New-Project
Structure of GRUs
The GRU consists of two main gates:

Update Gate (
z
t
z 
t
​
 ​): This gate decides how much information from previous hidden state should be retained for the next time step.
Reset Gate (
r
t
r 
t
​
 ​): This gate determines how much of the past hidden state should be forgotten.
These gates allow GRU to control the flow of information in a more efficient manner compared to traditional RNNs which solely rely on hidden state.

Equations for GRU Operations
The internal workings of a GRU can be described using following equations:

1. Reset gate:
r
t
=
σ
(
W
r
⋅
[
h
t
−
1
,
x
t
]
)
r 
t
​
 =σ(W 
r
​
 ⋅[h 
t−1
​
 ,x 
t
​
 ])

The reset gate determines how much of the previous hidden state 
h
t
−
1
h 
t−1
​
  should be forgotten.

2. Update gate:
z
t
=
σ
(
W
z
⋅
[
h
t
−
1
,
x
t
]
)
z 
t
​
 =σ(W 
z
​
 ⋅[h 
t−1
​
 ,x 
t
​
 ])

The update gate controls how much of the new information 
x
t
x 
t
​
 ​ should be used to update the hidden state.

New-Project-
Architecture of GRUs
3. Candidate hidden state:
h
t
′
=
tanh
⁡
(
W
h
⋅
[
r
t
⋅
h
t
−
1
,
x
t
]
)
h 
t
′
​
 =tanh(W 
h
​
 ⋅[r 
t
​
 ⋅h 
t−1
​
 ,x 
t
​
 ])

This is the potential new hidden state calculated based on the current input and the previous hidden state.

4. Hidden state:
h
t
=
(
1
−
z
t
)
⋅
h
t
−
1
+
z
t
⋅
h
t
′
h 
t
​
 =(1−z 
t
​
 )⋅h 
t−1
​
 +z 
t
​
 ⋅h 
t
′
​
 

The final hidden state is a weighted average of the previous hidden state 
h
t
−
1
h 
t−1
​
  and the candidate hidden state 
h
t
′
h 
t
′
​
  based on the update gate 
z
t
z 
t
​
 .

How GRUs Solve the Vanishing Gradient Problem
Like LSTMs, GRUs were designed to address the vanishing gradient problem which is common in traditional RNNs. GRUs help mitigate this issue by using gates that regulate the flow of gradients during training ensuring that important information is preserved and that gradients do not shrink excessively over time. By using these gates, GRUs maintain a balance between remembering important past information and learning new, relevant data.

GRU vs LSTM
GRUs are more computationally efficient because they combine the forget and input gates into a single update gate. GRUs do not maintain an internal cell state as LSTMs do, instead they store information directly in the hidden state making them simpler and faster.

Feature	LSTM (Long Short-Term Memory)	GRU (Gated Recurrent Unit)
Gates	3 (Input, Forget, Output)	2 (Update, Reset)
Cell State	Yes it has cell state	No (Hidden state only)
Training Speed	Slower due to complexity	Faster due to simpler architecture
Computational Load	Higher due to more gates and parameters	Lower due to fewer gates and parameters
Performance	Often better in tasks requiring long-term memory	Performs similarly in many tasks with less complexity.

    TRANSFER LEARNING : 

Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when the second task is related to the first or when data for the second task is limited.

overview.webpoverview.webp
Using learned features from the initial task, the model can adapt more efficiently to the new task, accelerating learning and improving performance. Transfer learning also reduces the risk of overfitting, as the model already incorporates generalizable features useful for the second task.

Why is Transfer Learning Important?
Transfer learning offers solutions to key challenges like:

Limited Data: Acquiring extensive labelled data is often challenging and costly. Transfer learning enables us to use pre-trained models, reducing the dependency on large datasets.
Enhanced Performance: Starting with a pre-trained model which has already learned from substantial data allows for faster and more accurate results on new tasks ideal for applications needing high accuracy and efficiency.
Time and Cost Efficiency: Transfer learning shortens training time and conserves resources by utilizing existing models hence eliminating the need for training from scratch.
Adaptability: Models trained on one task can be fine-tuned for related tasks making transfer learning versatile for various applications from image recognition to natural language processing.
How Does Transfer Learning Work?
Transfer learning involves a structured process to use existing knowledge from a pre-trained model for new tasks:

Pre-trained Model: Start with a model already trained on a large dataset for a specific task. This pre-trained model has learned general features and patterns that are relevant across related tasks.
Base Model: This pre-trained model, known as the base model, includes layers that have processed data to learn hierarchical representations, capturing low-level to complex features.
Transfer Layers: Identify layers within the base model that hold generic information applicable to both the original and new tasks. These layers often near the top of the network capture broad, reusable features.
Fine-tuning: Fine-tune these selected layers with data from the new task. This process helps retain the pre-trained knowledge while adjusting parameters to meet the specific requirements of the new task, improving accuracy and adaptability.
Transfer Learning-Geeksforgeeks
Transfer Learning
Low-level features learned for task A should be beneficial for learning of model for task B.

Frozen Vs Trainable Layers in Transfer Learning
In Transfer Learning, two main components help in adapting models effectively:

Frozen Layers: These layers from a pre-trained model remain unchanged during fine-tuning. They retain general features learned from the original task, extracting universal patterns from input data.
Trainable Layers: These layers are adjusted during fine-tuning to learn task-specific features from the new dataset, allowing the model to meet the new task’s unique requirements.
Freezed and Trainable Layers

How to Decide Which Layers to Freeze or Train?
The extent to which you freeze or fine-tune layers depends on the similarity and size of your target dataset:

Small, Similar Dataset: For smaller datasets that resemble the original dataset, you freeze most layers and only fine-tune the last one or two layers to prevent overfitting.
Large, Similar Dataset: With large, similar datasets you can unfreeze more layers allowing the model to adapt while retaining learned features from the base model.
Small, Different Dataset: For smaller, dissimilar datasets, fine-tuning layers closer to the input layer helps the model learn task-specific features from scratch.
Large, Different Dataset: In this case, fine-tuning the entire model helps the model adapt to the new task while using the broad knowledge from the pre-trained model.
Transfer Learning with MobileNetV2 for MNIST Classification
In this section, we’ll explore transfer learning by fine-tuning a MobileNetV2 model pre-trained on ImageNet for classifying MNIST digits.

1. Preparing the Dataset
We start by loading the MNIST dataset. Since MobileNetV2 is pre-trained on three-channel RGB images of size 224x224, we make a few adjustments to match its expected input shape:

Reshape the images from grayscale (28x28, 1 channel) to RGB (28x28, 3 channels).
Resize images to 32x32 pixels, aligning with our model’s configuration.
Normalize pixel values to fall between 0 and 1 by dividing by 255.

from tensorflow.keras.datasets import mnist
import numpy as np

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = np.stack([train_images]*3, axis=-1) / 255.0
test_images = np.stack([test_images]*3, axis=-1) / 255.0

train_images = tf.image.resize(train_images, [32, 32])
test_images = tf.image.resize(test_images, [32, 32])

train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)
2. Building the Model
We load MobileNetV2 with pre-trained weights from ImageNet excluding the fully connected top layers to customize for our 10-class classification task:

Freeze the base model to retain learned features and avoid overfitting.
Add a global average pooling layer to reduce model complexity.
Add a dense layer with softmax activation for the output classes.

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
base_model.trainable = False  # Freeze base model

inputs = Input(shape=(32, 32, 3))
x = base_model(inputs, training=False)
x = GlobalAveragePooling2D()(x)
outputs = Dense(10, activation='softmax')(x)
model = Model(inputs, outputs)
Output:

base_model=MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

3. Compiling and Training the Model
The model is compiled with categorical cross-entropy as the loss function and accuracy as the evaluation metric. Using Adam optimizer we train the model on the MNIST training data for ten epochs.


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=10, validation_split=0.2)
Output:

training
Training the model
4. Fine-Tuning the Model
After initial training we unfreeze the last few layers of the base model to perform fine-tuning. This allows the model to adjust high-level features for the MNIST data while retaining its foundational knowledge.


base_model.trainable = True
for layer in base_model.layers[:100]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, validation_split=0.2)
Output:

fine-tuning
Fine-Tuning the model
5. Model Evaluation
Once the model has been trained and fine-tuned we evaluate it on the test set, measuring its loss and accuracy. This step assesses how well the transfer learning model has adapted to the MNIST dataset and demonstrates its effectiveness in digit classification.


loss, accuracy = model.evaluate(test_images, test_labels)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")
Output:

Test loss: 0.5697252154350281
Test accuracy: 0.8434000015258789

6. Visualizing Model Performance
To visualize the performance further a confusion matrix provides a breakdown of correct and incorrect classifications.


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

test_predictions = model.predict(test_images)
test_predictions_classes = np.argmax(test_predictions, axis=1)
test_true_classes = np.argmax(test_labels, axis=1)

cm = confusion_matrix(test_true_classes, test_predictions_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
Output:

confusion-matrix
7. Sample Image Visualization
Finally we select a few test images to visualize the model’s predictions against their true labels.


def display_sample(sample_images, sample_labels, sample_predictions):
    fig, axes = plt.subplots(3, 3, figsize=(12, 12))
    fig.subplots_adjust(hspace=0.5, wspace=0.5)

    for i, ax in enumerate(axes.flat):
        ax.imshow(sample_images[i].reshape(32, 32), cmap='gray')
        ax.set_xlabel(f"True: {sample_labels[i]}\nPredicted: {sample_predictions[i]}")
        ax.set_xticks([])
        ax.set_yticks([])

    plt.show()

test_images_gray = np.dot(test_images[...,:3], [0.2989, 0.5870, 0.1140])

random_indices = np.random.choice(len(test_images_gray), 9, replace=False)
sample_images = test_images_gray[random_indices]
sample_labels = test_true_classes[random_indices]
sample_predictions = test_predictions_classes[random_indices]
display_sample(sample_images, sample_labels, sample_predictions)
Output:

Labeled-Images
Labelled Images Output
You can get the complete source code from here.

Applications of Transfer Learning
Transfer learning is widely used across multiple domains including:

Computer Vision: Transfer learning is prevalent in image recognition tasks where models pre-trained on large image datasets are adapted to specific tasks such as medical imaging, facial recognition and object detection.
Natural Language Processing (NLP): In NLP models like BERT, GPT or ELMo are pre-trained on vast text corpora and later fine-tuned for specific tasks such as sentiment analysis, machine translation and question-answering.
Healthcare: Transfer learning helps develop medical diagnostic tools using knowledge from general image recognition models to analyze medical images like X-rays or MRIs.
Finance: Transfer learning in finance assists in fraud detection, risk assessment and credit scoring by transferring patterns learned from related financial datasets.
Advantages of Transfer Learning
Speed up the training process: By using a pre-trained model the model can learn more quickly and effectively on the second task, as it already has a good understanding of the features and patterns in the data.
Better performance: Transfer learning can lead to better performance on the second task, as the model can use the knowledge it has gained from the first task.
Handling small datasets: When there is limited data available for the second task, transfer learning can help to prevent overfitting as the model will have already learned general features that are likely to be useful in the second task.
Disadvantages of Transfer Learning
Domain mismatch: The pre-trained model may not be well-suited to the second task if the two tasks are vastly different or the data distribution between the two tasks is very different.
Overfitting: Transfer learning can lead to overfitting if the model is fine-tuned too much on the second task, as it may learn task-specific features that do not generalize well to new data.
Complexity: The pre-trained model and the fine-tuning process can be computationally expensive and may require specialized hardware.
Transfer learning enhances model performance by using knowledge from previously trained models. By starting with pre-existing models and fine-tuning them for specific tasks, transfer learning saves time, improves accuracy and enables effective learning even with limited data.

Transfer learning uses pre-trained models from one machine learning task or dataset to improve performance and generalizability on a related task or dataset.

Transfer learning is a machine learning technique in which knowledge gained through one task or dataset is used to improve model performance on another related task and/or different dataset.1 In other words, transfer learning uses what has been learned in one setting to improve generalization in another setting.2 Transfer learning has many applications, from solving regression problems in data science to training deep learning models. Indeed, it is particularly appealing for the latter given the large amount of data needed to create deep neural networks.

Traditional learning processes build a new model for each new task, based on the available labeled data. This is because traditional machine learning algorithms assume training and test data come from the same feature space, and so if the data distribution changes, or the trained model is applied to a new dataset, users must retrain a newer model from scratch, even if attempting a similar task as the first model (e.g. sentiment analysis classifier of movie reviews versus song reviews). Transfer learning algorithms, however, takes already-trained models or networks as a starting point. It then applies that model’s knowledge gained in an initial source task or data (e.g. classifying movie reviews) towards a new, yet related, target task or data (e.g. classifying song reviews).3

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Advantages and disadvantages of transfer learning
Advantages
- Computational costs. Transfer learning reduces the requisite computational costs to build models for new problems. By repurposing pretrained models or pretrained networks to tackle a different task, users can reduce the amount of model training time, training data, processor units, and other computational resources. For instance, a fewer number of epochs—i.e. passes through a dataset—may be needed to achieve a desired learning rate. In this way, transfer learning can accelerate and simplify model training processes.

- Dataset size. Transfer learning particularly helps alleviate difficulties involved in acquiring large datasets. For instance, large language models (LLMs) require large amounts of training data to obtain optimal performance. Quality publicly available datasets can be limited, and producing sufficient manually labelled data can be time-consuming and expensive.

- Generalizability. While transfer learning aids model optimization, it can further increase a model’s generalizability. Because transfer learning involves retraining an existing model with a new dataset, the retrained model will consist of knowledge gained from multiple datasets. It will potentially display better performance on a wider variety of data than the initial base model trained on only one type of dataset. Transfer learning can thus inhibit overfitting.4

Of course, the transfer of knowledge from one domain to another cannot offset the negative impact of poor-quality data. Preprocessing techniques and feature engineering, such as data augmentation and feature extraction, are still necessary when using transfer learning.

Disadvantages
It is less the case that there are disadvantages inherent to transfer learning than that there are potential negative consequences that result from its misapplication. Transfer learning works best when three conditions are met:

both learning tasks are similar
source and target datasets data distributions do not vary too greatly
a comparable model can be applied to both tasks
When these conditions are not met, transfer learning can negatively affect model performance. Literature refers to this as negative transfer. Ongoing research proposes a variety of tests for determining whether datasets and tasks meet the above conditions, and so will not result in negative transfer.5 Distant transfer is one method developed to correct for negative transfer that results from too great a dissimilarity in the data distributions of source and target datasets.6

Note that there is no widespread, standard metric to determine similarity between tasks for transfer learning. A handful of studies, however, propose different evaluation methods to predict similarities between datasets and machine learning tasks, and so viability for transfer learning.7

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Types of transfer learning
There are three adjacent practices or sub-settings of transfer learning. Their distinction from one another—as well as transfer learning more broadly—largely result from changes in the relationship between the source domain, target domain, and tasks to be completed.8

- Inductive transfer. This is when the source and target tasks are different, regardless of any difference or similitude between the target and source domains (i.e. datasets). This can manifest in computer vision models when architectures pretrained for feature extraction on large datasets are then are adopted for further training on a specific task, such as object detection. Multitask learning, which consists of simultaneously learning two different tasks (such as image classification and object detection) on the same dataset, can be considered a form of inductive transfer.9

- Unsupervised learning. This is similar to inductive transfer, as the target and source tasks are different. But in inductive transfer, source and/or target data is often labeled. Per its name, unsupervised transfer learning is unsupervised, meaning there is no manually labeled data.10 By comparison, inductive transfer can be considered supervised learning. One common application of unsupervised learning is fraud detection. By identify common patterns across an unlabeled dataset of transactions, a model can further learn to identify deviating behaviors as possible fraud.

- Transductive transfer. This occurs when the source and target tasks are the same, but the datasets (or domains) are different. More specifically, the source data is typically labelled while the target data is unlabeled. Domain adaptation is a form of transductive learning, as it applies knowledge gained from performing a task on one data distribution towards the same task on another data distribution.11 An example of transductive transfer learning is the application of a text classification model trained and tested on restaurant reviews to classify movie reviews.

Transfer learning versus finetuning
Transfer learning is distinct from finetuning. Both, admittedly, reuse preexisting machine learning models as opposed to training new models. But the similarities largely end there. Finetuning refers to the process of further training a model on a task-specific dataset to improve performance on the initial, specific task for which the model was built. For instance, one may create a general purpose object detection model using massive imagesets such as COCO or ImageNet and then further train the resulting model on a smaller, labeled dataset specific for car detection. In this way, a user finetunes an object detection model for car detection. By contrast, transfer learning signifies when users adapt a model to a new, related problem as opposed to the same problem.

Transfer learning use cases
There are many applications of transfer learning in real-world machine learning and artificial intelligence settings. Developers and data scientists can use transfer learning to aid in a myriad of tasks and combine it with other learning approaches, such as reinforcement learning.

Natural language processing
One salient issue affecting transfer learning in NLP is feature mismatch. Features in different domains can have different meanings, and so connotations (e.g. light signifying weight and optics). This disparity in feature representations affects sentiment classification tasks, language models, and more. Deep learning-based models—in particular, word embeddings—show promise in correcting for this, as they can adequately capture semantic relations and orientations for domain adaptation tasks.12

Computer vision
Because of difficulties in acquiring sufficient manually labeled data for diverse computer vision tasks, a wealth of research examines transfer learning applications with convolutional neural networks (CNNs). One notable example is ResNet, a pretrained model architecture that demonstrates improved performance in image classification and object detection tasks.13 Recent research investigates the renowned ImageNet dataset for transfer learning, arguing that (contra computer vision folk wisdom) only small subsets of this dataset are needed to train reliably generalizable models.14 Many transfer learning tutorials for computer vision use both or either ResNet and ImageNet with TensorFlow’s keras library.

Backpropagation in Neural Network
Last Updated : 01 Jul, 2025
Back Propagation is also known as "Backward Propagation of Errors" is a method used to train neural network . Its goal is to reduce the difference between the model’s predicted output and the actual output by adjusting the weights and biases in the network.

It works iteratively to adjust weights and bias to minimize the cost function. In each epoch the model adapts these parameters by reducing loss by following the error gradient. It often uses optimization algorithms like gradient descent or stochastic gradient descent. The algorithm computes the gradient using the chain rule from calculus allowing it to effectively navigate complex layers in the neural network to minimize the cost function.

Backpropagation-in-Neural-Network-1
Fig(a) A simple illustration of how the backpropagation works by adjustments of weights
Back Propagation plays a critical role in how neural networks improve over time. Here's why:

Efficient Weight Update: It computes the gradient of the loss function with respect to each weight using the chain rule making it possible to update weights efficiently.
Scalability: The Back Propagation algorithm scales well to networks with multiple layers and complex architectures making deep learning feasible.
Automated Learning: With Back Propagation the learning process becomes automated and the model can adjust itself to optimize its performance.
Working of Back Propagation Algorithm
The Back Propagation algorithm involves two main steps: the Forward Pass and the Backward Pass.

1. Forward Pass Work
In forward pass the input data is fed into the input layer. These inputs combined with their respective weights are passed to hidden layers. For example in a network with two hidden layers (h1 and h2) the output from h1 serves as the input to h2. Before applying an activation function, a bias is added to the weighted inputs.

Each hidden layer computes the weighted sum (`a`) of the inputs then applies an activation function like ReLU (Rectified Linear Unit) to obtain the output (`o`). The output is passed to the next layer where an activation function such as softmax converts the weighted outputs into probabilities for classification.

Backpropagation-in-Neural-Network-2
The forward pass using weights and biases
2. Backward Pass
In the backward pass the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the Mean Squared Error (MSE) given by:

MSE
=
(
Predicted Output
−
Actual Output
)
2
MSE=(Predicted Output−Actual Output) 
2
 

Once the error is calculated the network adjusts weights using gradients which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer ensuring that the network learns and improves its performance. The activation function through its derivative plays a crucial role in computing these gradients during Back Propagation.

Example of Back Propagation in Machine Learning
Let’s walk through an example of Back Propagation in machine learning. Assume the neurons use the sigmoid activation function for the forward and backward pass. The target output is 0.5 and the learning rate is 1.

1. Defining Neural Network
We define a neural network as Input layer with 2 inputs, Hidden layer with 4 neurons, Output layer with 1 output neuron and use Sigmoid function as activation function.

self.input_size = input_size: stores the size of the input layer
self.hidden_size = hidden_size: stores the size of the hidden layer
self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size): initializes weights for input to hidden layer
self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size): initializes weights for hidden to output layer
self.bias_hidden = np.zeros((1, self.hidden_size)): initializes bias for hidden layer
self.bias_output = np.zeros((1, self.output_size)): initializes bias for output layer



import numpy as np
​
​
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
​
        self.weights_input_hidden = np.random.randn(
            self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(
            self.hidden_size, self.output_size)
​
        self.bias_hidden = np.zeros((1, self.hidden_size))
        self.bias_output = np.zeros((1, self.output_size))
​
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
​
    def sigmoid_derivative(self, x):
        return x * (1 - x)
2. Defining Feed Forward Network
In Forward pass inputs are passed through the network activating the hidden and output layers using the sigmoid function.

self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden: calculates activation for hidden layer
self.hidden_output= self.sigmoid(self.hidden_activation): applies activation function to hidden layer
self.output_activation= np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output: calculates activation for output layer
self.predicted_output = self.sigmoid(self.output_activation): applies activation function to output layer



def feedforward(self, X):
    self.hidden_activation = np.dot(
        X, self.weights_input_hidden) + self.bias_hidden
    self.hidden_output = self.sigmoid(self.hidden_activation)
​
    self.output_activation = np.dot(
        self.hidden_output, self.weights_hidden_output) + self.bias_output
    self.predicted_output = self.sigmoid(self.output_activation)
​
    return self.predicted_output
3. Defining Backward Network
In Backward pass or Back Propagation the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly.

output_error = y - self.predicted_output: calculates the error at the output layer
output_delta = output_error * self.sigmoid_derivative(self.predicted_output): calculates the delta for the output layer
hidden_error = np.dot(output_delta, self.weights_hidden_output.T): calculates the error at the hidden layer
hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output): calculates the delta for the hidden layer
self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate: updates weights between hidden and output layers
self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate: updates weights between input and hidden layers



def backward(self, X, y, learning_rate):
    output_error = y - self.predicted_output
    output_delta = output_error * \
        self.sigmoid_derivative(self.predicted_output)
​
    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)
​
    self.weights_hidden_output += np.dot(self.hidden_output.T,
                                         output_delta) * learning_rate
    self.bias_output += np.sum(output_delta, axis=0,
                               keepdims=True) * learning_rate
    self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate
    self.bias_hidden += np.sum(hidden_delta, axis=0,
                               keepdims=True) * learning_rate
4. Training Network
The network is trained over 10,000 epochs using the Back Propagation algorithm with a learning rate of 0.1 progressively reducing the error.

output = self.feedforward(X): computes the output for the current inputs
self.backward(X, y, learning_rate): updates weights and biases using Back Propagation
loss = np.mean(np.square(y - output)): calculates the mean squared error (MSE) loss



def train(self, X, y, epochs, learning_rate):
    for epoch in range(epochs):
        output = self.feedforward(X)
        self.backward(X, y, learning_rate)
        if epoch % 4000 == 0:
            loss = np.mean(np.square(y - output))
            print(f"Epoch {epoch}, Loss:{loss}")
5. Testing Neural Network
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]): defines the input data
y = np.array([[0], [1], [1], [0]]): defines the target values
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1): initializes the neural network
nn.train(X, y, epochs=10000, learning_rate=0.1): trains the network
output = nn.feedforward(X): gets the final predictions after training



X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
​
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=10000, learning_rate=0.1)
​
output = nn.feedforward(X)
print("Predictions after training:")
print(output)
Output:

Screenshot-2025-03-07-130223
Trained Model
The output shows the training progress of a neural network over 10,000 epochs. Initially the loss was high (0.2713) but it gradually decreased as the network learned reaching a low value of 0.0066 by epoch 8000.
The final predictions are close to the expected XOR outputs: approximately 0 for [0, 0] and [1, 1] and approximately 1 for [0, 1] and [1, 0] indicating that the network successfully learned to approximate the XOR function.
Advantages of Back Propagation for Neural Network Training
The key benefits of using the Back Propagation algorithm are:

Ease of Implementation: Back Propagation is beginner-friendly requiring no prior neural network knowledge and simplifies programming by adjusting weights with error derivatives.
Simplicity and Flexibility: Its straightforward design suits a range of tasks from basic feedforward to complex convolutional or recurrent networks.
Efficiency: Back Propagation accelerates learning by directly updating weights based on error especially in deep networks.
Generalization: It helps models generalize well to new data improving prediction accuracy on unseen examples.
Scalability: The algorithm scales efficiently with larger datasets and more complex networks making it ideal for large-scale tasks.
Challenges with Back Propagation
While Back Propagation is useful it does face some challenges:

Vanishing Gradient Problem: In deep networks the gradients can become very small during Back Propagation making it difficult for the network to learn. This is common when using activation functions like sigmoid or tanh.
Exploding Gradients: The gradients can also become excessively large causing the network to diverge during training.
Overfitting: If the network is too complex it might memorize the training data instead of learning general patterns.


What is backpropagation?
Backpropagation is a machine learning technique essential to the optimization of artificial neural networks. It facilitates the use of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”

Short for "backward propagation of error", backpropagation is an elegant method to calculate how changes to any of the weights or biases of a neural network will affect the accuracy of model predictions. It’s essential to the use of supervised learning, semi-supervised learning or self-supervised learning to train neural networks.

Though equivalents and predecessors to backpropagation were independently proposed in varying contexts dating back to the 1960s, David E. Rumelhart, Geoffrey Hinton and Ronald J. Williams first published the formal learning algorithm. Their 1986 paper, “Learning representations by back-propagating errors,” provided the derivation of the backpropagation algorithm as used and understood in a modern machine learning context.

The logic of backpropagation is that the layers of neurons in artificial neural networks are essentially a series of nested mathematical functions. During training, those interconnected equations are nested into yet another function: a "loss function" that measures the difference (or “loss”) between the desired output (or “ground truth”) for a given input and the neural network’s actual output.

We can therefore use the "chain rule", a calculus principle dating back to the 17th century, to compute the rate at which each neuron contributes to overall loss. In doing so, we can calculate the impact of changes to any variable—that is, to any weight or bias—within the equations those neurons represent.

Mathematically speaking, backpropagation works backward from the output to efficiently calculate the "gradient" of the loss function: a vector of derivatives for every equation in the network. This gradient tells optimization algorithms such as "gradient descent" which equations to adjust, and which direction to adjust them in, to reduce loss.

These three interwoven processes—a loss function that tracks model error across different inputs, the backward propagation of that error to see how different parts of the network contribute to the error and the gradient descent algorithms that adjust model weights accordingly—are how deep learning models “learn.” As such, backpropagation is fundamental to training neural network models, from the most basic multilayer perceptrons to the complex deep neural network architectures used for generative AI.

Industry newsletter

The latest tech news, backed by expert insights
Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

How neural networks work
Because the process of backpropagation is so fundamental to how neural networks are trained, a helpful explanation of the process requires a working understanding of how neural networks make predictions.

Most importantly, it’s useful to understand the purpose and context of "weights" and "biases": the adjustable model parameters that are optimized through backpropagation and gradient descent.

Neural network structure
Neural networks aim to roughly mimic the structure of the human brain. They’re composed of many interconnected nodes (or neurons), arranged in layers. Neural networks make predictions once the original input data has made a "forward pass" through the entire network.

Neurons in the "input layer" receive input data, usually as a vector embedding, with each input neuron receiving an individual feature of the input vector. For example, a model that works with 10x10 pixel grayscale images will typically have 100 neurons in its input layer, with each input neuron corresponding to an individual pixel. Neural networks thus typically require inputs of fixed size, though techniques like pooling or normalization can provide some flexibility.

In a standard feedforward neural network, each neuron in the input layer is connected to each of the neurons in the following layer, which are themselves connected to the neurons in the next layer, and so on until the output layer where final predictions are made. The intermediate layers between the input layer and output layer called the network’s hidden layers, are where most “learning” occurs.

While some specialized neural network architectures, such as mixture of expert models or convolutional neural networks, entail variations, additions or exceptions to this straightforward arrangement, all neural networks employ this core structure.

Visual depiction of a deep neural network
Visual depiction of a basic feedforward neural network with 3 hidden layers. During inference, information about input data flows from left to right; during backpropagation, information about error flows from right to left.

Weights and biases
Though each neuron receives input from each node of the previous layer, not all of those inputs are given the same importance. Each connection between two neurons is given a unique "weight": a multiplier that increases or decreases one neuron’s contribution to a neuron in the following layer.

Each individual neuron may also be given a "bias": a constant value added to the sum of the weighted inputs from the neurons in the previous layer.

The ultimate goal of backpropagation and gradient descent is to calculate the weights and biases that will yield the best model predictions. Neurons corresponding to data features that significantly correlate with accurate predictions are given greater weights; other connections may be given weights approaching zero.

Modern deep neural networks, often with dozens of hidden layers each containing many neurons, might comprise thousands, millions or—in the case of most large language models (LLMs)—billions of such adjustable parameters.

Activation functions
Each neuron is configured to perform a mathematical operation, called an "activation function", on the sum of varyingly weighted inputs it receives from nodes in the previous layer. Activation functions introduce "nonlinearity", enabling the model to capture complex patterns in input data and yield gradients that can be optimized. Using only linear activation functions essentially collapses the neural network into a linear regression model.

Common activation functions in neural networks include:

The sigmoid function, which maps any input to a value between 0 and 1.
The hyperbolic tangent (or tanh) function, which maps inputs to a value between -1 and 1.
The rectified linear unit (or ReLU), which maps any negative input to 0 and leaves any positive input unchanged.
The softmax function, which converts a vector of inputs to a vector whose elements range from 0 and 1 and collectively sum to 1.
Consider a hypothetical hidden unit z, with a tanh activation function and bias term t, in the second layer of a neural network with 3 input nodes, a, b and c, in its input layer. Each of the connections between the input nodes and node z has a unique weight, w. We can describe the output value that node z will pass to the neurons in the next layer with the simplified equation z = tanh(waz*a + wbz*b + wcz*c + t).

The neuron z is connected to neurons in the next layer. That equation for z is therefore part of the activation functions in the next layer and, by extension, also part of every activation function for any neurons in any subsequent layer.

Formulas and visualizations for common activation functions in neural networks
Formulas and visualizations for common activation functions in neural networks. LEFT: sigmoid; CENTER: tanh; RIGHT: ReLU

Why use backpropagation?
As will be explained in the following sections, backpropagation is a remarkably fast, efficient algorithm to untangle the massive web of interconnected variables and equations in a neural network.

To illustrate backpropagation’s efficiency, Michael Nielsen compares it to a simple and intuitive alternative approach to computing the gradient of a neural network’s loss function in his online textbook, "Neural Networks and Deep Learning".

As Nielsen explains, one can easily estimate the impact of changes to any specific weight wj in the network by simply completing a forward pass for two slightly different values of wj, while keeping all other parameters unchanged, and comparing the resulting loss for each pass. By formalizing that process into a straightforward equation and implementing a few lines of code in Python, you can automate that process for each weight in the network.

But now imagine that there are 1 million weights in your model, which would be quite modest for a modern deep learning model. To compute the entire gradient, you’d need to complete 1,000,001 forward passes through the network: 1 to establish a baseline, and then another pass to evaluate changes to each of the million weights.

Backpropagation can achieve the same goal in 2 passes: 1 forward pass and 1 backward pass.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
Key mathematical concepts for backpropagation
To simplify an explanation of how backpropagation works, it will be helpful to first briefly review some core mathematical concepts and terminology.

A derivative is the rate of change in an equation at a specific instant. In a linear equation, the rate of change is a constant slope. In a nonlinear equation, like those used for activation functions, this slope varies. Differentiation is the process of finding the derivative of a specific function. By differentiating a nonlinear function, we can then find the slope—its instantaneous rate of change—at any specific point in the curve.
In functions with multiple variables, a partial derivative is the derivative of one variable concerning the others. If we change one variable, but keep the others the same, how does the output of the overall function change? The activation functions of individual nodes in a neural network have many variables, including the many inputs from neurons in previous layers and the weights applied to those inputs. When dealing with a specific node n, finding the partial derivatives of the activation functions of neurons from the previous layer allows us to isolate the impact of each on the overall output of n’s own activation function.
A gradient is a vector containing all the partial derivatives of a function with multiple variables. It essentially represents all the factors affecting the rate at which the output of a complex equation will change following a change in the input.
The chain rule is a formula for calculating the derivatives of functions that involve not just multiple variables, but multiple functions. For example, consider a composite function ƒ(x) = A(B(x)). The derivative of the composite function, f, is equal to the derivative of the outer function (A) multiplied by the derivative of the inner function (B).
The chain rule is essential to calculating the derivatives of activation functions in neural networks, which are composed of the outputs of activation functions of other neurons in previous layers.

Though the logic behind backpropagation is relatively straightforward, the mathematics and notation can become very complex, especially for those unfamiliar with variable calculus.

How does backpropagation work?
Working backward from the model’s output, backpropagation applies the "chain rule" to calculate the influence of changes to each individual neural network parameter on the overall error of the model’s predictions.

Abstractly speaking, the purpose of backpropagation is to train a neural network to make better predictions through supervised learning. More fundamentally, the goal of backpropagation is to determine how model weights and biases should be adjusted to minimize error as measured by a "loss function".

On a technical, mathematical level, the goal of backpropagation is to calculate the gradient of the loss function with respect to each of the individual parameters of the neural network. In simpler terms, backpropagation uses the chain rule to calculate the rate at which loss changes in response to any change to a specific weight (or bias) in the network.

Generally speaking, training neural networks with backpropagation entails the following steps:

A forward pass, making predictions on training data.
A loss function measures the error of the model’s predictions during that forward pass.
Backpropagation of error, or a backward pass, to calculate the partial derivatives of the loss function.
Gradient descent, to update model weights.
Forward pass
Neural networks output predictions through forward propagation. Forward propagation is essentially a long series of nested equations, with the outputs of the activation functions from one layer of neurons serving as inputs to the activation functions of neurons in the next layer.

Model training typically begins with a random initialization of weights and biases. Model hyperparameters, such as the number of hidden layers, the number of nodes in each layer and activation functions for specific neurons, are configured manually and not subject to training.

In each forward pass, an input is sampled from the training data set. The nodes of the input layer receive the input vector, and each passes their value—multiplied by some random initial weight—to the nodes of the first hidden layer. The hidden units take the weighted sum of these output values as input to an activation function, whose output value (conditioned by a random initial weight) serves as input to the neurons in the next layer. This continues until the output layer, where a final prediction occurs.

Consider this simplified example of a neural network that classifies inputs into one of 5 categories:

The input layer receives a numerical representation of an example sampled from the training data.
The input nodes pass their values to hidden units in the next layer. The hidden units use a ReLU activation function.
Data flows through the hidden layers, each progressively extracting key features until it reaches the output layer.
The output layer contains 5 neurons, each corresponding to a potential classification category.
The output neurons use a softmax activation function. The output value of each output neuron’s softmax function corresponds to the probability, out of 1, that the input should be classified as the category that the neuron represents.
The network predicts that the original input belongs to the category of whichever output neuron has the highest softmax value.
In a well-trained network, this model will consistently output a high probability value for the correct classification and output low probability values for the other, incorrect classifications. However, this neural network isn’t yet trained. At this point, its weights and biases have random initial values, so its predictions are generally inaccurate.

Loss function
After each forward pass, a "loss function" measures the difference (or “loss”) between the model’s predicted output for a given input and the correct predictions (or “ground truth”) for that input. In other words, it measures how different the model’s actual output is from the desired output.

In supervised learning, which uses labeled data, ground truth is provided by manual annotations. In self-supervised learning, which masks or transforms parts of unlabeled data samples and task models by reconstructing it, the original sample serves as ground truth.

The goal of this loss function is to quantify inaccuracy in a way that appropriately reflects both the nature and magnitude of the error of the model’s output for each input. Different mathematical formulas for loss are best suited to specific tasks: for example, variants of mean squared error work well for regression problems, whereas variants of cross-entropy loss work well for classification.

Because the loss function takes the output of a neural network as an input, and that neural network output is a composite function comprising many nested activation functions of individual neurons, differentiating the loss function entails differentiating the entire network. To do so, backpropagation uses the chain rule.

"Loss function," "cost function" or "error function?"
It’s worth quickly noting that in some contexts, the terms cost function or error function are used in place of loss function, with “cost” or “error” replacing “loss.”

Though some machine learning literature assigns unique nuance to each term, they’re generally interchangeable.1 An objective function is a broader term for any such evaluation function that we want to either minimize or maximize. Loss function, cost function or error function refer specifically to terms we want to minimize.

Backwards pass
Starting from the final layer, a "backward pass" differentiates the loss function to compute how each individual parameter of the network contributes to the overall error for a single input.

Returning to our earlier example of the classifier model, we would start with the 5 neurons in the final layer, which we’ll call layer L. The softmax value of each output neuron represents the likelihood, out of 1, that an input belongs to their category. In a perfectly trained model, the neuron representing the correct classification would have an output value close to 1 and the other neurons would have an output value close to 0.

For now, we’ll focus on the output unit representing the correct prediction, which we’ll call Lc. Lc’s activation function is a composite function, containing the many nested activation functions of the entire neural network from the input layer to the output layer. Minimizing the loss function would entail making adjustments throughout the network that bring the output of Lc’s activation function closer to 1.

To do so, we’ll need to know how any change in previous layers will change Lc’s own output. In other words, we’ll need to find the partial derivatives of Lc’s activation function.

The output of Lc’s activation function depends on the contributions that it receives from neurons in the penultimate layer, which we’ll call layer L-1. One way to change Lc’s output is to change the weights between the neurons in L-1 and Lc. By calculating the partial derivative of each L-1 weight with respect to the other weights, we can see how increasing or decreasing any of them will bring the output of Lc closer to (or further away from) 1.

But that’s not the only way to change Lc’s output. The contributions Lc receives from L-1 neurons are determined not just by the weights applied to L-1’s output values, but by the actual (pre-weight) output values themselves. The L-1 neurons’ output values, in turn, are influenced by weights applied to inputs they receive from L-2. So we can differentiate the activation functions in L-1 to find the partial derivatives of the weights applied to L-2’s contributions. These partial derivatives show us how any change to an L-2 weight will affect the outputs in L-1, which would subsequently affect the output value of Lc and thereby affect the loss function.

By that same logic, we could also influence the output values that L-1 neurons receive from L-2 neurons by adjusting the contributions that L-2 neurons receive from neurons in L-3. So we find the partial derivatives in L-3, and so on, recursively repeating this process until we’ve reached the input layer. When we’re done, we have the gradient of the loss function: a vector of its partial derivative for each weight and bias parameter in the network.

We’ve now completed a forward pass and backward pass for a single training example. However, our goal is to train the model to generalize well to new inputs. To do so requires training on a large number of samples that reflect the diversity and range of inputs the model will be tasked with making predictions on post-training.

Gradient descent
Now that we have the gradients of the loss function with respect to each weight and bias parameter in the network, we can minimize the loss function—and thus optimize the model—by using gradient descent to update the model parameters.

Moving down—descending—the gradient of the loss function will decrease the loss. Since the gradient we calculated during backpropagation contains the partial derivatives for every model parameter, we know which direction to “step” each of our parameters to reduce loss.

Each step reflects the model “learning” from its training data. Our goal is to iteratively update weights until we have reached the minimum gradient. The object of gradient descent algorithms is to find the specific parameter adjustments that will move us down the gradient most efficiently.

Learning rate
The size of each step is a tunable hyperparameter, called the learning rate. Choosing the right learning rate is important for efficient and effective training.

Recall that the activation functions in a neural network are nonlinear. Some gradients may be approximately U-shaped: stepping in one direction moves down the gradient, but continuing to step in that direction will eventually move up the gradient.

A low learning rate ensures we always step in the right direction, but calculating so many changes is time-consuming and computationally expensive. A high learning rate is computationally efficient, but risks overshooting the minimum.

Batch size
Another consideration in gradient descent is how often to update weights. One option is to compute the gradients for every example in the training data set, then take an average of those gradients and use it to update parameters. The process is repeated iteratively in a series of training epochs until the error rate stabilizes. This method is batch gradient descent.

When the training data set is very large—as it typically is in deep learning—batch gradient descent entails prohibitively long processing times. Calculating gradients for millions of examples for each iteration of weight updates becomes inefficient. In stochastic gradient descent (SGD), each epoch uses a single training example for each step. While loss might fluctuate on an epoch-to-epoch basis, it quickly converges to the minimum throughout many updates.

Mini-batch gradient descent represents a middle-ground approach. Training examples are randomly sampled in batches of fixed size, and their gradients are then calculated and averaged together. This mitigates the memory storage requirements of batch gradient descent while also reducing the relative instability of SGD.

Activation functions in Neural Networks
Last Updated : 03 Jun, 2025
While building a neural network, one key decision is selecting the Activation Function for both the hidden layer and the output layer. It is a mathematical function applied to the output of a neuron. It introduces non-linearity into the model, allowing the network to learn and represent complex patterns in the data. Without this non-linearity feature a neural network would behave like a linear regression model no matter how many layers it has.

Activation function decides whether a neuron should be activated by calculating the weighted sum of inputs and adding a bias term. This helps the model make complex decisions and predictions by introducing non-linearities to the output of each neuron.

Before diving into the activation function, you should have prior knowledge of the following topics: Neural Networks, Backpropagation

Activation-functions-in-Neural-Networks
Activation Functions in neural Networks
Introducing Non-Linearity in Neural Network
Non-linearity means that the relationship between input and output is not a straight line. In simple terms the output does not change proportionally with the input. A common choice is the ReLU function defined as 
σ
(
x
)
=
max
⁡
(
0
,
x
)
σ(x)=max(0,x).

Imagine you want to classify apples and bananas based on their shape and color.

If we use a linear function it can only separate them using a straight line.
But real-world data is often more complex like overlapping colors, different lighting, etc.
By adding a non-linear activation function like ReLU, Sigmoid or Tanh the network can create curved decision boundaries to separate them correctly.
Effect of Non-Linearity
The inclusion of the ReLU activation function 
σ
σ allows 
h
1
h 
1
​
 ​ to introduce a non-linear decision boundary in the input space. This non-linearity enables the network to learn more complex patterns that are not possible with a purely linear model such as:

Modeling functions that are not linearly separable.
Increasing the capacity of the network to form multiple decision boundaries based on the combination of weights and biases.
Why is Non-Linearity Important in Neural Networks?
Neural networks consist of neurons that operate using weights, biases and activation functions.

In the learning process these weights and biases are updated based on the error produced at the output—a process known as backpropagation. Activation functions enable backpropagation by providing gradients that are essential for updating the weights and biases.

Without non-linearity even deep networks would be limited to solving only simple, linearly separable problems. Activation functions help neural networks to model highly complex data distributions and solve advanced deep learning tasks. Adding non-linear activation functions introduce flexibility and enable the network to learn more complex and abstract patterns from data.

Linear activation function is used at just one place i.e. output layer.
Using linear activation across all layers makes the network's ability to learn complex patterns limited.
Linear activation functions are useful for specific tasks but must be combined with non-linear functions to enhance the neural network’s learning and predictive capabilities.

Linear-Activation-Function
Linear Activation Function or Identity Function returns the input as the output
2. Non-Linear Activation Functions
1. Sigmoid Function 

Sigmoid Activation Function is characterized by 'S' shape. It is mathematically defined as
A
=
1
1
+
e
−
x
A= 
1+e 
−x
 
1
​
 ​. This formula ensures a smooth and continuous output that is essential for gradient-based optimization methods.

It allows neural networks to handle and model complex patterns that linear equations cannot.
The output ranges between 0 and 1, hence useful for binary classification.
The function exhibits a steep gradient when x values are between -2 and 2. This sensitivity means that small changes in input x can cause significant changes in output y which is critical during the training process.
Sigmoid-Activation-Function
Sigmoid or Logistic Activation Function Graph
2. Tanh Activation Function 

Tanh function (hyperbolic tangent function) is a shifted version of the sigmoid, allowing it to stretch across the y-axis. It is defined as:

f
(
x
)
=
tanh
⁡
(
x
)
=
2
1
+
e
−
2
x
−
1.
f(x)=tanh(x)= 
1+e 
−2x
 
2
​
 −1.

Alternatively, it can be expressed using the sigmoid function:

tanh
⁡
(
x
)
=
2
×
sigmoid
(
2
x
)
−
1
tanh(x)=2×sigmoid(2x)−1

Value Range: Outputs values from -1 to +1.
Non-linear: Enables modeling of complex data patterns.
Use in Hidden Layers: Commonly used in hidden layers due to its zero-centered output, facilitating easier learning for subsequent layers.
Tanh-Activation-Function
Tanh Activation Function
3. ReLU (Rectified Linear Unit) Function 

ReLU activation is defined by 
A
(
x
)
=
max
⁡
(
0
,
x
)
A(x)=max(0,x), this means that if the input x is positive, ReLU returns x, if the input is negative, it returns 0.

Value Range: 
[
0
,
∞
)
[0,∞), meaning the function only outputs non-negative values.
Nature: It is a non-linear activation function, allowing neural networks to learn complex patterns and making backpropagation more efficient.
Advantage over other Activation: ReLU is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.
relu-activation-function

ReLU Activation Function

3. Exponential Linear Units

1. Softmax Function

Softmax function is designed to handle multi-class classification problems. It transforms raw output scores from a neural network into probabilities. It works by squashing the output values of each class into the range of 0 to 1 while ensuring that the sum of all probabilities equals 1.

Softmax is a non-linear activation function.
The Softmax function ensures that each class is assigned a probability, helping to identify which class the input belongs to.
softmax
Softmax Activation Function
2. SoftPlus Function

Softplus function is defined mathematically as: 
A
(
x
)
=
log
⁡
(
1
+
e
x
)
A(x)=log(1+e 
x
 ).

This equation ensures that the output is always positive and differentiable at all points which is an advantage over the traditional ReLU function.

Nature: The Softplus function is non-linear.
Range: The function outputs values in the range 
(
0
,
∞
)
(0,∞), similar to ReLU, but without the hard zero threshold that ReLU has.
Smoothness: Softplus is a smooth, continuous function, meaning it avoids the sharp discontinuities of ReLU which can sometimes lead to problems during optimization.
softplus
Softplus Activation Function
Impact of Activation Functions on Model Performance
The choice of activation function has a direct impact on the performance of a neural network in several ways:

Convergence Speed: Functions like ReLU allow faster training by avoiding the vanishing gradient problem while Sigmoid and Tanh can slow down convergence in deep networks.
Gradient Flow: Activation functions like ReLU ensure better gradient flow, helping deeper layers learn effectively. In contrast Sigmoid can lead to small gradients, hindering learning in deep layers.
Model Complexity: Activation functions like Softmax allow the model to handle complex multi-class problems, whereas simpler functions like ReLU or Leaky ReLU are used for basic layers.
Activation functions are the backbone of neural networks enabling them to capture non-linear relationships in data. From classic functions like Sigmoid and Tanh to modern variants like ReLU and Swish, each has its place in different types of neural networks. The key is to understand their behavior and choose the right one based on your model’s needs.

Application Of Generative Adversarial Networks (GANs)
Image Synthesis & Generation: GANs generate realistic images, avatars and high-resolution visuals by learning patterns from training data. They are used in art, gaming and AI-driven design.
Image-to-Image Translation: They can transform images between domains while preserving key features. Examples include converting day images to night, sketches to realistic images or changing artistic styles.
Text-to-Image Synthesis: They create visuals from textual descriptions helps applications in AI-generated art, automated design and content creation.
Data Augmentation: They generate synthetic data to improve machine learning models helps in making them more robust and generalizable in fields with limited labeled data.
High-Resolution Image Enhancement: They upscale low-resolution images which helps in improving clarity for applications like medical imaging, satellite imagery and video enhancement.
Advantages of GAN
Lets see various advantages of the GANs:

Synthetic Data Generation: GANs produce new, synthetic data resembling real data distributions which is useful for augmentation, anomaly detection and creative tasks.
High-Quality Results: They can generate photorealistic images, videos, music and other media with high quality.
Unsupervised Learning: They don’t require labeled data helps in making them effective in scenarios where labeling is expensive or difficult.
Versatility: They can be applied across many tasks including image synthesis, text-to-image generation, style transfer, anomaly detection and more.
GANs are evolving and shaping the future of artificial intelligence. As the technology improves, we can expect even more innovative applications that will change how we create, work and interact with digital content.

Natural Language Processing (NLP) - Overview
Last Updated : 08 Apr, 2025
Natural Language Processing (NLP) is a field that combines computer science, artificial intelligence and language studies. It helps computers understand, process and create human language in a way that makes sense and is useful. With the growing amount of text data from social media, websites and other sources, NLP is becoming a key tool to gain insights and automate tasks like analyzing text or translating languages.

natural-language-processing
Natural Language Processing
Table of Content

NLP Techniques
How Natural Language Processing (NLP) Works
Technologies related to Natural Language Processing
Applications of Natural Language Processing (NLP)
Future Scope
NLP is used by many applications that use language, such as text translation, voice recognition, text summarization and chatbots. You may have used some of these applications yourself, such as voice-operated GPS systems, digital assistants, speech-to-text software and customer service bots. NLP also helps businesses improve their efficiency, productivity and performance by simplifying complex tasks that involve language.

NLP Techniques
NLP encompasses a wide array of techniques that aimed at enabling computers to process and understand human language. These tasks can be categorized into several broad areas, each addressing different aspects of language processing. Here are some of the key NLP techniques:

1. Text Processing and Preprocessing
Tokenization: Dividing text into smaller units, such as words or sentences.
Stemming and Lemmatization: Reducing words to their base or root forms.
Stopword Removal: Removing common words (like "and", "the", "is") that may not carry significant meaning.
Text Normalization: Standardizing text, including case normalization, removing punctuation and correcting spelling errors.
2. Syntax and Parsing
Part-of-Speech (POS) Tagging: Assigning parts of speech to each word in a sentence (e.g., noun, verb, adjective).
Dependency Parsing: Analyzing the grammatical structure of a sentence to identify relationships between words.
Constituency Parsing: Breaking down a sentence into its constituent parts or phrases (e.g., noun phrases, verb phrases).
3. Semantic Analysis
Named Entity Recognition (NER): Identifying and classifying entities in text, such as names of people organizations, locations, dates, etc.
Word Sense Disambiguation (WSD): Determining which meaning of a word is used in a given context.
Coreference Resolution: Identifying when different words refer to the same entity in a text (e.g., "he" refers to "John").
4. Information Extraction
Entity Extraction: Identifying specific entities and their relationships within the text.
Relation Extraction: Identifying and categorizing the relationships between entities in a text.
5. Text Classification in NLP
Sentiment Analysis: Determining the sentiment or emotional tone expressed in a text (e.g., positive, negative, neutral).
Topic Modeling: Identifying topics or themes within a large collection of documents.
Spam Detection: Classifying text as spam or not spam.
6. Language Generation
Machine Translation: Translating text from one language to another.
Text Summarization: Producing a concise summary of a larger text.
Text Generation: Automatically generating coherent and contextually relevant text.
7. Speech Processing
Speech Recognition: Converting spoken language into text.
Text-to-Speech (TTS) Synthesis: Converting written text into spoken language.
8. Question Answering
Retrieval-Based QA: Finding and returning the most relevant text passage in response to a query.
Generative QA: Generating an answer based on the information available in a text corpus.
9. Dialogue Systems
Chatbots and Virtual Assistants: Enabling systems to engage in conversations with users, providing responses and performing tasks based on user input.
10. Sentiment and Emotion Analysis in NLP
Emotion Detection: Identifying and categorizing emotions expressed in text.
Opinion Mining: Analyzing opinions or reviews to understand public sentiment toward products, services or topics.
How Natural Language Processing (NLP) Works
nlp-working
NLP Working
Working in natural language processing (NLP) typically involves using computational techniques to analyze and understand human language. This can include tasks such as language understanding, language generation and language interaction.

1. Text Input and Data Collection
Data Collection: Gathering text data from various sources such as websites, books, social media or proprietary databases.
Data Storage: Storing the collected text data in a structured format, such as a database or a collection of documents.
2. Text Preprocessing
Preprocessing is crucial to clean and prepare the raw text data for analysis. Common preprocessing steps include:

Tokenization: Splitting text into smaller units like words or sentences.
Lowercasing: Converting all text to lowercase to ensure uniformity.
Stopword Removal: Removing common words that do not contribute significant meaning, such as "and," "the," "is."
Punctuation Removal: Removing punctuation marks.
Stemming and Lemmatization: Reducing words to their base or root forms. Stemming cuts off suffixes, while lemmatization considers the context and converts words to their meaningful base form.
Text Normalization: Standardizing text format, including correcting spelling errors, expanding contractions and handling special characters.
3. Text Representation
Bag of Words (BoW): Representing text as a collection of words, ignoring grammar and word order but keeping track of word frequency.
Term Frequency-Inverse Document Frequency (TF-IDF): A statistic that reflects the importance of a word in a document relative to a collection of documents.
Word Embeddings: Using dense vector representations of words where semantically similar words are closer together in the vector space (e.g., Word2Vec, GloVe).
4. Feature Extraction
Extracting meaningful features from the text data that can be used for various NLP tasks.

N-grams: Capturing sequences of N words to preserve some context and word order.
Syntactic Features: Using parts of speech tags, syntactic dependencies and parse trees.
Semantic Features: Leveraging word embeddings and other representations to capture word meaning and context.
5. Model Selection and Training
Selecting and training a machine learning or deep learning model to perform specific NLP tasks.

Supervised Learning: Using labeled data to train models like Support Vector Machines (SVM), Random Forests or deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).
Unsupervised Learning: Applying techniques like clustering or topic modeling (e.g., Latent Dirichlet Allocation) on unlabeled data.
Pre-trained Models: Utilizing pre-trained language models such as BERT, GPT or transformer-based models that have been trained on large corpora.
6. Model Deployment and Inference
Deploying the trained model and using it to make predictions or extract insights from new text data.

Text Classification: Categorizing text into predefined classes (e.g., spam detection, sentiment analysis).
Named Entity Recognition (NER): Identifying and classifying entities in the text.
Machine Translation: Translating text from one language to another.
Question Answering: Providing answers to questions based on the context provided by text data.
7. Evaluation and Optimization
Evaluating the performance of the NLP algorithm using metrics such as accuracy, precision, recall, F1-score and others.

Hyperparameter Tuning: Adjusting model parameters to improve performance.
Error Analysis: Analyzing errors to understand model weaknesses and improve robustness.
Technologies related to Natural Language Processing
There are a variety of technologies related to natural language processing (NLP) that are used to analyze and understand human language. Some of the most common include:

Machine learning: NLP relies heavily on machine learning techniques such as supervised and unsupervised learning, deep learning and reinforcement learning to train models to understand and generate human language.
Natural Language Toolkits (NLTK) and other libraries: NLTK is a popular open-source library in Python that provides tools for NLP tasks such as tokenization, stemming and part-of-speech tagging. Other popular libraries include spaCy, OpenNLP and CoreNLP.
Parsers: Parsers are used to analyze the syntactic structure of sentences, such as dependency parsing and constituency parsing.
Text-to-Speech (TTS) and Speech-to-Text (STT) systems: TTS systems convert written text into spoken words, while STT systems convert spoken words into written text.
Named Entity Recognition (NER) systems: NER systems identify and extract named entities such as people, places and organizations from the text.
Sentiment Analysis: A technique to understand the emotions or opinions expressed in a piece of text, by using various techniques like Lexicon-Based, Machine Learning-Based and Deep Learning-based methods
Machine Translation: NLP is used for language translation from one language to another through a computer.
Chatbots: NLP is used for chatbots that communicate with other chatbots or humans through auditory or textual methods.
AI Software: NLP is used in question-answering software for knowledge representation, analytical reasoning as well as information retrieval.
Applications of Natural Language Processing (NLP)
Spam Filters: One of the most irritating things about email is spam. Gmail uses natural language processing (NLP) to discern which emails are legitimate and which are spam. These spam filters look at the text in all the emails you receive and try to figure out what it means to see if it's spam or not.
Algorithmic Trading: Algorithmic trading is used for predicting stock market conditions. Using NLP, this technology examines news headlines about companies and stocks and attempts to comprehend their meaning in order to determine if you should buy, sell or hold certain stocks.
Questions Answering: NLP can be seen in action by using Google Search or Siri Services. A major use of NLP is to make search engines understand the meaning of what we are asking and generate natural language in return to give us the answers.
Summarizing Information: On the internet, there is a lot of information and a lot of it comes in the form of long documents or articles. NLP is used to decipher the meaning of the data and then provides shorter summaries of the data so that humans can comprehend it more quickly.
Future Scope
NLP is shaping the future of technology in several ways:

Chatbots and Virtual Assistants: NLP enables chatbots to quickly understand and respond to user queries, providing 24/7 assistance across text or voice interactions.
Invisible User Interfaces (UI): With NLP, devices like Amazon Echo allow for seamless communication through voice or text, making technology more accessible without traditional interfaces.
Smarter Search: NLP is improving search by allowing users to ask questions in natural language, as seen with Google Drive's recent update, making it easier to find documents.
Multilingual NLP: Expanding NLP to support more languages, including regional and minority languages, broadens accessibility.
Future Enhancements: NLP is evolving with the use of Deep Neural Networks (DNNs) to make human-machine interactions more natural. Future advancements include improved semantics for word understanding and broader language support, enabling accurate translations and better NLP models for languages not yet supported.


What is NLP?
Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.

NLP enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguistics, the rule-based modeling of human language together with statistical modeling, machine learning and deep learning.

NLP research has helped enable the era of generative AI, from the communication skills of large language models (LLMs) to the ability of image generation models to understand requests. NLP is already part of everyday life for many, powering search engines, prompting chatbots for customer service with spoken commands, voice-operated GPS systems and question-answering digital assistants on smartphones such as Amazon’s Alexa, Apple’s Siri and Microsoft’s Cortana.

NLP also plays a growing role in enterprise solutions that help streamline and automate business operations, increase employee productivity and simplify business processes.


AI Training Ground Natural Language Processing
AI Training Ground Natural Language Processing (2:33 min)
Benefits of NLP
NLP makes it easier for humans to communicate and collaborate with machines, by allowing them to do so in the natural human language they use every day. This offers benefits across many industries and applications.

Automation of repetitive tasks
Improved data analysis and insights
Enhanced search
Content generation
Automation of repetitive tasks 
NLP is especially useful in fully or partially automating tasks like customer support, data entry and document handling. For example, NLP-powered chatbots can handle routine customer queries, freeing up human agents for more complex issues. In document processing, NLP tools can automatically classify, extract key information and summarize content, reducing the time and errors associated with manual data handling. NLP facilitates language translation, converting text from one language to another while preserving meaning, context and nuances.

Improved data analysis
NLP enhances data analysis by enabling the extraction of insights from unstructured text data, such as customer reviews, social media posts and news articles. By using text mining techniques, NLP can identify patterns, trends and sentiments that are not immediately obvious in large datasets. Sentiment analysis enables the extraction of subjective qualities, attitudes, emotions, sarcasm, confusion or suspicion from text. This is often used for routing communications to the system or the person most likely to make the next response.

This allows businesses to better understand customer preferences, market conditions and public opinion. NLP tools can also perform categorization and summarization of vast amounts of text, making it easier for analysts to identify key information and make data-driven decisions more efficiently.

Enhanced search
NLP benefits search by enabling systems to understand the intent behind user queries, providing more accurate and contextually relevant results. Instead of relying solely on keyword matching, NLP-powered search engines analyze the meaning of words and phrases, making it easier to find information even when queries are vague or complex. This improves user experience, whether in web searches, document retrieval or enterprise data systems.

Powerful content generation
NLP powers advanced language models to create human-like text for various purposes. Pre-trained models, such as GPT-4, can generate articles, reports, marketing copy, product descriptions and even creative writing based on prompts provided by users. NLP-powered tools can also assist in automating tasks like drafting emails, writing social media posts or legal documentation. By understanding context, tone and style, NLP sees to it that the generated content is coherent, relevant and aligned with the intended message, saving time and effort in content creation while maintaining quality.

Industry newsletter

The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.

Business email
johndoe@yourdomain.com
Subscribe

Approaches to NLP
NLP combines the power of computational linguistics together with machine learning algorithms and deep learning. Computational linguistics uses data science to analyze language and speech. It includes two main types of analysis: syntactical analysis and semantical analysis. Syntactical analysis determines the meaning of a word, phrase or sentence by parsing the syntax of the words and applying preprogrammed rules of grammar. Semantical analysis uses the syntactic output to draw meaning from the words and interpret their meaning within the sentence structure.

The parsing of words can take one of two forms. Dependency parsing looks at the relationships between words, such as identifying nouns and verbs, while constituency parsing then builds a parse tree (or syntax tree): a rooted and ordered representation of the syntactic structure of the sentence or string of words. The resulting parse trees underly the functions of language translators and speech recognition. Ideally, this analysis makes the output either text or speech understandable to both NLP models and people.

Self-supervised learning (SSL) in particular is useful for supporting NLP because NLP requires large amounts of labeled data to train AI models. Because these labeled datasets require time-consuming annotation, a process involving manual labeling by humans, gathering sufficient data can be prohibitively difficult. Self-supervised approaches can be more time-effective and cost-effective, as they replace some or all manually labeled training data.
 
Three different approaches to NLP include:

Rules-based NLP
The earliest NLP applications were simple if-then decision trees, requiring preprogrammed rules. They are only able to provide answers in response to specific prompts, such as the original version of Moviefone, which had rudimentary natural language generation (NLG) capabilities. Because there is no machine learning or AI capability in rules-based NLP, this function is highly limited and not scalable.

Statistical NLP
Developed later, statistical NLP automatically extracts, classifies and labels elements of text and voice data and then assigns a statistical likelihood to each possible meaning of those elements. This relies on machine learning, enabling a sophisticated breakdown of linguistics such as part-of-speech tagging.
 
Statistical NLP introduced the essential technique of mapping language elements, such as words and grammatical rules to a vector representation so that language can be modeled by using mathematical (statistical) methods, including regression or Markov models. This informed early NLP developments such as spellcheckers and T9 texting (Text on 9 keys, to be used on Touch-Tone telephones).

Deep learning NLP
Recently, deep learning models have become the dominant mode of NLP, by using huge volumes of raw, unstructured data both text and voice to become ever more accurate. Deep learning can be viewed as a further evolution of statistical NLP, with the difference that it uses neural network models. There are several subcategories of models:

Sequence-to-Sequence (seq2seq) models: Based on recurrent neural networks (RNN), they have mostly been used for machine translation by converting a phrase from one domain (such as the German language) into the phrase of another domain (such as English).

Transformer models: They use tokenization of language (the position of each token words or subwords) and self-attention (capturing dependencies and relationships) to calculate the relation of different language parts to one another. Transformer models can be efficiently trained by using self-supervised learning on massive text databases. A landmark in transformer models was Google’s bidirectional encoder representations from transformers (BERT), which became and remains the basis of how Google’s search engine works.

Autoregressive models: This type of transformer model is trained specifically to predict the next word in a sequence, which represents a huge leap forward in the ability to generate text. Examples of autoregressive LLMs include GPT, Llama, Claude and the open-source Mistral.

Foundation models: Prebuilt and curated foundation models can speed the launching of an NLP effort and boost trust in its operation. For example, the IBM® Granite™ foundation models are widely applicable across industries. They support NLP tasks including content generation and insight extraction. Additionally, they facilitate retrieval-augmented generation, a framework for improving the quality of response by linking the model to external sources of knowledge. The models also perform named entity recognition which involves identifying and extracting key information in a text.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
NLP Tasks
Several NLP tasks typically help process human text and voice data in ways that help the computer make sense of what it’s ingesting. Some of these tasks include:

Coreference resolution

Named entity recognition

Part-of-speech tagging

Word sense disambiguation

Coreference resolution
This is the task of identifying if and when two words refer to the same entity. The most common example is determining the person or object to which a certain pronoun refers (such as “she” = “Mary”). But it can also identify a metaphor or an idiom in the text (such as an instance in which “bear” isn’t an animal, but a large and hairy person). 

Named entity recognition (NER)
NER identifies words or phrases as useful entities. NER identifies “London” as a location or “Maria” as a person's name.

Part-of-speech tagging
Also called grammatical tagging, this is the process of determining which part of speech a word or piece of text is, based on its use and context. For example, part-of-speech identifies “make” as a verb in “I can make a paper plane,” and as a noun in “What make of car do you own?”

Word sense disambiguation
This is the selection of a word meaning for a word with multiple possible meanings. This uses a process of semantic analysis to examine the word in context. For example, word sense disambiguation helps distinguish the meaning of the verb “make” in “make the grade” (to achieve) versus “make a bet” (to place). Sorting out “I will be merry when I marry Mary” requires a sophisticated NLP system.

How NLP works
NLP works by combining various computational techniques to analyze, understand and generate human language in a way that machines can process. Here is an overview of a typical NLP pipeline and its steps:

Text preprocessing
NLP text preprocessing prepares raw text for analysis by transforming it into a format that machines can more easily understand. It begins with tokenization, which involves splitting the text into smaller units like words, sentences or phrases. This helps break down complex text into manageable parts. Next, lowercasing is applied to standardize the text by converting all characters to lowercase, ensuring that words like "Apple" and "apple" are treated the same. Stop word removal is another common step, where frequently used words like "is" or "the" are filtered out because they don't add significant meaning to the text. Stemming or lemmatization reduces words to their root form (e.g., "running" becomes "run"), making it easier to analyze language by grouping different forms of the same word. Additionally, text cleaning removes unwanted elements such as punctuation, special characters and numbers that may clutter the analysis.

After preprocessing, the text is clean, standardized and ready for machine learning models to interpret effectively.

Feature extraction
Feature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. This involves transforming text into structured data by using NLP techniques like Bag of Words and TF-IDF, which quantify the presence and importance of words in a document. More advanced methods include word embeddings like Word2Vec or GloVe, which represent words as dense vectors in a continuous space, capturing semantic relationships between words. Contextual embeddings further enhance this by considering the context in which words appear, allowing for richer, more nuanced representations.

Text analysis
Text analysis involves interpreting and extracting meaningful information from text data through various computational techniques. This process includes tasks such as part-of-speech (POS) tagging, which identifies grammatical roles of words and named entity recognition (NER), which detects specific entities like names, locations and dates. Dependency parsing analyzes grammatical relationships between words to understand sentence structure, while sentiment analysis determines the emotional tone of the text, assessing whether it is positive, negative or neutral. Topic modeling identifies underlying themes or topics within a text or across a corpus of documents. Natural language understanding (NLU) is a subset of NLP that focuses on analyzing the meaning behind sentences. NLU enables software to find similar meanings in different sentences or to process words that have different meanings. Through these techniques, NLP text analysis transforms unstructured text into insights.

Model training
Processed data is then used to train machine learning models, which learn patterns and relationships within the data. During training, the model adjusts its parameters to minimize errors and improve its performance. Once trained, the model can be used to make predictions or generate outputs on new, unseen data. The effectiveness of NLP modeling is continually refined through evaluation, validation and fine-tuning to enhance accuracy and relevance in real-world applications.

Different software environments are useful throughout the said processes. For example, the Natural Language Toolkit (NLTK) is a suite of libraries and programs for English that is written in the Python programming language. It supports text classification, tokenization, stemming, tagging, parsing and semantic reasoning functionalities. TensorFlow is a free and open-source software library for machine learning and AI that can be used to train models for NLP applications. Tutorials and certifications abound for those interested in familiarizing themselves with such tools.

Challenges of NLP 
Even state-of-the-art NLP models are not perfect, just as human speech is prone to error. As with any AI technology, NLP comes with potential pitfalls. Human language is filled with ambiguities that make it difficult for programmers to write software that accurately determines the intended meaning of text or voice data. Human language might take years for humans to learn and many never stop learning. But then programmers must teach natural language-powered applications to recognize and understand irregularities so their applications can be accurate and useful. Associated risks might include:

Biased training
As with any AI function, biased data used in training will skew the answers. The more diverse the users of an NLP function, the more significant this risk becomes, such as in government services, healthcare and HR interactions. Training datasets scraped from the web, for example, are prone to bias.

Misinterpretation
As in programming, there is a risk of garbage in, garbage out (GIGO). Speech recognition, also known as speech-to-text, is the task of reliably converting voice data into text data. But NLP solutions can become confused if spoken input is in an obscure dialect, mumbled, too full of slang, homonyms, incorrect grammar, idioms, fragments, mispronunciations, contractions or recorded with too much background noise.

New vocabulary
New words are continually being invented or imported. The conventions of grammar can evolve or be intentionally broken. In these cases, NLP can either make a best guess or admit it’s unsure and either way, this creates a complication.

Tone of voice
When people speak, their verbal delivery or even body language can give an entirely different meaning than the words alone. Exaggeration for effect, stressing words for importance or sarcasm can be confused by NLP, making the semantic analysis more difficult and less reliable.

NLP use cases by industry
NLP applications can now be found across virtually every industry.

Finance
In financial dealings, nanoseconds might make the difference between success and failure when accessing data, or making trades or deals. NLP can speed the mining of information from financial statements, annual and regulatory reports, news releases or even social media.

Healthcare
New medical insights and breakthroughs can arrive faster than many healthcare professionals can keep up. NLP and AI-based tools can help speed the analysis of health records and medical research papers, making better-informed medical decisions possible, or assisting in the detection or even prevention of medical conditions.

Insurance
NLP can analyze claims to look for patterns that can identify areas of concern and find inefficiencies in claims processing, leading to greater optimization of processing and employee efforts.

Legal
Almost any legal case might require reviewing mounds of paperwork, background information and legal precedent. NLP can help automate legal discovery, assisting in the organization of information, speeding review and making sure that all relevant details are captured for consideration.



Statistics and Probability
Beginner Level Questions
1. What is Marginal Probability?
Marginal probability is simply the chance of one specific event happening, without worrying about what happens with other events. For example, if you’re looking at the probability of it raining tomorrow, you only care about the chance of rain, not what happens with other weather conditions like wind or temperature.

2. What are the Probability Axioms?
The probability axioms are just basic rules that help us understand how probabilities work. There are three main ones:

Non-Negativity Axiom: Probabilities can't be negative. The chance of something happening is always 0 or more, never less.
Normalization Axiom: If something is certain to happen (like the sun rising tomorrow), its probability is 1. So, 1 means "definitely happening."
Additivity Axiom: If two events can't happen at the same time (like rolling a 3 or a 4 on a die), the chance of either one happening is just the sum of their individual chances.
3. What is the difference between Dependent and Independent Events in Probability?
Independent Events: Two events are independent if one event doesn't change the likelihood of the other happening. For example, flipping a coin twice – the first flip doesn't affect the second flip. So, the probability of both events happening is just the product of their individual probabilities.
Dependent Events: Two events are dependent if one event affects the likelihood of the other happening. For example, if you draw a card from a deck and don't put it back (without replacement), the chance of drawing a second card depends on what the first card was. The probability changes because one card was already taken out.
4. What is Conditional Probability?
Conditional probability refers to the probability of an event occurring given that another event has already occurred. Mathematically, it is defined as the probability of event A occurring, given that event B has occurred and is denoted by 
P
(
A
∣
B
)
P(A∣B). The formula for conditional probability is:

P
(
A
∣
B
)
=
P
(
A
∩
B
)
P
(
B
)
P(A∣B)= 
P(B)
P(A∩B)
​
 

where:

P(A|B) is the conditional probability of event A given event B.
P
(
A
∩
B
)
P(A∩B) is the joint probability of both events A and B occurring simultaneously.
P(B)  is the probability of event B occurring.
5. What is Bayes’ Theorem and when do we use it in Data Science?
Bayes' Theorem helps us figure out the probability of an event happening based on some prior knowledge or evidence. It’s like updating our guess about something when we learn new things. The formula for Bayes' Theorem is:

P
(
A
∣
B
)
=
P
(
B
∣
A
)
⋅
P
(
A
)
P
(
B
)
P(A∣B)= 
P(B)
P(B∣A)⋅P(A)
​
 

Where:

P
(
A
∣
B
)
P(A∣B) is the probability of event A happening, given that B has happened.
P
(
B
∣
A
)
P(B∣A) is the probability of event B happening, given that A happens.
P
(
A
)
P(A) is the probability of event A happening, regardless of B.
P
(
B
)
P(B) is the probability of event B happening, regardless of A.
6. Define Variance and Conditional Variance.
Variance is a way to measure how spread out or different the numbers in a dataset are from the average. If the numbers are all close to the average, the variance is low. If the numbers are spread far apart, the variance is high. Think of it like measuring how much everyone’s test score differs from the average score in a class.
Conditional Variance is similar, but it looks at how much a variable changes when we know something else about it. For example, imagine you want to know how much people's height varies based on their age. The conditional variance would tell you how much height changes for a specific age group, using the knowledge of age to focus on the variability within that group.
7. Explain the concepts of Mean, Median, Mode and Standard Deviation.
Mean: The mean is simply the average of a set of numbers. To find it, you add up all the numbers and divide by how many numbers there are. It gives you a central value that represents the overall data.
Median: The median is the middle number when you arrange the data in order from smallest to largest. If there’s an even number of numbers, you average the two middle numbers. The median is useful because it’s not affected by extremely high or low values, making it a better measure of the "middle" when there are outliers.
Mode: The mode is the number that appears the most often in your data. You can have one mode, more than one mode or no mode at all if all the numbers appear equally often.
Standard Deviation: Standard deviation tells us how spread out the numbers are. If the numbers are close to the average, the standard deviation is small. If they’re more spread out, it’s large. It shows us how much variation or "scatter" there is in the data.
8. What is Normal Distribution and Standard Normal Distribution?
Normal Distribution: A normal distribution is a bell-shaped curve that shows how most data points are close to the average (mean) and the further away you go from the mean, the less likely those data points are. It’s a common pattern in nature like people's heights or test scores.
Standard Normal Distribution: This is a special type of normal distribution where the mean is 0 and the standard deviation is 1. It helps make comparisons between different sets of data easier because the data is standardized.
Intermediate Level Questions
9. What is the difference between correlation and causation?
Correlation means that two things are related or happen at the same time, but one doesn’t necessarily cause the other. For example, if people eat more ice cream in summer and also go swimming more, there's a correlation between the two, but eating ice cream doesn’t cause swimming. They just both happen together.

Causation means one thing directly causes the other to happen. For example, if you study more, your test scores will likely improve. In this case, studying causes better test scores. To prove causation, you need more evidence, often from experiments, to show that one thing is actually causing the other.

Click here to learn more about the topic: Correlation vs Causation

10. What are Uniform, Bernoulli and Binomial Distributions and how do they differ?
Uniform Distribution: Uniform distribution means that every possible outcome has an equal chance of occurring. For example, when rolling a fair six-sided die, each number (1 through 6) has the same probability of showing up, resulting in a flat line when graphed.
Bernoulli Distribution: Bernoulli distribution is used in situations where there are only two possible outcomes such as success or failure. A common example is flipping a coin where you either get heads (success) or tails (failure).
Binomial Distribution: Binomial distribution applies when you perform a set number of independent trials, each with two possible outcomes. It helps calculate the probability of getting a specific number of successes across multiple trials such as flipping a coin 5 times and determining the chance of getting exactly 3 heads.
11. Explain the Exponential Distribution and where it’s commonly used.
The Exponential distribution helps us understand the time between random events that happen at a constant rate. For example, it can show how long you might have to wait for the next customer to arrive at a store or how long a light bulb will last before it burns out.

12. Describe the Poisson Distribution and its characteristics.
The Poisson distribution tells us how often an event happens within a certain period of time or space. It’s used when events happen at a steady rate like how many cars pass by a toll booth in an hour.

Key points:

It counts the number of events that happen.
The events happen at a constant rate.
Each event is independent, meaning one event doesn’t affect the others.
13. Explain the t-distribution and its relationship with the normal distribution.
Thet-distribution is similar to the normal distribution, but it’s used when we don’t have much data and don’t know the exact spread of the population. It’s wider and more spread out than the normal distribution, but as we get more data, it looks more like the normal distribution.

14. Describe the chi-squared distribution.
The chi-squared distributionis used when we want to test how well our data matches a certain pattern or to see if two things are related. It’s often used in tests like checking if dice rolls are fair or if two factors like age and voting preference, are linked.

15. What is the difference between z-test, F-test and t-test?
Z-test: We use the z-test when we want to compare the average of a sample to a known average of a larger population and we know the population's spread (standard deviation). It’s typically used with large samples or when we have good information about the population.
T-test: The t-test is similar to the z-test, but it's used when we don't know the population’s spread (standard deviation). It’s often used with smaller samples or when we don’t have enough data to know the population’s spread.
F-test: The F-test is used when we want to compare how much the data is spread out (variance) in two or more groups. For example, you might use it to see if two different teaching methods lead to different results in students.
16. What is the central limit theorem and why is it significant in statistics?
The Central Limit Theorem (CLT) says that if you take many samples from a population, no matter how the population looks, the average of those samples will start to look like a normal (bell-shaped) distribution as the sample size gets bigger. This is important because it means we can use normal distribution rules to make predictions, even if the population itself doesn’t look normal.

Advanced Level Questions
17. Describe the process of hypothesis testing, including null and alternative hypotheses.
Hypothesis testing helps us decide if a claim about a population is likely to be true, based on sample data.

Null Hypothesis (H0): This is the "no effect" assumption, meaning nothing is happening or nothing has changed.
Alternative Hypothesis (H1): This is the opposite, suggesting there is a change or effect.
We collect data and check if it supports the alternative hypothesis or not. If the data shows enough evidence, we reject the null hypothesis.

18. How do you calculate a confidence interval and what does it represent?
A confidence interval gives us a range of values that we believe the true population value lies in, based on our sample data.

To calculate: You first collect sample data, then calculate the sample mean and margin of error (how much the sample result could vary). The confidence interval is the range around the mean where the true population value should be, with a certain level of confidence (like 95%).

19. What is a p-value in statistics?
A p-value tells us how likely it is that we would get the data we have if the null hypothesis were true. A small p-value (less than 0.05) means the data is unlikely under the null hypothesis, so we may reject the null hypothesis. A large p-value means the data fits with the null hypothesis, so we don’t reject it.

20. Explain Type I and Type II errors in hypothesis testing.
Type I Error (False Positive): Mistakenly reject a true null hypothesis, thinking something has changed when it hasn’t.
Type II Error (False Negative): Fail to reject a false null hypothesis, missing a real effect.
21. What is the significance level (alpha) in hypothesis testing?
The significance level (alpha) is the threshold you set to decide when to reject the null hypothesis. It shows how much risk you're willing to take for a Type I error (wrongly rejecting the null hypothesis). Commonly, alpha is 0.05, meaning there’s a 5% chance of making a Type I error.

22. How can you calculate the correlation coefficient between two variables?
The correlation coefficient measures how strongly two variables are related.

To calculate it, you:

Collect data for both variables.
Find the average for each variable.
Calculate how much the variables move together (covariance).
Divide by the standard deviations to standardize the result.
This gives you a number between -1 and 1 where 1 means a perfect positive relationship, -1 means a perfect negative relationship and 0 means no relationship.

23. What is covariance and how is it related to correlation?
Covariance shows how two variables change together. If both increase together, covariance is positive and if one increases while the other decreases, it’s negative. However, it depends on the scale of the variables, so it's harder to compare across different data.
Correlation standardizes covariance by using the standard deviations of the variables. It’s easier to interpret because it gives you a number between -1 and 1 that shows the strength and direction of the relationship.
24. Explain how to perform a hypothesis test for comparing two population means.
When comparing two population means, we:

1. Set up hypotheses:

Null hypothesis (H0): The two means are equal.
Alternative hypothesis (H1): The two means are different.
Collect data from both populations.
2. Calculate the test statistic (often using a t-test or z-test).

3. Compare the results to see if the difference is statistically significant.

4. If the results show a big enough difference, we reject the null hypothesis.

25. Explain multivariate distribution in data science.
A multivariate distribution involves multiple variables and it helps us model situations where we care about the relationships between those variables. For example, predicting house prices based on factors like size, location and age of the house. It’s a way to see how different features or variables work together and affect the outcome.

26. Describe the concept of conditional probability density function (PDF).
A conditional probability density function (PDF) describes the probability of an event happening, given that we already know some other event has occurred. For example, it tells us the chance of a person getting a disease given they have a certain symptom. It helps us understand how one event affects the probability of another.

27. What is the cumulative distribution function (CDF) and how is it related to PDF?
The probability that a continuous random variable will take on particular values within a range is described by the Probability Density Function (PDF), whereas the Cumulative Distribution Function (CDF) provides the cumulative probability that the random variable will fall below a given value. Both of these concepts are used in probability theory and statistics to describe and analyse probability distributions. The PDF is the CDF’s derivative and they are related by integration and differentiation.

28. What is ANOVA? What are the different ways to perform ANOVA tests?
The statistical method known as ANOVA or Analysis of Variance, is used to examine the variation in a dataset and determine whether there are statistically significant variations between group averages. When comparing the means of several groups or treatments to find out if there are any notable differences, this method is frequently used.

There are several different ways to perform ANOVA tests, each suited for different types of experimental designs and data structures:

One-Way ANOVA
Two-Way ANOVA
When conducting ANOVA tests we typically calculate an F-statistic and compare it to a critical value or use it to calculate a p-value.

29. What is the difference between a population and a sample in statistics?
Population: This is the whole group you want to study. For example, if you're looking at the average height of all students in a school, the population is every student in that school.
Sample: A sample is just a smaller part of the population. Since it's often not possible to study everyone, you choose a few people from the group to represent the whole population. For example, you might measure the height of 100 students and use that data to estimate the average height of all students.
Machine Learning
Beginner Level Questions
30. What are the different types of machine learning?
Supervised Learning: In supervised learning, the computer is given data that already has the correct answers (called labels). For example, you show it pictures of dogs and cats and each picture is labeled as "dog" or "cat." The computer learns from these labeled examples so it can correctly identify new pictures on its own. It's like teaching with a quiz where the answers are already provided.

Unsupervised Learning: In unsupervised learning, the computer is given data without any answers. It has to figure out patterns or groups by itself. For example, you might give it a bunch of photos and the computer might group all the dog pictures together and all the cat pictures together, even though you didn’t tell it what they were.

31. What is linear regression and what are the different assumptions of linear regression algorithms?
Linear Regression is type of Supervised Learning where we compute a linear relationship between the predictor and response variable. It is based on the linear equation concept given by:

y
^
=
β
1
x
+
β
o
y
^
​
 =β 
1
​
 x+β 
o
​
 ,

where

y
^
y
^
​
  = response / dependent variable
β
1
β 
1
​
  = slope of the linear regression
β
o
β 
o
​
  = intercept for linear regression
x
x = predictor / independent variable(s)
There are 4 assumptions we make about a Linear regression problem:

Linear relationship : This assumes that there is a linear relationship between predictor and response variable. This means that which changing values of predictor variable, the response variable changes linearly (either increases or decreases).
Normality : This assumes that the dataset is normally distributed, i.e., the data is symmetric about the mean of the dataset.
Independence : The features are independent of each other, there is no correlation among the features/predictor variables of the dataset.
Homoscedasticity : This assumes that the dataset has equal variance for all the predictor variables. This means that the amount of independent variables have no effect on the variance of data.
32. Logistic Regression is a classification technique and why is its name regression not Logistic Classification?
While logistic regression is used for classification it still maintains a regression structure underneath. The key idea is to model the probability of an event occurring (e.g., class 1 in binary classification) using a linear combination of features and then apply a logistic (Sigmoid) function to transform this linear combination into a probability between 0 and 1. This transformation is what makes it suitable for classification tasks.

In short while logistic regression is indeed used for classification, it retains the mathematical and structural characteristics of a regression model.

33. What is the Logistic function (Sigmoid function) in logistic regression?
The logistic function or sigmoid function, is used in logistic regression to predict probabilities. It takes any real number as input and maps it to a value between 0 and 1 which makes it great for predicting binary outcomes like "yes" or "no."

The formula looks like this:

f
(
x
)
=
1
1
+
e
−
x
f(x)= 
1+e 
−x
 
1
​
 

The sigmoid function helps us predict the probability of an event happening. If the output is close to 1, we predict one class and if it's close to 0, we predict the other.

Sigmoid
Sigmoid Function
34. What is overfitting and how can be overcome this?
Overfitting refers to the result of analysis of a dataset which fits so closely with training data that it fails to generalize with unseen/future data. This happens when the model is trained with noisy data which causes it to learn the noisy features from the training as well.

To avoid Overfitting and overcome this problem in machine learning, one can follow the following rules:

Feature selection : Sometimes the training data has too many features which might not be necessary for our problem statement. In that case, we use only the necessary features that serve our purpose
Cross Validation : This technique is a very powerful method to overcome overfitting. In this, the training dataset is divided into a set of mini training batches which are used to tune the model.
Regularization : Regularization is the technique to supplement the loss with a penalty term so as to reduce overfitting. This penalty term regulates the overall loss function, thus creating a well trained model.
Ensemble models : These models learn the features and combine the results from different training models into a single prediction.
Intermediate Level Questions
35. What is a support vector machine (SVM) and what are its key components?
Support Vector machines are a type of Supervised algorithm which can be used for both Regression and Classification problems. In SVMs, the main goal is to find a hyperplane which will be used to segregate different data points into classes. Any new data point will be classified based on this defined hyperplane.

Support Vector machines are highly effective when dealing with high dimensionality space and can handle non linear data very well. But if the number of features are greater than number of data samples, it is susceptible to overfitting.

The key components of SVM are:

Kernels Function: It is a mapping function used for data points to convert it into high dimensionality feature space.
Hyperplane: It is the decision boundary which is used to differentiate between the classes of data points.
Margin: It is the distance between Support Vector and Hyperplane
C: It is a regularization parameter which is used for margin maximization and misclassification minimization.
36. Explain the k-nearest neighbors (KNN) algorithm.
The k-Nearest Neighbors (KNN) algorithm is a simple and versatile supervised machine learning algorithm used for both classification and regression tasks. KNN makes predictions by memorizing the data points rather than building a model about it. This is why it is also called “lazy learner” or “memory based” model too.

KNN relies on the principle that similar data points tend to belong to the same class or have similar target values. This means that, In the training phase, KNN stores the entire dataset consisting of feature vectors and their corresponding class labels (for classification) or target values (for regression). It then calculates the distances between that point and all the points in the training dataset. (commonly used distance metrics are Euclidean distance and Manhattan distance).

(Note : Choosing an appropriate value for k is crucial. A small k may result in noisy predictions while a large k can smooth out the decision boundaries. The choice of distance metric and feature scaling also impact KNN’s performance.)

37. What is the Naïve Bayes algorithm and what are the different assumptions of Naive Bayes?
The Naïve Bayes algorithm is a probabilistic classification algorithm based on Bayes’ theorem with a “naïve” assumption of feature independence within each class. It is commonly used for both binary and multi-class classification tasks, particularly in situations where simplicity, speed and efficiency are essential.

The main assumptions that Naïve Bayes theorem makes are:

Feature independence – It assumes that the features involved in Naïve Bayes algorithm are conditionally independent, i.e., the presence/ absence of one feature does not affect any other feature
Equality – This assumes that the features are equal in terms of importance (or weight).
Normality – It assumes that the feature distribution is Normal in nature, i.e., the data is distributed equally around its mean.
38. What are Decision Trees and how do they work?
Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They work by creating a tree-like structure of decisions based on input features to make predictions or decisions. Lets dive into its core concepts and how they work briefly:

Decision trees consist of nodes and edges.
The tree starts with a root node and branches into internal nodes that represent features or attributes.
These nodes contain decision rules that split the data into subsets.
Edges connect nodes and indicate the possible decisions or outcomes.
Leaf nodes represent the final predictions or decisions.
Decision-Tree

The objective is to increase data homogeneity which is often measured using standards like mean squared error (for regression) or Gini impurity (for classification). Decision trees can handle a variety of attributes and can effectively capture complex data relationships. They can, however, overfit, especially when deep or complex. To reduce overfitting, strategies like pruning and restricting tree depth are applied.

39. Explain the concepts of Entropy and Information gain in decision trees.
Entropy is like a measure of how mixed or uncertain your data is. If all the data points belong to the same class, entropy is low. If the data is spread out across many different classes, entropy is high. Formula for entropy is:

H
(
S
)
=
−
∑
i
=
1
n
p
i
log
⁡
2
(
p
i
)
H(S)=−∑ 
i=1
n
​
 p 
i
​
 log 
2
​
 (p 
i
​
 )

where,

p
i
p 
i
​
  is the probability of each class in the dataset.
Information Gain tells us how much we reduce that uncertainty after we split the data using a feature. A higher information gain means the feature helps us organize the data better and makes it easier to predict the target class. It's the difference between the uncertainty before and after the split. Formula for information gain is:

Information Gain
=
H
(
S
)
−
∑
i
=
1
k
∣
S
i
∣
∣
S
∣
H
(
S
i
)
Information Gain=H(S)−∑ 
i=1
k
​
  
∣S∣
∣S 
i
​
 ∣
​
 H(S 
i
​
 )

where,

H(S) is the entropy before the split,
H(Si) is the entropy of the subsets after the split,
|Si| is the number of instances in subset, and
|S| is the total number of instances in the dataset.
40. What is the difference between the Bagging and Boosting model?
Bagging and Boosting are two techniques used to improve the accuracy of machine learning models by combining multiple models together, but they work in different ways.

In Bagging, we train several models independently on different random parts of the data. Each model makes its own predictions and then we combine those predictions by either averaging or voting. Example: Random Forest Algorithm.

In Boosting, models are trained one after another. Each new model tries to fix the mistakes of the previous one and the final prediction is based on a combination of all models where better models are given more weight. Example: AdaBoost and Gradient Boosting.

41. Describe Random Forests and their advantages over Single-Decision Trees.
Random Forests are an ensemble learning technique that combines multiple decision trees to improve predictive accuracy and reduce overfitting. The advantages it has over single decision trees are:

Improved Generalization: Single decision trees are prone to overfitting, especially when they become deep and complex. Random Forests mitigate this issue by averaging predictions from multiple trees, resulting in a more generalized model that performs better on unseen data
Better Handling of High-Dimensional Data : Random Forests are effective at handling datasets with a large number of features. They select a random subset of features for each tree which can improve the performance when there are many irrelevant or noisy features
Robustness to Outliers: Random Forests are more robust to outliers because they combine predictions from multiple trees which can better handle extreme cases
42. What is K-Means and how does it work?
K-Means is an unsupervised machine learning algorithm used for clustering or grouping similar data points together. It aims to partition a dataset into K clusters where each cluster represents a group of data points that are close to each other in terms of some similarity measure. The working of K-means is as follow:

Choose the number of clusters K
For each data point in the dataset, calculate its distance to each of the K centroids and then assign each data point to the cluster whose centroid is closest to it
Recalculate the centroids of the K clusters based on the current assignment of data points.
Repeat the above steps until a group of clusters are formed.
43. What is a Confusion Matrix? Explain with an example.
Confusion matrix is a table used to evaluate the performance of a classification model by presenting a comprehensive view of the model’s predictions compared to the actual class labels. It provides valuable information for assessing the model’s accuracy, precision, recall and other performance metrics in a binary or multi-class classification problem.

A famous example demonstration would be Cancer Confusion matrix:



Actual Cancer

Actual Not Cancer

Predicted Cancer

True Positive (TP)

False Positive (FP)

Predicted Not Cancer

False Negative (FN)

True Negative (TN)

TP (True Positive) = The number of instances correctly predicted as the positive class
TN (True Negative) = The number of instances correctly predicted as the negative class
FP (False Positive) = The number of instances incorrectly predicted as the positive class
FN (False Negative) = The number of instances incorrectly predicted as the negative class
44. What is a classification report and explain the parameters used to interpret the result of classification tasks with an example.
A classification report is a summary of the performance of a classification model, providing various metrics that help assess the quality of the model’s predictions on a classification task.

The parameters used in a classification report typically include:

Precision: Precision is the ratio of true positive predictions to the total predicted positives. It measures the accuracy of positive predictions made by the model.
Precision = TP/(TP+FP)

Recall (Sensitivity or True Positive Rate): Recall is the ratio of true positive predictions to the total actual positives. It measures the model’s ability to identify all positive instances correctly.
Recall = TP / (TP + FN)

Accuracy: Accuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. It measures the overall correctness of the model’s predictions.
Accuracy = (TP + TN) / (TP + TN + FP + FN)

F1-Score: The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure of both precision and recall and is particularly useful when dealing with imbalanced datasets.
F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

45. What is Regularization in Machine Learning? State the differences between L1 and L2 regularization.
Regularization is a technique used to prevent a model from becoming too complex and overfitting the training data. It adds a penalty to the model's cost function to keep the model simpler, helping it perform better on new, unseen data.

There are two common types of regularization: L1 and L2

L1 Regularization (Lasso): It adds the absolute value of the model's coefficients to the cost function. L1 encourages sparsity, meaning it can make some feature weights exactly zero, effectively removing those features from the model. This is useful for feature selection.

L2 Regularization (Ridge): It adds the square of the coefficients to the cost function. L2 reduces the size of all coefficients but doesn’t set them to zero, keeping all features in the model but making them less influential.

46. Explain the concepts of Bias-Variance trade-off in machine learning.
When creating predictive models, the bias-variance trade-off is a key concept in machine learning that deals with finding the right balance between two sources of error, bias and variance. It plays a crucial role in model selection and understanding the generalization performance of a machine learning algorithm. Here’s an explanation of these concepts:

Bias: Bias is simply described as the model’s inability to forecast the real value due of some difference or inaccuracy. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias.
Variance: Variance is a measure of data dispersion from its mean location. In machine learning, variance is the amount by which a predictive model’s performance differs when trained on different subsets of the training data. More specifically, variance is the model’s variability in terms of how sensitive it is to another subset of the training dataset, i.e. how much it can adapt on the new subset of the training dataset.
Low Bias

High Bias

Low Variance

Best fit (Ideal Scenario )

Underfitting

High Variance

Overfitting

Not capture the underlying patterns (Worst Case)

As a Data Scientist, the goal is to find a model that makes good predictions without overfitting to the data. Low bias means the model fits the data well while low variance means it doesn’t change too much with different data. Too simple a model may not fit the data well (high bias) while too complex a model may overfit (high variance). The bias-variance trade-off is about finding the right balance for the best results.

ML--Bias-Vs-Variance
Bias-Variance Trade-Off
47. How does Naive Bayes handle categorical and continuous features?
Naive Bayes is a method that calculates the probability of each class based on the features, assuming that the features are independent of each other.

For categorical features like colors or yes/no answers, Naive Bayes looks at how often each category appears within each class and uses that information to calculate the probability for each class.
For continuous features like height or weight, Naive Bayes assumes the data follows a normal (bell curve) distribution. It uses the average and spread (standard deviation) of the data for each class to calculate the probability.
Finally, it selects the class with the highest probability as the prediction for new data.

48. What is Laplace smoothing (add-one smoothing) and why is it used in Naive Bayes?
In Naïve Bayes, the conditional probability of an event given a class label is determined as P(event| class). When using this in a classification problem (let’s say a text classification), there could a word which did not appear in the particular class. In those cases, the probability of feature given a class label will be zero. This could create a big problem when getting predictions out of the training data.

To overcome this problem, we use Laplace smoothing. Laplace smoothing addresses the zero probability problem by adding a small constant (usually 1) to the count of each feature in each class and to the total count of features in each class. Without smoothing, if any feature is missing in a class, the probability of that class given the features becomes zero, making the classifier overly confident and potentially leading to incorrect classifications.

49. What are imbalanced datasets and how can we handle them?
Imbalanced datasets are datasets in which the distribution of class labels (or target values) is heavily skewed, meaning that one class has significantly more instances than any other class. Imbalanced datasets pose challenges because models trained on such data can have a bias toward the majority class, leading to poor performance on the minority class which is often of greater interest. This will lead to the model not generalizing well on the unseen data.

To handle imbalanced datasets, we can approach the following methods:

1. Resampling (Method of either increasing or decreasing the number of samples):

Up-sampling: In this case, we can increase the classes for minority by either sampling without replacement or generating synthetic examples.
Down-sampling: Another case would be to randomly cut down the majority class such that it is comparable to minority class.
2. Ensemble methods (using models which are capable of handling imbalanced dataset inherently):

Bagging : Techniques like Random Forests which can mitigate the impact of class imbalance by constructing multiple decision trees from bootstrapped samples
Boosting: Algorithms like AdaBoost and XGBoost can give more importance to misclassified minority class examples in each iteration, improving their representation in the final model
50. What are outliers in the dataset and how can we detect and remove them?
An Outlier is a data point that is significantly different from other data points. Usually, Outliers are present in the extremes of the distribution and stand out as compared to their out data point counterparts.

For detecting Outliers we can use the following approaches:

Visual inspection: This is the easiest way which involves plotting the data points into scatter plot/box plot, etc.
statistics: By using measure of central tendency, we can determine if a data point falls significantly far from its mean, median, etc. making it a potential outlier.
Z-score: if a data point has very high Z-score, it can be identified as Outlier
For removing the outliers, we can use the following:

Removal of outliers manually
Doing transformations like applying logarithmic transformation or square rooting the outlier
Performing imputations wherein the outliers are replaced with different values like mean, median, mode, etc.
51. What is the curse of dimensionality and how can we overcome this?
When dealing with a dataset that has high dimensionality (high number of features), we are often encountered with various issues and problems. Some of the issues faced while dealing with dimensionality dataset are listed below:

Computational expense: The biggest problem with handling a dataset with vast number of features is that it takes a long time to process and train the model on it. This can lead to wastage of both time and monetary resources.
Data sparsity: Many times data points are far from each other (high sparsity). This makes it harder to find the underlying patterns between features and can be a hinderance in proper analysis
Visualising issues and Overfitting: It is rather easy to visualize 2d and 3d data. But beyond this order, it is difficult to properly visualize our data. Furthermore, more data features can be correlated and provide misleading information to the model training and cause overfitting.
These issues are what are generally termed as “Curse of Dimensionality”.

To overcome this, we can follow different approaches – some of which are mentioned below:

Feature Selection: Many a times, not all the features are necessary. It is the user’s job to select out the features that would be necessary in solving a given problem statement.
Feature engineering: Sometimes, we may need a feature that is the combination of many other features. This method can, in general, reduces the features count in the dataset.
Dimensionality Reduction techniques: These techniques reduce the number of features in a dataset while preserving as much useful information as possible. Some of the famous Dimensionality reduction techniques are: Principle component analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), etc.
Regularization: Some regularization techniques like L1 and L2 regularizations are useful when deciding the impact each feature has on the model training.
52. Describe gradient descent and its role in optimizing machine learning models.
Gradient descent is a fundamental optimization algorithm used to minimize a cost or loss function in machine learning and deep learning. Its primary role is to iteratively adjust the parameters of a machine learning model to find the values that minimize the cost function, thereby improving the model’s predictive performance. Here’s how Gradient descent help in optimizing Machine learning models:

Minimizing Cost functions: The primary goal of gradient descent is to find parameter values that result in the lowest possible loss on the training data.
Convergence: The algorithm continues to iterate and update the parameters until it meets a predefined convergence criterion which can be a maximum number of iterations or achieving a desired level of accuracy.
Generalization: Gradient descent ensure that the optimized model generalizes well to new, unseen data.
Advanced Level Questions
53. How does the random forest algorithm handle feature selection?
Mentioned below is how Random forest handles feature selection

When creating individual trees in the Random Forest ensemble, a subset of features is assigned to each tree which is called Feature Bagging. Feature Bagging introduces randomness and diversity among the trees.
After the training, the features are assigned a “importance score” based on how well those features performed by reducing the error of the model. Features that consistently contribute to improving the model’s accuracy across multiple trees are deemed more important
Then the features are ranked based on their importance scores. Features with higher importance scores are considered more influential in making predictions.
54. What is Feature Engineering? Explain the different feature engineering methods.
Feature Engineering can be defined as a method of preprocessing of data for better analysis purpose which involves different steps like selection, transformation, deletion of features to suit our problem at hand. Feature Engineering is a useful tool which can be used for:

Improving the model’s performance and Data interpretability
Reduce computational costs
Include hidden patterns for elevated Analysis results
Some of the different methods of doing feature engineering are mentioned below:

Principle Component Analysis (PCA) : It identifies orthogonal axes (principal components) in the data that capture the maximum variance, thereby reducing the data features.
Encoding: It is a technique of converting the data to be represented a numbers with some meaning behind it. It can be done in two ways :
Feature Transformation: Sometimes, we can create new columns essential for better modelling just by combining or modifying one or more columns.
55. How we will deal with the categorical text values in machine learning?
Often times, we are encountered with data that has Categorical text values. For example, male/female, first-class/second-class/third-class, etc. These Categorical text values can be divided into two types and based on that we deal with them as follows:

If it is Categorical Nominal Data: If the data does not have any hidden order associated with it (e.g., male/female), we perform One-Hot encoding on the data to convert it into binary sequence of digits
If it is Categorical Ordinal Data : When there is a pattern associated with the text data, we use Label encoding. In this, the numerical conversion is done based on the order of the text data.
56. What is DBSCAN and how do we use it?
Density-Based Spatial Clustering of Applications with Noise (DBSCAN), is a density-based clustering algorithm used for grouping together data points that are close to each other in high-density regions and labeling data points in low-density regions as outliers or noise. Here is how it works:

For each data point in the dataset, DBSCAN calculates the distance between that point and all other data points
DBSCAN identifies dense regions by connecting core points that are within each other’s predefined threshold (eps) neighborhood.
DBSCAN forms clusters by grouping together data points that are density-reachable from one another.
57. How does the EM (Expectation-Maximization) algorithm work in clustering?
The Expectation-Maximization (EM) algorithm is a probabilistic approach used for clustering data when dealing with mixture models. EM is commonly used when the true cluster assignments are not known and when there is uncertainty about which cluster a data point belongs to. Here is how it works:

First, the number of clusters K to be formed is specified.
Then, for each data point, the likelihood of it belonging to each of the K clusters is calculated. This is called the Expectation (E) step
Based on the previous step, the model parameters are updated. This is called Maximization (M) step.
Together it is used to check for convergence by comparing the change in log-likelihood or the parameter values between iterations.
If it converges, then we have achieved our purpose. If not, then the E-step and M-step are repeated until we reach convergence.
58. Explain the concept of silhouette score in clustering evaluation.
Silhouette score is a metric used to evaluate the quality of clusters produced by a clustering algorithm. Here is how it works:

the average distance between the data point and all other data points in the same cluster is first calculated. Let us call this as (a)
Then for the same data point, the average distance (b) between the data point and all data points in the nearest neighboring cluster (i.e., the cluster to which it is not assigned)
silhouette coefficient for each data point is calculated which given by: S = (b – a) / max(a, b)
if -1<S<0, it signifies that data point is closer to a neighboring cluster than to its own cluster.
if S is close to zero, data point is on or very close to the decision boundary between two neighboring clusters.
if 0<S<1, data point is well within its own cluster and far from neighboring clusters.
59. What is the relationship between eigenvalues and eigenvectors in PCA?
In Principal Component Analysis (PCA), eigenvalues and eigenvectors play a crucial role in the transformation of the original data into a new coordinate system. Let us first define the essential terms:

Eigen Values: Eigenvalues are associated with each eigenvector and represent the magnitude of the variance (spread or extent) of the data along the corresponding eigenvector
Eigen Vectors: Eigenvectors are the directions or axes in the original feature space along which the data varies the most or exhibits the most variance
The relationship between them is given as:

A
V
=
λ
V
AV=λV, where

A = Feature matrix
V = eigen vector
λ
λ = Eigen value.
A larger eigenvalue implies that the corresponding eigenvector captures more of the variance in the data.The sum of all eigenvalues equals the total variance in the original data. Therefore, the proportion of total variance explained by each principal component can be calculated by dividing its eigenvalue by the sum of all eigenvalues

60. What is the Cross Validation technique in Machine Learning?
Cross-validation is a resampling technique used in machine learning to assess and validate the performance of a predictive model. It helps in estimating how well a model is likely to perform on unseen data, making it a crucial step in model evaluation and selection. Cross validation is usually helpful when avoiding overfitting the model. Some of the widely known cross validation techniques are:

K-Fold Cross-Validation: In this, the data is divided into K subsets and K iterations of training and testing are performed.
Stratified K-Fold Cross-Validation: This technique ensures that each fold has approximately the same proportion of classes as the original dataset (helpful in handling data imbalance)
Shuffle-Split Cross-Validation: It randomly shuffles the data and splits it into training and testing sets.
61. What are the ROC and AUC? Explain their significance in binary classification.
Receiver Operating Characteristic (ROC) is a graphical representation of a binary classifier’s performance. It plots the true positive rate (TPR) vs the false positive rate (FPR) at different classification thresholds.

True positive rate (TPR) : It is the ratio of true positive predictions to the total actual positives.

Recall = TP / (TP + FN)

False positive rate (FPR) : It is the ratio of False positive predictions to the total actual positives.

FPR= FP / (TP + FN)

Area Under the Curve (AUC) as the name suggests is the area under the ROC curve. The AUC is a scalar value that quantifies the overall performance of a binary classification model and ranges from 0 to 1 where a model with an AUC of 0.5 indicates random guessing and an AUC of 1 represents a perfect classifier.

AUC-ROC-Curve
AUC-ROC Curve
62. Describe Batch Gradient Descent, Stochastic Gradient Descent and Mini-Batch Gradient Descent.
1. Batch Gradient Descent: In Batch Gradient Descent, the entire training dataset is used to compute the gradient of the cost function with respect to the model parameters (weights and biases) in each iteration. This means that all training examples are processed before a single parameter update is made. It converges to a more accurate minimum of the cost function but can be slow, especially in a high dimensionality space.

2. Stochastic Gradient Descent: In Stochastic Gradient Descent, only one randomly selected training example is used to compute the gradient and update the parameters in each iteration. The selection of examples is done independently for each iteration. This is capable of faster updates and can handle large datasets because it processes one example at a time but high variance can cause it to converge slower.

3. Mini-Batch Gradient Descent: Mini-Batch Gradient Descent strikes a balance between BGD and SGD. It divides the training dataset into small, equally-sized subsets called mini-batches. In each iteration, a mini-batch is randomly sampled and the gradient is computed based on this mini-batch. It utilizes parallelism well and takes advantage of modern hardware like GPUs but can still exhibits some level of variance in updates compared to Batch Gradient Descent.

63. Explain the Apriori - Association Rule Mining.
Association Rule Mining is a method used to find patterns or relationships between items in large datasets like identifying which products are often bought together. Apriori is a common algorithm used for this and it works by first finding the most frequently bought items and then looking for combinations of those items that appear together often.

The key idea behind Apriori is the Apriori Property which says that if a group of items is frequently bought together, all smaller groups of those items should also be frequent. For example, if people often buy bread and butter together, Apriori can help identify this pattern and suggest that if someone buys bread, they might also buy butter.

64. How can you prevent Gradient Descent from getting stuck in local minima?
Local minima happen when the algorithm gets stuck in a small minimum point, instead of finding the best solution. To avoid this:

Good Starting Point: Use techniques like Xavier/Glorot or He initialization to set good starting values for the model’s weights.
Smart Optimizers: Use optimizers like Adam or RMSProp that adjust the learning speed based on past steps, helping the algorithm move past local minima.
Randomness with Mini-Batches: Use mini-batch gradient descent which introduces some randomness allowing the algorithm to jump out of local minima.
Increase Model Complexity: Adding more layers or neurons can create a more complex model, reducing the chance of getting stuck.
Tune Hyperparameters: Try different settings for the model using methods like random search or grid search to find the best ones.
65. Explain the Gradient Boosting algorithms in Machine Learning.
Gradient Boosting techniques like XGBoost and CatBoost are used for regression and classification problems. It is a boosting algorithm that combines the predictions of weak learners to create a strong model. The key steps involved in gradient boosting are:

Initialize the model with weak learners such as a decision tree.
Calculate the difference between the target value and predicted value made by the current model.
Add a new weak learner to calculate residuals and capture the errors made by the current ensemble.
Update the model by adding fraction of the new weak learner’s predictions. This updating process can be controlled by learning rate.
Repeat the process from step 2 to 4, with each iteration focusing on correcting the errors made by the previous model.
SQL and DBMS
Beginner Level Questions
66. What is SQL, and what does it stand for?
SQL stands for Structured Query Language.It is a specialized programming language used for managing and manipulating relational databases. It is designed for tasks related to database management, data retrieval, data manipulation and data definition.

67. Explain the differences between SQL and NoSQL databases.
Both SQL (Structured Query Language) and NoSQL (Not Only SQL) databases, differ in their data structures, schema, query languages and use cases. The following are the main variations between SQL and NoSQL databases.

SQL

NoSQL

SQL databases are relational databases, they organise and store data using a structured schema with tables, rows and columns.

NoSQL databases use a number of different types of data models such as document-based (like JSON and BSON), key-value pairs, column families and graphs.

SQL databases have a set schema, thus before inserting data, we must establish the structure of our data.The schema may need to be changed which might be a difficult process.

NoSQL databases frequently employ a dynamic or schema-less approach, enabling you to insert data without first creating a predetermined schema.

SQL is a strong and standardised query language that is used by SQL databases. Joins, aggregations and subqueries are only a few of the complicated processes supported by SQL queries.

The query languages or APIs used by NoSQL databases are frequently tailored to the data model.

68. What are the primary SQL database management systems (DBMS)?
Relational database systems, both open source and commercial, are the main SQL (Structured Query Language) database management systems (DBMS) which are widely used for managing and processing structured data. Some of the most popular SQL database management systems are listed below:

MySQL
Microsoft SQL Server
SQLite
PostgreSQL
Oracle Database
Amazon RDS
69. What is the ER model in SQL?
The structure and relationships between the data entities in a database are represented by the Entity-Relationship (ER) model, a conceptual framework used in database architecture. The ER model is frequently used in combination with SQL for creating the structure of relational databases even though it is not a component of the SQL language itself.

70. What is Data Transformation?
The process of transforming data from one structure, format or representation into another is referred to as data transformation. In order to make the data more suited for a given goal such as analysis, visualisation, reporting or storage, this procedure may involve a variety of actions and changes to the data. Data integration, cleansing and analysis depend heavily on data transformation which is a common stage in data preparation and processing pipelines.

71. What are the main components of a SQL query?
A relational database’s data can be retrieved, modified or managed via a SQL (Structured Query Language) query. The operation of a SQL query is defined by a number of essential components, each of which serves a different function.

SELECT
FROM
WHERE
GROUP BY
HAVING
ORDER BY
LIMIT
JOIN
72. What is a Primary Key?
A relational database table’s main key, also known as a primary keyword, is a column that is unique for each record. It is a distinctive identifier. The primary key of a relational database must be unique. Every row of data must have a primary key value and none of the rows can be null.

Intermediate Level Questions
73. What is the purpose of the GROUP BY clause and how is it used?
In SQL, the GROUP BY clause is used to create summary rows out of rows that have the same values in a set of specified columns. In order to do computations on groups of rows as opposed to individual rows, it is frequently used in conjunction with aggregate functions like SUM, COUNT, AVG, MAX or MIN. we may produce summary reports and perform more in-depth data analysis using the GROUP BY clause.

74. What is the WHERE clause used for and how is it used to filter data?
In SQL, the WHERE clause is used to filter rows from a table or result set according to predetermined criteria. It enables us to pick only the rows that satisfy particular requirements or follow a pattern. A key element of SQL queries, the WHERE clause is frequently used for data retrieval and manipulation.

75. How do you retrieve distinct values from a column in SQL?
Using the DISTINCT keyword in combination with the SELECT command, we can extract distinct values from a column in SQL. By filtering out duplicate values and returning only unique values from the specified column, the DISTINCT keyword is used.

76. What is the HAVING clause?
To filter query results depending on the output of aggregation functions, the HAVING clause, a SQL clause, is used along with the GROUP BY clause. The HAVING clause filters groups of rows after they have been grouped by one or more columns, in contrast to the WHERE clause which filters rows before they are grouped.

77. How do you handle missing or NULL values in a database table?
Missing or NULL values can arise due to various reasons such as incomplete data entry, optional fields or data extraction processes.

Replace NULL with Placeholder Values
Handle NULL Values in Queries
Use Default Values
Advanced Level Questions
78. Explain the concept of Normalization in database design.
By minimising data duplication and enhancing data integrity, normalisation is a method in database architecture that aids in the effective organisation of data. It include dividing a big, complicated table into smaller, associated tables while making sure that connections between data elements are preserved. The basic objective of normalisation is to reduce data anomalies which can happen when data is stored in an unorganised way and include insertion, update and deletion anomalies.

79. What is Database Denormalization?
Database denormalization is the process of intentionally introducing redundancy into a relational database by merging tables or incorporating redundant data to enhance query performance. Unlike normalization which minimizes data redundancy for consistency, denormalization prioritizes query speed. By reducing the number of joins required, denormalization can improve read performance for complex queries. However, it may lead to data inconsistencies and increased maintenance complexity. Denormalization is often employed in scenarios where read-intensive operations outweigh the importance of maintaining a fully normalized database structure. Careful consideration and trade-offs are essential to strike a balance between performance and data integrity.

80. Define different types of SQL functions.
SQL functions can be categorized into several types based on their functionality.

Scalar Functions
Aggregate Functions
Window Functions
Table-Valued Functions
System Functions
User-Defined Functions
Conversion Functions
Conditional Functions
81. Explain the difference between INNER JOIN and LEFT JOIN.
INNER JOIN and LEFT JOIN are two types of SQL JOIN operations used to combine data from multiple tables in a relational database. Here are the some main differences between them.

INNER JOIN

LEFT JOIN

Only rows with a match in the designated columns between the two tables being connected are returned by an INNER JOIN.

LEFT JOIN returns all rows from the left table and the matching rows from the right table.

A row is not included in the result set if there is no match for it in either of the tables.

Columns from the right table’s rows are returned with NULL values if there is no match for that row.

When we want to retrieve data from both tables depending on a specific criterion, INNER JOIN can be helpful.

It makes sure that every row from the left table appears in the final product, even if there are no matches for that row in the right table.

82. What are Window Functions in SQL and how do they differ from regular aggregate functions?
Window Functions: A window function performs calculations over a set of rows related to the current row, but it still keeps the original rows in the result. For example, you can use a window function to calculate a running total for each row without losing the row’s original data. Some examples of window functions are ROW_NUMBER(), RANK() and SUM() with the OVER() clause.

Difference from Aggregate Functions: Regular aggregate functions like SUM(), COUNT() and AVG() group rows together and return just one result for each group. But window functions let you calculate across rows while still showing the individual rows. For example, with window functions, you can get a running total on each row while keeping all the details of each row.

83. How do you perform mathematical calculations in SQL queries?
In SQL, we can perform mathematical calculations in queries using arithmetic operators and functions. Here are some common methods for performing mathematical calculations.

Arithmetic Operators
Mathematical Functions
Aggregate Functions
Custom Expressions
84. What is the difference between a JOIN and a SUBQUERY in SQL and when would you use each?
JOIN: A JOIN is used to combine data from two or more tables based on a shared column. For example, if you have a table of customers and a table of orders, you can use a JOIN to link customer information with their orders. There are different types of joins like INNER JOIN (to get matching rows) or LEFT JOIN (to get all rows from the left table and matching ones from the right).

SUBQUERY: A SUBQUERY is a query within another query. It’s used when you need to get a value or a set of values to use in the outer query. For example, you might use a subquery to find customers who spent more than a certain amount, then use that result in another query.

85. What is the difference between a Database and a Data Warehouse?
Database: Consistency and real-time data processing are prioritised and they are optimised for storing, retrieving and managing structured data. Databases are frequently used for administrative functions like order processing, inventory control and customer interactions.

Data Warehouse: Data warehouses are made for processing analytical data. They are designed to facilitate sophisticated querying and reporting by storing and processing massive amounts of historical data from various sources. Business intelligence, data analysis and decision-making all employ data warehouses.

Deep Learning and Artificial Intelligence
Beginner Level Questions
86. Explain the convolution operations of CNN architecture.
In a Convolutional Neural Network (CNN), convolutions help the model find important features in images like edges or textures. Small filters (also called kernels) slide over the image, checking one small part at a time. These filters look for patterns by doing a calculation at each position, creating something called a feature map.

The strides control how far the filter moves at each step. This helps the network recognize the same feature, even if it's in a different part of the image. After convolutions, pooling layers shrink the feature maps, keeping the important details while making the data smaller and faster to process.

In short, convolution operations help CNNs find features in images and recognize patterns, no matter where they are in the image.

87. What is Feed Forward Network and how it is different from Recurrent Neural Network?
Deep learning designs that are basic are feedforward neural networks and recurrent neural networks. They are both employed for different tasks, but their structure and how they handle sequential data differ.

Feed Forward Neural Network

In FFNN, the information flows in one direction, from input to output, with no loops
It consists of multiple layers of neurons, typically organized into an input layer, one or more hidden layers and an output layer.
Each neuron in a layer is connected to every neuron in the subsequent layer through weighted connections.
FNNs are primarily used for tasks such as classification and regression where they take a fixed-size input and produce a corresponding output
Recurrent Neural Network

A recurrent neural network is designed to handle sequential data where the order of input elements matters. Unlike FNNs, RNNs have connections that loop back on themselves allowing them to maintain a hidden state that carries information from previous time steps.
This hidden state enables RNNs to capture temporal dependencies and context in sequential data, making them well-suited for tasks like natural language processing, time series analysis and sequence generation.
However, standard RNNs have limitations in capturing long-range dependencies due to the vanishing gradient problem.
Intermediate Level Questions
88. Explain the difference between generative and discriminative models?
Generative models and discriminative models are used for different purposes in machine learning. Generative models try to understand how data is generated, meaning they learn the relationship between input data 𝑋 and target labels 𝑌. This allows them to create new data that looks like the original dataset. These models are often used for generating new images, text or other types of data. Examples of generative models include GANs (Generative Adversarial Networks) and VAEs (Variational Autoencoders).

On the other hand, discriminative modelsfocus on distinguishing between different classes or making predictions based on the input data. They learn the relationship between the input 𝑋 and the target 𝑌 directly, without trying to generate new data. Discriminative models are typically used for tasks like classification where you need to assign a label to new data. Examples include Logistic Regression, Support Vector Machines (SVMs) and CNNs (Convolutional Neural Networks) for image classification.

89. What is the forward and backward propogations in deep learning?
Forward Propagation: This is the process of passing input data through the network to generate predictions. The data moves from the input layer through hidden layers where each neuron processes the data, applies an activation function and sends the output to the next layer. The final output layer generates the prediction.
Backward Propagation: After forward propagation, the model calculates the error by comparing the prediction to the actual result. Then, using the chain rule, it calculates the gradients of the error with respect to each parameter (weights and biases). These gradients are used to update the parameters (weights and biases) to reduce the error in future predictions.
90. Describe the use of Markov models in sequential data analysis?
Markov Models are effective methods for capturing and modeling dependencies between successive data points or states in a sequence. They are especially useful when the current condition is dependent on earlier states. The Markov property which asserts that the future state or observation depends on the current state and is independent of all prior states. There are two types of Markov models used in sequential data analysis:

Markov chains are the simplest form of Markov models, consisting of a set of states and transition probabilities between these states. Each state represents a possible condition or observation and the transition probabilities describe the likelihood of moving from one state to another.
Hidden Markov Models extend the concept of Markov chains by introducing a hidden layer of states and observable emissions associated with each hidden state. The true state of the system (hidden state) is not directly observable, but the emissions are observable.
Applications:

HMMs are used to model phonemes and words in speech recognition systems allowing for accurate transcription of spoken language
HMMs are applied in genomics for gene prediction and sequence alignment tasks. They can identify genes within DNA sequences and align sequences for evolutionary analysis.
Markov models are used in modeling financial time series data such as stock prices, to capture the dependencies between consecutive observations and make predictions.
91. What is Generative AI?
Generative AI is an abbreviation for Generative Artificial Intelligence which refers to a class of artificial intelligence systems and algorithms that are designed to generate new, unique data or material that is comparable to or indistinguishable from, human-created data. It is a subset of artificial intelligence that focuses on the creative component of AI allowing machines to develop innovative outputs such as writing, graphics, audio and more. There are several generative AI models and methodologies, each adapted to different sorts of data and applications such as:

Generative AI models such as GPT (Generative Pretrained Transformer) can generate human-like text.” Natural language synthesis, automated content production and chatbot responses are all common uses for these models.
Images are generated using generative adversarial networks (GANs).” GANs are made up of a generator network that generates images and a discriminator network that determines the authenticity of the generated images. Because of the struggle between the generator and discriminator, high-quality, realistic images are produced.
Generative AI can also create audio content such as speech synthesis and music composition.” Audio content is generated using models such as WaveGAN and Magenta.
Advanced Level Questions
92. What are different neural network architecture used to generate artificial data in deep learning?
Various neural networks are used to generate artificial data. Here are some of the neural network architectures used for generating artificial data:

GANs consist of two components – generator and discriminator which are trained simultaneously through adversarial training. They are used to generating high-quality images such as photorealistic faces, artwork and even entire scenes.
VAEs are generative models that learn a probabilistic mapping from the data space to a latent space. They also consist of encoder and decoder. They are used for generating images, reconstructing missing parts of images and generating new data samples. They are also applied in generating text and audio.
RNNs are a class of neural networks with recurrent connections that can generate sequences of data. They are often used for sequence-to-sequence tasks. They are used in text generation, speech synthesis, music composition.
Transformers are a type of neural network architecture that has gained popularity for sequence-to-sequence tasks. They use self-attention mechanisms to capture dependencies between different positions in the input data. They are used in natural language processing tasks like machine translation, text summarization and language generation.
Autoencoders are neural networks that are trained to reconstruct their input data. Variants like denoising autoencoders and contractive autoencoders can be used for data generation. They are used for image denoising, data inpainting and generating new data samples.
93. What is Deep Reinforcement Learning technique?
Deep Reinforcement Learning (DRL) is a cutting-edge machine learning technique that combines the principles of reinforcement learning with the capability of deep neural networks. Its ability to enable machines to learn difficult tasks independently by interacting with their environments, similar to how people learn via trial and error, has garnered significant attention.

DRL is made up of three fundamental components:

The agent interacts with the environment and takes decision.
The environment is the outside world with which the agent interacts and receives feedback.
The reward signal is a scalar value provided by the environment after each action, guiding the agent toward maximizing cumulative rewards over time.
Applications:

In robotics, DRL is used to control robots, manipulation and navigation.
DRL plays a role in self-driving cars and vehicle control
Can also be used for customized recommendations
94. What is transfer learning and how is it applied in deep learning?
Transfer learning is a technique where a model trained on one task is used to help solve a different, but similar task. Instead of starting from scratch, the model uses what it has already learned from a large dataset to make learning faster and easier for a new task.

The process has two main steps: feature extraction and fine-tuning. First, in feature extraction, the pretrained model is used to get useful features from the new data while ignoring the final prediction layers. Then, in fine-tuning, new layers are added to the model and the model is adjusted to fit the new task by learning from the target data. This helps save time, reduce computing power and improve the model's performance, especially when there's not much data available for the new task.

95. What is difference between Object Detection and Image Segmentation?
Object detection and Image segmentation are both computer vision tasks that entail evaluating and comprehending image content, but they serve different functions and give different sorts of information.

Object Detection:

Goal of object detection is to identify and locate objects and represent the object in bounding boxes with their respective labels.
Used in applications like autonomous driving for detecting pedestrians and vehicle
Image Segmentation:

Focuses on partitioning an image into multiple regions where each segment corresponding to a coherent part of the image.
Provide pixel level labeling of the entire image
Used in applications that require pixel level understanding such as medical image analysis for organ and tumor delineation.
96. Explain the concept of word embeddings in Natural Language Processing (NLP).
In NLP, the concept of word embedding is use to capture semantic and contextual information. Word embeddings are dense representations of words or phrases in continuous-valued vectors in a high-dimensional space. Each word is mapped to a vector with the real numbers, these vectors are learned from large corpora of text data.

Word embeddings are based on the Distributional Hypothesis which suggests that words that appear in similar context have similar meanings. This idea is used by word embedding models to generate vector representations that reflect the semantic links between words depending on how frequently they co-occur with other words in the text.

The most common word embeddings techniques are:

Bag of Words (BOW)
Word2Vec
Glove: Global Vector for word representation
Term frequency-inverse document frequency (TF-IDF)
BERT
97. What is seq2seq model?
A neural network architecture called a Sequence-to-Sequence (Seq2Seq) model is made to cope with data sequences, making it particularly helpful for jobs involving variable-length input and output sequences. Machine translation, text summarization, question answering and other tasks all benefit from its extensive use in natural language processing.

The Seq2Seq consists of two main components: encoder and decoder. The encoder takes input sequence and converts into fixed length vector . The vector captures features and context of the sequence. The decoder takes the vector as input and generated output sequence. This autoregressive technique frequently entails influencing the subsequent prediction using the preceding one.

98. What is Artificial Neural Networks?
Artificial Neural Networks take inspiration from structure and functioning of human brain. The computational units in ANN are called neurons and these neurons are responsible to process and pass the information to the next layer.

ANN has three main components:

Input Layer: where the network receives input features.
Hidden Layer: one or more layers of interconnected neurons responsible for learning patterns in the data
Output Layer: provides final output on processed information.
  

1. What is Data Science?
An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science.

The following figure represents the life cycle of data science.


It starts with gathering the business requirements and relevant data.
Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture.
Data processing does the task of exploring the data, mining it, and analyzing it which can be finally used to generate the summary of the insights extracted from the data.
Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements.
In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture. Learn More.

Create a free personalised study plan
Get into your dream companies with expert guidance
Real-Life Problems
Prep for Target Roles
Custom Plan Duration
Create My Plan
2. Define the terms KPI, lift, model fitting, robustness and DOE.
KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives.
Lift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model.
Model fitting: This indicates how well the model under consideration fits given observations.
Robustness: This represents the system’s capability to handle differences and variances effectively.
DOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables.
3. What is the difference between data analytics and data science?
Data science involves the task of transforming data by using various technical analysis methods to extract meaningful insights using which a data analyst can apply to their business scenarios.
Data analytics deals with checking the existing hypothesis and information and answers questions for a better and effective business-related decision-making process.
Data Science drives innovation by answering questions that build connections and answers for futuristic problems. Data analytics focuses on getting present meaning from existing historical context whereas data science focuses on predictive modeling.
Data Science can be considered as a broad subject that makes use of various mathematical and scientific tools and algorithms for solving complex problems whereas data analytics can be considered as a specific field dealing with specific concentrated problems using fewer tools of statistics and visualization.
The following Venn diagram depicts the difference between data science and data analytics clearly:


You can download a PDF version of Data Science Interview Questions.

Download PDF

4. What are some of the techniques used for sampling? What is the main advantage of sampling?
Data analysis can not be done on a whole volume of data at a time especially when it involves larger datasets. It becomes crucial to take some data samples that can be used for representing the whole population and then perform analysis on it. While doing this, it is very much necessary to carefully take sample data out of the huge data that truly represents the entire dataset.


There are majorly two categories of sampling techniques based on the usage of statistics, they are:

Probability Sampling techniques: Clustered sampling, Simple random sampling, Stratified sampling.
Non-Probability Sampling techniques: Quota sampling, Convenience sampling, snowball sampling, etc.
7. What are Eigenvectors and Eigenvalues?
Eigenvectors are column vectors or unit vectors whose length/magnitude is equal to 1. They are also called right vectors. Eigenvalues are coefficients that are applied on eigenvectors which give these vectors different values for length or magnitude.


A matrix can be decomposed into Eigenvectors and Eigenvalues and this process is called Eigen decomposition. These are then eventually used in machine learning methods like PCA (Principal Component Analysis) for gathering valuable insights from the given matrix.


Advance your career with  
Mock Assessments
Real-world coding challenges for top company interviews
Real-Life Problems
Detailed reports
Attempt Now
8. What does it mean when the p-values are high and low?
A p-value is the measure of the probability of having results equal to or more than the results achieved under a specific hypothesis assuming that the null hypothesis is correct. This represents the probability that the observed difference occurred randomly by chance.

Low p-value which means values ≤ 0.05 means that the null hypothesis can be rejected and the data is unlikely with true null.
High p-value, i.e values ≥ 0.05 indicates the strength in favor of the null hypothesis. It means that the data is like with true null.
p-value = 0.05 means that the hypothesis can go either way.
9. When is resampling done?
Resampling is a methodology used to sample data for improving accuracy and quantify the uncertainty of population parameters. It is done to ensure the model is good enough by training the model on different patterns of a dataset to ensure variations are handled. It is also done in the cases where models need to be validated using random subsets or when substituting labels on data points while performing tests.

10. What do you understand by Imbalanced Data?
Data is said to be highly imbalanced if it is distributed unequally across different categories. These datasets result in an error in model performance and result in inaccuracy.

11. Are there any differences between the expected value and mean value?
There are not many differences between these two, but it is to be noted that these are used in different contexts. The mean value generally refers to the probability distribution whereas the expected value is referred to in the contexts involving random variables.

12. What do you understand by Survivorship Bias?
This bias refers to the logical error while focusing on aspects that survived some process and overlooking those that did not work due to lack of prominence. This bias can lead to deriving wrong conclusions.

logo
Get Access to 250+ Guides with Scaler Mobile App!
Experience free learning content on the Scaler Mobile App
logo
4.5
100K+
Play Store
13. What is a Gradient and Gradient Descent?
Gradient: Gradient is the measure of a property that how much the output has changed with respect to a little change in the input. In other words, we can say that it is a measure of change in the weights with respect to the change in error. The gradient can be mathematically represented as the slope of a function.


Gradient Descent: Gradient descent is a minimization algorithm that minimizes the Activation function. Well, it can minimize any function given to it but it is usually provided with the activation function only. 

Gradient descent, as the name suggests means descent or a decrease in something. The analogy of gradient descent is often taken as a person climbing down a hill/mountain. The following is the equation describing what gradient descent means:

So, if a person is climbing down the hill, the next position that the climber has to come to is denoted by “b” in this equation. Then, there is a minus sign because it denotes the minimization (as gradient descent is a minimization algorithm). The Gamma is called a waiting factor and the remaining term which is the Gradient term itself shows the direction of the steepest descent. 

This situation can be represented in a graph as follows:


Here, we are somewhere at the “Initial Weights” and we want to reach the Global minimum. So, this minimization algorithm will help us do that.

14. Define confounding variables.
Confounding variables are also known as confounders. These variables are a type of extraneous variables that influence both independent and dependent variables causing spurious association and mathematical relationships between those variables that are associated but are not casually related to each other.

15. Define and explain selection bias?
The selection bias occurs in the case when the researcher has to make a decision on which participant to study. The selection bias is associated with those researches when the participant selection is not random. The selection bias is also called the selection effect. The selection bias is caused by as a result of the method of sample collection.

Four types of selection bias are explained below:

Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias.
Time interval: Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value.
Data: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed.
Attrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial.
16. Define bias-variance trade-off?
Let us first understand the meaning of bias and variance in detail:

Bias: It is a kind of error in a machine learning model when an ML Algorithm is oversimplified. When a model is trained, at that time it makes simplified assumptions so that it can easily understand the target function. Some algorithms that have low bias are Decision Trees, SVM, etc. On the other hand, logistic and linear regression algorithms are the ones with a high bias.

Variance: Variance is also a kind of error. It is introduced into an ML Model when an ML algorithm is made highly complex. This model also learns noise from the data set that is meant for training. It further performs badly on the test data set. This may lead to over lifting as well as high sensitivity.

When the complexity of a model is increased, a reduction in the error is seen. This is caused by the lower bias in the model. But, this does not happen always till we reach a particular point called the optimal point. After this point, if we keep on increasing the complexity of the model, it will be over lifted and will suffer from the problem of high variance. We can represent this situation with the help of a graph as shown below:


As you can see from the image above, before the optimal point, increasing the complexity of the model reduces the error (bias). However, after the optimal point, we see that the increase in the complexity of the machine learning model increases the variance.

Trade-off Of Bias And Variance: So, as we know that bias and variance, both are errors in machine learning models, it is very essential that any machine learning model has low variance as well as a low bias so that it can achieve good performance.

Let us see some examples. The K-Nearest Neighbor Algorithm is a good example of an algorithm with low bias and high variance. This trade-off can easily be reversed by increasing the k value which in turn results in increasing the number of neighbours. This, in turn, results in increasing the bias and reducing the variance.

Another example can be the algorithm of a support vector machine. This algorithm also has a high variance and obviously, a low bias and we can reverse the trade-off by increasing the value of parameter C. Thus, increasing the C parameter increases the bias and decreases the variance.

So, the trade-off is simple. If we increase the bias, the variance will decrease and vice versa.

17. Define the confusion matrix?
It is a matrix that has 2 rows and 2 columns. It has 4 outputs that a binary classifier provides to it. It is used to derive various measures like specificity, error rate, accuracy, precision, sensitivity, and recall.


The test data set should contain the correct and predicted labels. The labels depend upon the performance. For instance, the predicted labels are the same if the binary classifier performs perfectly. Also, they match the part of observed labels in real-world scenarios. The four outcomes shown above in the confusion matrix mean the following:

True Positive: This means that the positive prediction is correct.
False Positive: This means that the positive prediction is incorrect.
True Negative: This means that the negative prediction is correct.
False Negative: This means that the negative prediction is incorrect.
The formulas for calculating basic measures that comes from the confusion matrix are:

Error rate: (FP + FN)/(P + N)
Accuracy: (TP + TN)/(P + N)
Sensitivity = TP/P
Specificity = TN/N
Precision = TP/(TP + FP)
F-Score  = (1 + b)(PREC.REC)/(b2 PREC + REC) Here, b is mostly 0.5 or 1 or 2.
In these formulas:

FP = false positive
FN = false negative
TP = true positive
RN = true negative

Also,

Sensitivity is the measure of the True Positive Rate. It is also called recall.
Specificity is the measure of the true negative rate.
Precision is the measure of a positive predicted value.
F-score is the harmonic mean of precision and recall.

18. What is logistic regression? State an example where you have recently used logistic regression.
Logistic Regression is also known as the logit model. It is a technique to predict the binary outcome from a linear combination of variables (called the predictor variables). 

For example, let us say that we want to predict the outcome of elections for a particular political leader. So, we want to find out whether this leader is going to win the election or not. So, the result is binary i.e. win (1) or loss (0). However, the input is a combination of linear variables like the money spent on advertising, the past work done by the leader and the party, etc. 

19. What is Linear Regression? What are some of the major drawbacks of the linear model?
Linear regression is a technique in which the score of a variable Y is predicted using the score of a predictor variable X. Y is called the criterion variable. Some of the drawbacks of Linear Regression are as follows:

The assumption of linearity of errors is a major drawback.
It cannot be used for binary outcomes. We have Logistic Regression for that.
Overfitting problems are there that can’t be solved.
20. What is a random forest? Explain it’s working.
Classification is very important in machine learning. It is very important to know to which class does an observation belongs. Hence, we have various classification algorithms in machine learning like logistic regression, support vector machine, decision trees, Naive Bayes classifier, etc. One such classification technique that is near the top of the classification hierarchy is the random forest classifier. 

So, firstly we need to understand a decision tree before we can understand the random forest classifier and its works. So, let us say that we have a string as given below:


So, we have the string with 5 ones and 4 zeroes and we want to classify the characters of this string using their features. These features are colour (red or green in this case) and whether the observation (i.e. character) is underlined or not. Now, let us say that we are only interested in red and underlined observations. So, the decision tree would look something like this:


So, we started with the colour first as we are only interested in the red observations and we separated the red and the green-coloured characters. After that, the “No” branch i.e. the branch that had all the green coloured characters was not expanded further as we want only red-underlined characters. So, we expanded the “Yes” branch and we again got a “Yes” and a “No” branch based on the fact whether the characters were underlined or not. 

So, this is how we draw a typical decision tree. However, the data in real life is not this clean but this was just to give an idea about the working of the decision trees. Let us now move to the random forest.

Random Forest

It consists of a large number of decision trees that operate as an ensemble. Basically, each tree in the forest gives a class prediction and the one with the maximum number of votes becomes the prediction of our model. For instance, in the example shown below, 4 decision trees predict 1, and 2 predict 0. Hence, prediction 1 will be considered.


The underlying principle of a random forest is that several weak learners combine to form a keen learner. The steps to build a random forest are as follows:

Build several decision trees on the samples of data and record their predictions.
Each time a split is considered for a tree, choose a random sample of mm predictors as the split candidates out of all the pp predictors. This happens to every tree in the random forest.
Apply the rule of thumb i.e. at each split m = p√m = p.
Apply the predictions to the majority rule.
21. In a time interval of 15-minutes, the probability that you may see a shooting star or a bunch of them is 0.2. What is the percentage chance of you seeing at least one star shooting from the sky if you are under it for about an hour?
Let us say that Prob is the probability that we may see a minimum of one shooting star in 15 minutes.

So, Prob = 0.2

Now, the probability that we may not see any shooting star in the time duration of 15 minutes is = 1 - Prob

1-0.2 = 0.8

The probability that we may not see any shooting star for an hour is: 

= (1-Prob)(1-Prob)(1-Prob)*(1-Prob)
= 0.8 * 0.8 * 0.8 * 0.8 = (0.8)⁴  
≈ 0.40

So, the probability that we will see one shooting star in the time interval of an hour is = 1-0.4 = 0.6

So, there are approximately 60% chances that we may see a shooting star in the time span of an hour.

22. What is deep learning? What is the difference between deep learning and machine learning?
Deep learning is a paradigm of machine learning. In deep learning,  multiple layers of processing are involved in order to extract high features from the data. The neural networks are designed in such a way that they try to simulate the human brain. 

Deep learning has shown incredible performance in recent years because of the fact that it shows great analogy with the human brain.

The difference between machine learning and deep learning is that deep learning is a paradigm or a part of machine learning that is inspired by the structure and functions of the human brain called the artificial neural networks. Learn More.

Data Science Interview Questions for Experienced
1. How are the time series problems different from other regression problems?
Time series data can be thought of as an extension to linear regression which uses terms like autocorrelation, movement of averages for summarizing historical data of y-axis variables for predicting a better future.
Forecasting and prediction is the main goal of time series problems where accurate predictions can be made but sometimes the underlying reasons might not be known.
Having Time in the problem does not necessarily mean it becomes a time series problem. There should be a relationship between target and time for a problem to become a time series problem.
The observations close to one another in time are expected to be similar to the ones far away which provide accountability for seasonality. For instance, today’s weather would be similar to tomorrow’s weather but not similar to weather from 4 months from today. Hence, weather prediction based on past data becomes a time series problem.
2. What are RMSE and MSE in a linear regression model?
RMSE: RMSE stands for Root Mean Square Error. In a linear regression model, RMSE is used to test the performance of the machine learning model. It is used to evaluate the data spread around the line of best fit. So, in simple words, it is used to measure the deviation of the residuals.

RMSE is calculated using the formula:


Yi is the actual value of the output variable.
Y(Cap) is the predicted value and,
N is the number of data points.
MSE: Mean Squared Error is used to find how close is the line to the actual data. So, we make the difference in the distance of the data points from the line and the difference is squared. This is done for all the data points and the submission of the squared difference divided by the total number of data points gives us the Mean Squared Error (MSE).

So, if we are taking the squared difference of N data points and dividing the sum by N, what does it mean? Yes, it represents the average of the squared difference of a data point from the line i.e. the average of the squared difference between the actual and the predicted values. The formula for finding MSE is given below:


Yi is the actual value of the output variable (the ith data point)
Y(cap) is the predicted value and,
N is the total number of data points.
So, RMSE is the square root of MSE.

3. What are Support Vectors in SVM (Support Vector Machine)?

In the above diagram, we can see that the thin lines mark the distance from the classifier to the closest data points (darkened data points). These are called support vectors. So, we can define the support vectors as the data points or vectors that are nearest (closest) to the hyperplane. They affect the position of the hyperplane. Since they support the hyperplane, they are known as support vectors.

4. So, you have done some projects in machine learning and data science and we see you are a bit experienced in the field. Let’s say your laptop’s RAM is only 4GB and you want to train your model on 10GB data set.
What will you do? Have you experienced such an issue before?
In such types of questions, we first need to ask what ML model we have to train. After that, it depends on whether we have to train a model based on Neural Networks or SVM.

The steps for Neural Networks are given below:

The Numpy array can be used to load the entire data. It will never store the entire data, rather just create a mapping of the data.
Now, in order to get some desired data, pass the index into the NumPy Array.
This data can be used to pass as an input to the neural network maintaining a small batch size.
The steps for SVM are given below:

For SVM, small data sets can be obtained. This can be done by dividing the big data set.
The subset of the data set can be obtained as an input if using the partial fit function.
Repeat the step of using the partial fit method for other subsets as well.
Now, you may describe the situation if you have faced such an issue in your projects or working in machine learning/ data science.

5. Explain Neural Network Fundamentals.
In the human brain, different neurons are present. These neurons combine and perform various tasks. The Neural Network in deep learning tries to imitate human brain neurons. The neural network learns the patterns from the data and uses the knowledge that it gains from various patterns to predict the output for new data, without any human assistance.

A perceptron is the simplest neural network that contains a single neuron that performs 2 functions. The first function is to perform the weighted sum of all the inputs and the second is an activation function.


There are some other neural networks that are more complicated. Such networks consist of the following three layers:

Input Layer: The neural network has the input layer to receive the input.
Hidden Layer: There can be multiple hidden layers between the input layer and the output layer. The initially hidden layers are used for detecting the low-level patterns whereas the further layers are responsible for combining output from previous layers to find more patterns.
Output Layer: This layer outputs the prediction.
An example neural network image is shown below:


6. What is Generative Adversarial Network?
This approach can be understood with the famous example of the wine seller. Let us say that there is a wine seller who has his own shop. This wine seller purchases wine from the dealers who sell him the wine at a low cost so that he can sell the wine at a high cost to the customers. Now, let us say that the dealers whom he is purchasing the wine from, are selling him fake wine. They do this as the fake wine costs way less than the original wine and the fake and the real wine are indistinguishable to a normal consumer (customer in this case). The shop owner has some friends who are wine experts and he sends his wine to them every time before keeping the stock for sale in his shop. So, his friends, the wine experts, give him feedback that the wine is probably fake. Since the wine seller has been purchasing the wine for a long time from the same dealers, he wants to make sure that their feedback is right before he complains to the dealers about it. Now, let us say that the dealers also have got a tip from somewhere that the wine seller is suspicious of them.

So, in this situation, the dealers will try their best to sell the fake wine whereas the wine seller will try his best to identify the fake wine. Let us see this with the help of a diagram shown below:


From the image above, it is clear that a noise vector is entering the generator (dealer) and he generates the fake wine and the discriminator has to distinguish between the fake wine and real wine. This is a Generative Adversarial Network (GAN).

In a GAN, there are 2 main components viz. Generator and Discrminator. So, the generator is a CNN that keeps producing images and the discriminator tries to identify the real images from the fake ones. 

7. What is a computational graph?
A computational graph is also known as a “Dataflow Graph”. Everything in the famous deep learning library TensorFlow is based on the computational graph. The computational graph in Tensorflow has a network of nodes where each node operates. The nodes of this graph represent operations and the edges represent tensors.

8. What are auto-encoders?
Auto-encoders are learning networks. They transform inputs into outputs with minimum possible errors. So, basically, this means that the output that we want should be almost equal to or as close as to input as follows. 

Multiple layers are added between the input and the output layer and the layers that are in between the input and the output layer are smaller than the input layer. It received unlabelled input. This input is encoded to reconstruct the input later.

9. What are Exploding Gradients and Vanishing Gradients?
Exploding Gradients: Let us say that you are training an RNN. Say, you saw exponentially growing error gradients that accumulate, and as a result of this, very large updates are made to the neural network model weights. These exponentially growing error gradients that update the neural network weights to a great extent are called Exploding Gradients.
Vanishing Gradients: Let us say again, that you are training an RNN. Say, the slope became too small. This problem of the slope becoming too small is called Vanishing Gradient. It causes a major increase in the training time and causes poor performance and extremely low accuracy.
10. What is the p-value and what does it indicate in the Null Hypothesis?
P-value is a number that ranges from 0 to 1. In a hypothesis test in statistics, the p-value helps in telling us how strong the results are. The claim that is kept for experiment or trial is called Null Hypothesis.

A low p-value i.e. p-value less than or equal to 0.05 indicates the strength of the results against the Null Hypothesis which in turn means that the Null Hypothesis can be rejected. 
A high p-value i.e. p-value greater than 0.05 indicates the strength of the results in favour of the Null Hypothesis i.e. for the Null Hypothesis which in turn means that the Null Hypothesis can be accepted.
11. Since you have experience in the deep learning field, can you tell us why TensorFlow is the most preferred library in deep learning?
Tensorflow is a very famous library in deep learning. The reason is pretty simple actually. It provides C++ as well as Python APIs which makes it very easier to work on. Also, TensorFlow has a fast compilation speed as compared to Keras and Torch (other famous deep learning libraries). Apart from that, Tenserflow supports both GPU and CPU computing devices. Hence, it is a major success and a very popular library for deep learning.

12. Suppose there is a dataset having variables with missing values of more than 30%, how will you deal with such a dataset?
Depending on the size of the dataset, we follow the below ways:

In case the datasets are small, the missing values are substituted with the mean or average of the remaining data. In pandas, this can be done by using mean = df.mean() where df represents the pandas dataframe representing the dataset and mean() calculates the mean of the data. To substitute the missing values with the calculated mean, we can use df.fillna(mean).
For larger datasets, the rows with missing values can be removed and the remaining data can be used for data prediction.
13. What is Cross-Validation?
Cross-Validation is a Statistical technique used for improving a model’s performance. Here, the model will be trained and tested with rotation using different samples of the training dataset to ensure that the model performs well for unknown data. The training data will be split into various groups and the model is run and validated against these groups in rotation.


The most commonly used techniques are:

K- Fold method
Leave p-out method
Leave-one-out method
Holdout method
14. What are the differences between correlation and covariance?
Although these two terms are used for establishing a relationship and dependency between any two random variables, the following are the differences between them:

Correlation: This technique is used to measure and estimate the quantitative relationship between two variables and is measured in terms of how strong are the variables related.
Covariance: It represents the extent to which the variables change together in a cycle. This explains the systematic relationship between pair of variables where changes in one affect changes in another variable.
Mathematically, consider 2 random variables, X and Y where the means are represented as 
 and 
 respectively and standard deviations are represented by 
 and 
 respectively and E represents the expected value operator, then:

covarianceXY = E[(X-
),(Y-
)]
correlationXY = E[(X-
),(Y-
)]/(
)
so that
correlation(X,Y) = covariance(X,Y)/(covariance(X) covariance(Y))
Based on the above formula, we can deduce that the correlation is dimensionless whereas covariance is represented in units that are obtained from the multiplication of units of two variables.

The following image graphically shows the difference between correlation and covariance:


15. How do you approach solving any data analytics based project?
Generally, we follow the below steps:

The first step is to thoroughly understand the business requirement/problem
Next, explore the given data and analyze it carefully. If you find any data missing, get the requirements clarified from the business.
Data cleanup and preparation step is to be performed next which is then used for modelling. Here, the missing values are found and the variables are transformed.
Run your model against the data, build meaningful visualization and analyze the results to get meaningful insights.
Release the model implementation, and track the results and performance over a specified period to analyze the usefulness.
Perform cross-validation of the model.
Check out the list of data analytics projects.


16. How regularly must we update an algorithm in the field of machine learning?
We do not want to update and make changes to an algorithm on a regular basis as an algorithm is a well-defined step procedure to solve any problem and if the steps keep on updating, it cannot be said well defined anymore. Also, this brings in a lot of problems to the systems already implementing the algorithm as it becomes difficult to bring in continuous and regular changes. So, we should update an algorithm only in any of the following cases:

If you want the model to evolve as data streams through infrastructure, it is fair to make changes to an algorithm and update it accordingly.
If the underlying data source is changing, it almost becomes necessary to update the algorithm accordingly.
If there is a case of non-stationarity, we may update the algorithm.
One of the most important reasons for updating any algorithm is its underperformance and lack of efficiency. So, if an algorithm lacks efficiency or underperforms it should be either replaced by some better algorithm or it must be updated.
17. Why do we need selection bias?
Selection Bias happens in cases where there is no randomization specifically achieved while picking a part of the dataset for analysis. This bias tells that the sample analyzed does not represent the whole population meant to be analyzed.

For example, in the below image, we can see that the sample that we selected does not entirely represent the whole population that we have. This helps us to question whether we have selected the right data for analysis or not.

18. Why is data cleaning crucial? How do you clean the data?
While running an algorithm on any data, to gather proper insights, it is very much necessary to have correct and clean data that contains only relevant information. Dirty data most often results in poor or incorrect insights and predictions which can have damaging effects.

For example, while launching any big campaign to market a product, if our data analysis tells us to target a product that in reality has no demand and if the campaign is launched, it is bound to fail. This results in a loss of the company’s revenue. This is where the importance of having proper and clean data comes into the picture.

Data Cleaning of the data coming from different sources helps in data transformation and results in the data where the data scientists can work on.
Properly cleaned data increases the accuracy of the model and provides very good predictions.
If the dataset is very large, then it becomes cumbersome to run data on it. The data cleanup step takes a lot of time (around 80% of the time) if the data is huge. It cannot be incorporated with running the model. Hence, cleaning data before running the model, results in increased speed and efficiency of the model.
Data cleaning helps to identify and fix any structural issues in the data. It also helps in removing any duplicates and helps to maintain the consistency of the data.
The following diagram represents the advantages of data cleaning:


19. What are the available feature selection methods for selecting the right variables for building efficient predictive models?
While using a dataset in data science or machine learning algorithms, it so happens that not all the variables are necessary and useful to build a model. Smarter feature selection methods are required to avoid redundant models to increase the efficiency of our model. Following are the three main methods in feature selection:

Filter Methods:
These methods pick up only the intrinsic properties of features that are measured via univariate statistics and not cross-validated performance. They are straightforward and are generally faster and require less computational resources when compared to wrapper methods.
There are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc.

Wrapper Methods:
These methods need some sort of method to search greedily on all possible feature subsets, access their quality by learning and evaluating a classifier with the feature.
The selection technique is built upon the machine learning algorithm on which the given dataset needs to fit.
There are three types of wrapper methods, they are:
Forward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.
Backward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.
Recursive Feature Elimination: The features are recursively checked and evaluated how well they perform.
These methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods.

Embedded Methods:
Embedded methods constitute the advantages of both filter and wrapper methods by including feature interactions while maintaining reasonable computational costs.
These methods are iterative as they take each model iteration and carefully extract features contributing to most of the training in that iteration.
Examples of embedded methods: LASSO Regularization (L1), Random Forest Importance.

20. During analysis, how do you treat the missing values?
To identify the extent of missing values, we first have to identify the variables with the missing values. Let us say a pattern is identified. The analyst should now concentrate on them as it could lead to interesting and meaningful insights. However, if there are no patterns identified, we can substitute the missing values with the median or mean values or we can simply ignore the missing values. 

If the variable is categorical, the common strategies for handling missing values include:

Assigning a New Category: You can assign a new category, such as "Unknown" or "Other," to represent the missing values.
Mode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.
Using a Separate Category: If the missing values carry significant information, you can create a separate category to indicate missing values.
It's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling.

If 80% of the values are missing for a particular variable, then we would drop the variable instead of treating the missing values.

21. Will treating categorical variables as continuous variables result in a better predictive model?
Yes! A categorical variable is a variable that can be assigned to two or more categories with no definite category ordering. Ordinal variables are similar to categorical variables with proper and clear ordering defines. So, if the variable is ordinal, then treating the categorical value as a continuous variable will result in better predictive models.

22. How will you treat missing values during data analysis?
The impact of missing values can be known after identifying what type of variables have missing values.

If the data analyst finds any pattern in these missing values, then there are chances of finding meaningful insights.
In case of patterns are not found, then these missing values can either be ignored or can be replaced with default values such as mean, minimum, maximum, or median values.
If the missing values belong to categorical variables, the common strategies for handling missing values include:
Assigning a new category: You can assign a new category, such as "Unknown" or "Other," to represent the missing values.
Mode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.
Using a separate category: If the missing values carry significant information, you can create a separate category to indicate the missing values.
It's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling.
If 80% of values are missing, then it depends on the analyst to either replace them with default values or drop the variables.
23. What does the ROC Curve represent and how to create it?
ROC (Receiver Operating Characteristic) curve is a graphical representation of the contrast between false-positive rates and true positive rates at different thresholds. The curve is used as a proxy for a trade-off between sensitivity and specificity.

The ROC curve is created by plotting values of true positive rates (TPR or sensitivity) against false-positive rates (FPR or (1-specificity)) TPR represents the proportion of observations correctly predicted as positive out of overall positive observations. The FPR represents the proportion of observations incorrectly predicted out of overall negative observations. Consider the example of medical testing, the TPR represents the rate at which people are correctly tested positive for a particular disease.


24. What are the differences between univariate, bivariate and multivariate analysis?
Statistical analyses are classified based on the number of variables processed at a given time.

Univariate analysis	Bivariate analysis	Multivariate analysis
This analysis deals with solving only one variable at a time.	This analysis deals with the statistical study of two variables at a given time.	This analysis deals with statistical analysis of more than two variables and studies the responses.
Example: Sales pie charts based on territory.	Example: Scatterplot of Sales and spend volume analysis study.	Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc.
25. What is the difference between the Test set and validation set?
The test set is used to test or evaluate the performance of the trained model. It evaluates the predictive power of the model.
The validation set is part of the training set that is used to select parameters for avoiding model overfitting.

26. What do you understand by a kernel trick?
Kernel functions are generalized dot product functions used for the computing dot product of vectors xx and yy in high dimensional feature space. Kernal trick method is used for solving a non-linear problem by using a linear classifier by transforming linearly inseparable data into separable ones in higher dimensions.


27. Differentiate between box plot and histogram.
Box plots and histograms are both visualizations used for showing data distributions for efficient communication of information.
Histograms are the bar chart representation of information that represents the frequency of numerical variable values that are useful in estimating probability distribution, variations and outliers.
Boxplots are used for communicating different aspects of data distribution where the shape of the distribution is not seen but still the insights can be gathered. These are useful for comparing multiple charts at the same time as they take less space when compared to histograms.


28. How will you balance/correct imbalanced data?
There are different techniques to correct/balance imbalanced data. It can be done by increasing the sample numbers for minority classes. The number of samples can be decreased for those classes with extremely high data points. Following are some approaches followed to balance data:

Use the right evaluation metrics: In cases of imbalanced data, it is very important to use the right evaluation metrics that provide valuable information. 
Specificity/Precision: Indicates the number of selected instances that are relevant.
Sensitivity: Indicates the number of relevant instances that are selected.
F1 score: It represents the harmonic mean of precision and sensitivity.
MCC (Matthews correlation coefficient): It represents the correlation coefficient between observed and predicted binary classifications.
AUC (Area Under the Curve): This represents a relation between the true positive rates and false-positive rates.
For example, consider the below graph that illustrates training data:

Here, if we measure the accuracy of the model in terms of getting "0"s, then the accuracy of the model would be very high -> 99.9%, but the model does not guarantee any valuable information. In such cases, we can apply different evaluation metrics as stated above.


Training Set Resampling: It is also possible to balance data by working on getting different datasets and this can be achieved by resampling. There are two approaches followed under-sampling that is used based on the use case and the requirements:
Under-sampling This balances the data by reducing the size of the abundant class and is used when the data quantity is sufficient. By performing this, a new dataset that is balanced can be retrieved and this can be used for further modeling.
Over-sampling This is used when data quantity is not sufficient. This method balances the dataset by trying to increase the samples size. Instead of getting rid of extra samples, new samples are generated and introduced by employing the methods of repetition, bootstrapping, etc.
Perform K-fold cross-validation correctly: Cross-Validation needs to be applied properly while using over-sampling. The cross-validation should be done before over-sampling because if it is done later, then it would be like overfitting the model to get a specific result. To avoid this, resampling of data is done repeatedly with different ratios. 
29. What is better - random forest or multiple decision trees?
Random forest is better than multiple decision trees as random forests are much more robust, accurate, and lesser prone to overfitting as it is an ensemble method that ensures multiple weak decision trees learn strongly.

30. Consider a case where you know the probability of finding at least one shooting star in a 15-minute interval is 30%. Evaluate the probability of finding at least one shooting star in a one-hour duration?
We know that,
Probability of finding atleast 1 shooting star in 15 min = P(sighting in 15min) = 30% = 0.3
Hence, Probability of not sighting any 
shooting star in 15 min = 1-P(sighting in 15min)
                       = 1-0.3
                       = 0.7
                       
Probability of not finding shooting star in 1 hour
                       = 0.7^4
                       = 0.1372
Probability of finding atleast 1 
shooting star in 1 hour = 1-0.1372
                       = 0.8628
So the probability is 0.8628 = 86.28%

31. Toss the selected coin 10 times from a jar of 1000 coins. Out of 1000 coins, 999 coins are fair and 1 coin is double-headed, assume that you see 10 heads. Estimate the probability of getting a head in the next coin toss.
We know that there are two types of coins - fair and double-headed. Hence, there are two possible ways of choosing a coin. The first is to choose a fair coin and the second is to choose a coin having 2 heads.

P(selecting fair coin) = 999/1000 = 0.999
P(selecting double headed coin) = 1/1000 = 0.001

Using Bayes rule,

P(selecting 10 heads in row) = P(selecting fair coin)* Getting 10 heads + P(selecting double headed coin)
P(selecting 10 heads in row) = P(A)+P(B)

P (A)  =  0.999 * (1/2)^10  
      =  0.999 * (1/1024)  
      =  0.000976
P (B)  =  0.001 * 1  =  0.001
P( A / (A + B) )  = 0.000976 /  (0.000976 + 0.001)                   = 0.4939
P( B / (A + B))   = 0.001 / 0.001976  
                 = 0.5061
P(selecting head in next toss) = P(A/A+B) * 0.5 + P(B/A+B) * 1 
                              = 0.4939 * 0.5 + 0.5061  
                              = 0.7531
So, the answer is 0.7531 or 75.3%.

32. What are some examples when false positive has proven important than false negative?
Before citing instances, let us understand what are false positives and false negatives.

False Positives are those cases that were wrongly identified as an event even if they were not. They are called Type I errors.
False Negatives are those cases that were wrongly identified as non-events despite being an event. They are called Type II errors.
Some examples where false positives were important than false negatives are:

In the medical field: Consider that a lab report has predicted cancer to a patient even if he did not have cancer. This is an example of a false positive error. It is dangerous to start chemotherapy for that patient as he doesn’t have cancer as starting chemotherapy would lead to damage of healthy cells and might even actually lead to cancer.
In the e-commerce field: Suppose a company decides to start a campaign where they give $100 gift vouchers for purchasing $10000 worth of items without any minimum purchase conditions. They assume it would result in at least 20% profit for items sold above $10000. What if the vouchers are given to the customers who haven’t purchased anything but have been mistakenly marked as those who purchased $10000 worth of products. This is the case of false-positive error.
33. Give one example where both false positives and false negatives are important equally?
In Banking fields: Lending loans are the main sources of income to the banks. But if the repayment rate isn’t good, then there is a risk of huge losses instead of any profits. So giving out loans to customers is a gamble as banks can’t risk losing good customers but at the same time, they can’t afford to acquire bad customers. This case is a classic example of equal importance in false positive and false negative scenarios.

34. Is it good to do dimensionality reduction before fitting a Support Vector Model?
If the features number is greater than observations then doing dimensionality reduction improves the SVM (Support Vector Model).

35. What are various assumptions used in linear regression? What would happen if they are violated?
Linear regression is done under the following assumptions:

The sample data used for modeling represents the entire population.
There exists a linear relationship between the X-axis variable and the mean of the Y variable.
The residual variance is the same for any X values. This is called homoscedasticity
The observations are independent of one another.
Y is distributed normally for any value of X.
Extreme violations of the above assumptions lead to redundant results. Smaller violations of these result in greater variance or bias of the estimates.

36. How is feature selection performed using the regularization method?
The method of regularization entails the addition of penalties to different parameters in the machine learning model for reducing the freedom of the model to avoid the issue of overfitting.
There are various regularization methods available such as linear model regularization, Lasso/L1 regularization, etc. The linear model regularization applies penalty over coefficients that multiplies the predictors. The Lasso/L1 regularization has the feature of shrinking some coefficients to zero, thereby making it eligible to be removed from the model.

37. How do you identify if a coin is biased?
To identify this, we perform a hypothesis test as below:
According to the null hypothesis, the coin is unbiased if the probability of head flipping is 50%. According to the alternative hypothesis, the coin is biased and the probability is not equal to 500. Perform the below steps:

Flip coin 500 times
Calculate p-value.
Compare the p-value against the alpha -> result of two-tailed test (0.05/2 = 0.025). Following two cases might occur:
p-value > alpha: Then null hypothesis holds good and the coin is unbiased.
p-value < alpha: Then the null hypothesis is rejected and the coin is biased.
38. What is the importance of dimensionality reduction?
The process of dimensionality reduction constitutes reducing the number of features in a dataset to avoid overfitting and reduce the variance. There are mostly 4 advantages of this process:

This reduces the storage space and time for model execution.
Removes the issue of multi-collinearity thereby improving the parameter interpretation of the ML model.
Makes it easier for visualizing data when the dimensions are reduced.
Avoids the curse of increased dimensionality.
39. How is the grid search parameter different from the random search tuning strategy?
Tuning strategies are used to find the right set of hyperparameters. Hyperparameters are those properties that are fixed and model-specific before the model is tested or trained on the dataset. Both the grid search and random search tuning strategies are optimization techniques to find efficient hyperparameters.

Grid Search:
Here, every combination of a preset list of hyperparameters is tried out and evaluated.
The search pattern is similar to searching in a grid where the values are in a matrix and a search is performed. Each parameter set is tried out and their accuracy is tracked. after every combination is tried out, the model with the highest accuracy is chosen as the best one.
The main drawback here is that, if the number of hyperparameters is increased, the technique suffers. The number of evaluations can increase exponentially with each increase in the hyperparameter. This is called the problem of dimensionality in a grid search.

Random Search:
In this technique, random combinations of hyperparameters set are tried and evaluated for finding the best solution. For optimizing the search, the function is tested at random configurations in parameter space as shown in the image below.
In this method, there are increased chances of finding optimal parameters because the pattern followed is random. There are chances that the model is trained on optimized parameters without the need for aliasing.
This search works the best when there is a lower number of dimensions as it takes less time to find the right set.
  

Preparing for your Data Science interview
Ace your data science interview by keeping these tips in mind:

Strengthen your foundation in statistics, probability, and machine learning. Be clear on concepts like p-values, bias-variance tradeoff, and classification metrics.
Practice coding in Python, R (optional), and SQL, especially tasks like data wrangling, joins, aggregations, and real-world feature engineering.
Build hands-on projects that demonstrate your ability to apply models to real data. Tools like Kaggle, HackerRank, and GitHub are great places to showcase this.
Learn to communicate your thinking. Practice explaining your process and trade-offs in business terms, not just technical ones.
Study this guide to get familiar with the kinds of questions you’ll likely be asked. Use your own examples to make answers more relatable and memorable.
Understand the company and its data use cases. Knowing their product, data stack, or industry challenges will help you tailor your answers and ask insightful questions.
Test yourself with Flashcards
You can either use these flashcards or jump to the questions list section below to see them in a list format.


0 / 63

Knew
0 Items
Learnt
0 Items
Skipped
0 Items
Reset
Core Concepts
What is the difference between correlation and causation?

Click to Reveal the Answer
Correlation is the statistical measure that shows a relationship between two variables. When one changes, the other changes as well, positively or negatively. However, this doesn't mean that one variable causes the other to change. Causation means that one variable directly causes a change in the other. It implies a cause-and-effect relationship, not just an association. Proving causation requires deeper analysis and additional evidence.

Example: There's a correlation between cart abandonment and uninstall rates in a shopping app. Users who abandon their carts often end up uninstalling the app shortly after. But that doesn't mean abandoning a cart causes someone to uninstall the app. The real cause might be a frustrating purchase process with too many steps. That complexity leads to both behaviors: abandoning the cart and uninstalling the app. So, while there's a correlation, you can't say it's causation without looking deeper.

Correlation vs. causation

Hide the Answer
Already Know that
Didn't Know that
Skip Question
Questions List
If you prefer to see the questions in a list format, you can find them below.

Core Concepts
What is the difference between correlation and causation?
Correlation is the statistical measure that shows a relationship between two variables. When one changes, the other changes as well, positively or negatively. However, this doesn't mean that one variable causes the other to change. Causation means that one variable directly causes a change in the other. It implies a cause-and-effect relationship, not just an association. Proving causation requires deeper analysis and additional evidence.

Example: There's a correlation between cart abandonment and uninstall rates in a shopping app. Users who abandon their carts often end up uninstalling the app shortly after. But that doesn't mean abandoning a cart causes someone to uninstall the app. The real cause might be a frustrating purchase process with too many steps. That complexity leads to both behaviors: abandoning the cart and uninstalling the app. So, while there's a correlation, you can't say it's causation without looking deeper.

Correlation vs. causation

What is the role of statistics in data science?
The role of statistics in data science is to help data scientists understand and summarize data, uncover patterns, validate models, handle uncertainty (like missing or noisy data), and make evidence-based decisions.

For example:

Mean and median summarize central tendencies.
Standard deviation and variance measure variability.
Hypothesis testing validates assumptions.
Regression analysis predicts relationships between variables.
Bayesian inference updates beliefs as new data comes in.
Use case: A marketing team runs an A/B test to compare two email campaigns. Statistical methods help determine whether the difference in click-through rates is real or just a coincidence.

How do you handle missing data?
After observing that my data set has missing values, I'll figure out how it occurs. Are they represented as NaN, None, empty strings, weird characters like -999, a combination of two or more, or something else?

How to handle missing data

Once I make sense of what my missing data looks like, I dig into why these values are missing, and they usually fall into three categories:

Missing Completely At Random (MCAR): No pattern, just random gaps. These are usually safe to drop, especially if there aren't many.

Example: In a survey dataset, 10% of income entries are missing due to a technical glitch that affected a random subset of responses. There's no pattern based on age, education, employment status, or anything else.

Missing At Random (MAR): This is when the missing data is related to other observed variables, but not to the income value itself.

Example: In the same dataset, 10% of income values are missing, mostly among respondents who are students. Here, missing data is related to the occupation variable, not the actual income value. Impute based on related features like occupation, education level, or age. Impute based on related features like occupation, education level, or age. Safe to drop or impute with mean/median since the missing data doesn't introduce bias.

MNAR (Missing Not At Random): The reason it's missing is tied to the value itself.

Example: If high spenders choose not to share income, that's tougher to handle and sometimes better tracked with a missingness flag. The probability of missingness increases with the income amount. Imputation is risky here. I'll consider flagging missingness with a binary indicator (income_missing) or using models that can account for MNAR, like EM algorithms or data augmentation techniques.

Once I know the type of missingness, I choose one of the following: a. Deletion (if safe):

Listwise: Drop rows with missing values (only when missingness is random and small).
Pairwise: Use available values for calculations, such as correlations.
Drop columns: Remove low-value features with lots of missing data.
b. Simple imputation:

Mean/Median/Mode: Use for numeric or categorical columns, depending on distribution.
Arbitrary values: Fill with 0 or "Unknown" if it makes sense contextually.
Forward/Backward fill: Best for time series to keep temporal consistency.
c. Advanced imputation:

KNN imputer: Fills gaps by finding similar rows using distance metrics.
Iterative imputer: Builds a model with other columns to estimate missing values.
Interpolation: Good for numeric sequences, especially when data follows a trend.
d. Use missingness as a feature:

If the missing value could carry a signal, I add a binary indicator column (e.g., was_missing = 1).
e. Oversampling or undersampling:

If missing data causes class imbalance, I use resampling to maintain a fair target distribution.
Common pitfall: Filling in values without understanding the pattern of missingness. For example, using mean imputation on MNAR data can introduce bias and weaken your model's predictive power.

What is the difference between univariate, bivariate, and multivariate analysis?
Univariate analysis is all about looking at one variable on its own, with no comparisons, just understanding its distribution, central tendency, or spread. For example, I might look at the average test score in a class or the frequency of different grade ranges using histograms or summary statistics.

Univariate, bivariate, and multivariate analysis

Bivariate analysis looks at the relationship between two variables, such as how students' study time affects their test scores. To analyze this, I'd use tools like correlation, scatter plots, or line graphs to identify trends or patterns.

Multivariate analysis, on the other hand, deals with three or more variables at once. It focuses on understanding how multiple factors combine to influence an outcome. For example, I might explore how sleep hours, study time, and caffeine intake together impact test scores. In that case, I'd use regression or a tree-based model to analyze the combined effect.

What is the difference between the long format data and wide format data?
The difference between long format data and wide format data comes down to how your data is structured. A wide format has values that do not repeat in the columns, while a long format has values that do repeat in the columns.

In wide format, you spread data across columns. Each variable (Jan, Feb, March) gets its own column. You'll usually see this in reports or dashboards.

Wide format data

In long format, data is stacked in rows. One column stores the values, and another column tells you what those values represent. This format is cleaner for grouped summaries and time series analysis.

Long format data

Use case: Wide format is useful for reporting and making data visualizations. Long format is preferred for time series, grouped summaries, and plotting tools like Seaborn or ggplot.

Common pitfall: Trying to perform group-level analysis on wide-format data without reshaping it first.

What is multicollinearity, and how do you detect it?
Multicollinearity is when two or more independent variables in a regression model are highly correlated, meaning they tell similar stories. This makes it hard for the model to figure out which variable is actually influencing the target, leading to unreliable or unstable coefficient estimates.

For example, in a regression model looking at economic growth, common variables will be GDP, Unemployment Rate, and Consumer Spending. These variables are all related, and the model might not be as effective as it should be.

To detect multicollinearity use:

Correlation matrix: A correlation matrix detects multicollinearity by visualizing the strength of relationships between variables. A general rule is that any correlation value above 0.6 indicates strong multicollinearity.
Variance inflation factor (VIF): VIF detects multicollinearity by giving a numerical value that indicates how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF value greater than 5 indicates moderate multicollinearity, while values above 10 suggest severe multicollinearity.
Condition index: The condition index is a tool for detecting multicollinearity. Values above 10 indicate moderate multicollinearity, and values above 30 indicate severe multicollinearity. The condition index works by checking how much the independent variables are related to each other by examining the relationships between their eigenvalues.
Common pitfall: Including highly correlated predictors without checking VIF may inflate model error and reduce stability.

What is variance in data science?
Variance in data science measures the spread between numbers in a dataset. Simply put, it measures how far each number in the set is from the mean (average). It helps us understand how spread out or consistent the values are in a dataset.

Low variance example: Mean: (78 + 79 + 80 + 81 + 82) / 5 = 80

What do you understand by imbalanced data?
Imbalanced data is a situation where the classes, labels, or categories in a dataset are not equally represented. In imbalanced datasets, one category has significantly more or fewer samples than the others. Imbalanced data refers to a common issue in supervised machine learning and deep learning where there is a non-uniform distribution of samples among different classes. This can lead to biased outcomes in models, such as those used in healthcare services, impacting their reliability and effectiveness.

For example, if you build an email spam detection model and your dataset contains 5% spam emails and 95% non-spam emails, the data is imbalanced toward non-spam. This imbalance can negatively affect the model's performance, especially in production, because it may achieve high accuracy by simply predicting the majority class while failing to detect spam effectively.

How would you approach categorical and continuous variables differently during preprocessing?
A categorical variable is a column with fixed options based on qualities or labels like gender, age group, or education level. You can't do math on them directly, so during preprocessing, I usually apply one-hot encoding to turn those categories into binary columns the model can understand.

Categorical vs. continuous variables

A continuous variable, on the other hand, can take on any value within a range like height, temperature, or speed. These are numeric, so you can run calculations on them. But before feeding them into a model, I scale them using normalization or standardization to keep all features on a similar range. This prevents one feature from overpowering the rest just because it has larger numbers.

Describe how you would transform data from wide format to long format. Why might this be necessary?
To transform data from wide format to long format, I usually use the melt() function in pandas to unpivot the columns into rows. It's especially useful when you have repeated measures across columns, like monthly sales or survey responses, and you want to make the data tidy for analysis or plotting.

For example, if I have a DataFrame where each column represents a month, I'll keep the identifier (like product name) as is, and melt the rest so each row shows one product-month-sales combo. This makes it easier to group, filter, or feed into models.

How do you combine data from multiple sources with inconsistent formats?
When I combine data from different sources with inconsistent formats, the first thing I do is standardize everything: dates, column names, booleans, numbers, etc., so they all speak the same language. After doing that, I'll:

Align schemas: match columns across datasets. If one has extras, I drop or keep them depending on relevance.
Unify categories: I clean up inconsistencies like "Y" vs. "Yes" to avoid downstream issues.
Tag the source: I add a source column so I know where each row came from. This is super useful for tracking or debugging later.
Merge or stack: If the structure is the same, I concat(). If I'm matching on something like customer ID, I go with a merge or join.
Final clean-up: I look for duplicates, mismatched types, or broken values post-merge and fix them.
I avoid merging before checking data types or keys. That’s a fast track to lost or duplicated rows.

Statistics and Probability
What is the Central Limit Theorem, and why is it important?
The Central Limit Theorem (CLT) states that if you take enough random samples from any dataset, even if the data is skewed or messy, the average of those samples will start to form a bell-shaped or normal distribution. This only holds if the samples are random, independent, and large enough, usually 30 or more.

Why does this matter? Because once those averages form a normal shape, you can use all the tools from the normal distribution. You can calculate standard errors, build confidence intervals, run hypothesis tests, make estimates, and use z-scores, even if the original data wasn’t normal to begin with.

What is the difference between Type I and Type II errors?
The difference between Type I and Type II error is that Type I error rejects a true null hypothesis (a false positive), while Type II error fails to reject a false null hypothesis (a false negative).

Type I vs Type II error

Let's assume you're testing a new drug. The null hypothesis is that the drug doesn't work, and in reality, it indeed doesn't. A type I error occurs when your test says it does, but it doesn't; this is a false positive. A type II, on the other hand, is when the drug works in reality, but your tests say it doesn't: a false negative.

What is a p-value? How do you interpret it?
A p-value is a statistical measure that determines the significance of the result you got in a hypothesis test. A small p-value (<0.05) indicates strong evidence that the null hypothesis is wrong, meaning you should reject it.

P-value interpretation

If the probability of the p-value is greater than 0.05, there is not enough evidence to reject the null hypothesis. For example, if you conduct an experiment and the p-value is 0.03, you should reject the null hypothesis.

Explain confidence intervals in simple terms.
A confidence interval is a range of values that's likely to have the true value of a population parameter based on sample data. Let's say you surveyed a sample of high school teachers and found their average salary was $75,000. Since that's just a sample, you can't say for sure it reflects all teachers, but we can say:

"We're 95% confident that the real average salary for all teachers falls between $72,000 and $78,000."

The confidence interval is between $72,000 and $78,000. It gives us a buffer to account for uncertainty in our estimate.

Confidence intervals for teachers salaries

What is a probability distribution? Name a few commonly used ones.
A probability distribution tells you how likely different possible outcomes are for a random event or experiment. It maps out the chances of different results happening. It’s a way of saying, “Here’s everything that could happen, and how often I expect each one to happen.”

The commonly used probability distributions are normal (bell curve), binomial, Poisson, and uniform distributions.

Common pitfall: Different types of distributions need different types of analysis. Using the wrong type of distribution for analysis can lead to wrong results. Another common problem is assuming that data follow a normal distribution without testing for normality.

When would you use a t-test vs. a z-test?
The difference between a t-test and a z-test comes down to what you know about your data: the sample size and the population standard deviation.

t-test vs z-test

Use a t-test when:

The sample size is small (usually n ≤ 30).
The population standard deviation is unknown.
You still want to compare means (sample vs. population or between two samples).
Use a z-test when:

You know the population standard deviation.
Your sample size is large (typically n > 30).
The data is roughly normally distributed.
What is the null hypothesis, and how do you determine whether to reject it?
The null hypothesis (H₀) is the starting point and default assumption of every statistical analysis. The idea is that there's nothing happening in your data: no stories, no effect, no relationship between the variables you're testing until your data gives strong evidence to reject it.

When to reject null hypothesis

To know whether to reject the null hypothesis, here's what I do:

First, I set a significance level, usually 0.05. Then, I calculate the p-value. If the p-value is less than or equal to 0.05, I reject the null hypothesis because the result is statistically significant. If it's more than 0.05, I don't reject it because there isn't enough evidence.

Explain the Central Limit Theorem if we don't assume that the random variables are identically distributed but are still independent with finite second moments.
This is a curveball question because the interviewer isn't really asking about the classic CLT. They're testing your knowledge about the Lindeberg-Feller CLT.

In the classic CLT, all the variables are independent and from the same probability distribution (identically distributed). But here, the interviewer is saying: What if they're still independent but not identically distributed? Lindeberg-Feller helps in situations like this.

It states that as long as the variables are independent, have finite second moments (meaning their variances aren't huge), and no single variable dominates (the Lindeberg condition), the normalized sum of those variables will still approach a normal distribution. So, even with different distributions, if those conditions hold, the average still forms a bell curve.

Explain the long-tail distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?
A long-tail distribution is a type of distribution where you group most of the data around the middle, but there are still many rare or unusual values that stretch far out to the sides (tail) and have a big impact.

Long-tail distribution

Some examples are:

Long-tail keywords in SEO: A few high-volume keywords (like "shoes") get massive search volume, but there's a long tail of specific, niche searches (like "waterproof hiking shoes for wide feet") that collectively make up most of the search traffic. These long-tail keywords aren't often searched for, but they convert well.

Book sales: Bestsellers like Harry Potter dominate the market, but tons of niche books (even classics like Jane Austen) sell steadily in the background. The collective sales of these less popular books often exceed those of the bestsellers.

Luxury bags: A few brands are always trending. However, there's a long list of unique, lesser-known ones that still sell and matter to the market.

Why they're important in classification and regression problems:

Long-tail distributions can throw off model performance because most models are trained on the majority class, the "head," and ignore rare but important events in the tail. This is risky in cases like fraud detection or churn modeling, where rare events matter most. They also affect Mean Squared Error, which squares the difference between predicted and actual values, so a few extreme errors from tail cases can blow up the score, even if your model does well overall.

Long-tail distributions can create models that are biased toward the majority, skew errors, and cause you to miss rare events that matter. Handling them requires better sampling techniques and using loss functions that properly account for these rare but significant occurrences.

Machine Learning and Algorithms
What is the difference between supervised, unsupervised, and reinforcement learning?
The difference between supervised, unsupervised, and reinforcement learning lies in how the model learns. This table describes their contrasts and common use cases for each.

Type of Learning	Description	Common Use Cases
Supervised Learning	Models are trained on labeled data, where each example includes an input and a known output. The model learns to predict the output for new, unseen inputs.	Regression and classification tasks.
Unsupervised Learning	Models are trained on unlabeled data. The goal is to find hidden patterns or structure in the data without explicit output labels.	Clustering, dimensionality, and reduction tasks.
Reinforcement Learning	A model learns by trial and error, receiving rewards or penalties based on its actions. It aims to make a sequence of decisions to maximize reward over time.	Games, robotics, and AI decision-making systems.
Explain the bias-variance tradeoff.
The bias-variance tradeoff refers to the balance between a model's ability to learn patterns (low bias) and its ability to generalize to new data (low variance). Bias is the error made when the model makes strong assumptions about the data: high bias could lead to underfitting. Variance is the model's sensitivity to small fluctuations in the data, and high variance could lead to overfitting.

Bias-variance tradeoff

What is cross-validation, and why is it important?
Cross-validation is a technique for evaluating a model's performance on an individual dataset. It divides the dataset into subsets, usually the training, test, and validation sets. This process is repeated multiple times to ensure the model generalizes well to unseen data and to stop overfitting.

Cross-validating data

What is overfitting, and how can you prevent it?
Overfitting in machine learning happens when the model learns from the training data too well, including non-relevant details. This leads the model to perform very well on the training data but poorly on other data.

Prevention techniques:

Regularization (L1/L2): This method adds a penalty to large weights to keep the model from becoming too complex.
Cross-validation: This helps test the model on different slices of data to make sure it generalizes well.
Pruning (for tree models): Cuts back unnecessary branches that overcomplicate the model.
Early stopping: Stops training when performance stops improving on the validation set.
Dropout (for neural nets): This method randomly drops neurons during training so the network doesn't become too dependent on specific paths.
How do decision trees work?
A decision tree is a machine learning algorithm used for classification and regression tasks. It makes decisions by following a tree-like structure where internal nodes represent attribute tests, branches represent attribute values, and leaf nodes represent predictions.

Decision tree

Decision trees are versatile and are used for many machine learning tasks.

Example: Loan approval decision tree

Step 1 – Ask a question (Root Node): Is the applicant's credit score > 700?
If yes, go to Internal Node.
If no, go to Leaf Node (Do not approve the loan).
Step 2 – More questions (Internal Nodes): Is the applicant's income > $50,000?
If yes, approve the loan (Leaf Node).
If no, go to Leaf Node (Do not approve the loan).
Step 3 - Decision (Leaf Node)
Leaf Node 1: Do not approve the loan (if credit score ≤ 700).
Leaf Node 2: Approve the loan (if credit score > 700 and income > $50,000).
Leaf Node 3: Do not approve the loan (if credit score > 700 and income ≤ $50,000).
Common pitfall: Trees tend to overfit the data if you allow it to go too deep and include too many branches.

How does a random forest differ from a decision tree?
This table describes the difference between decision trees and random forests and when to use them based on features like accuracy, training time, etc.

Decision tree vs random forest

A random forest is a collection of multiple decision trees, while a decision tree is just a single model that predicts outcomes based on a series of decisions. For a random forest, each tree is trained on a subset of the data and a subset of features, and both of these are random. A decision tree is a simple, tree-like structure used to represent decisions and the possible outcomes from them. Random forest multiple trees use bootstrapped samples and random feature selection, then average predictions to improve accuracy and reduce overfitting.

How does logistic regression work?
Logistic regression is a supervised machine learning algorithm commonly used for binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary outcome limited to two possible outcomes: yes/no, 0/1, or true/false. Logical regression analyzes the relationship between one or more independent variables and classifies the data into discrete classes. This relationship is then used to predict the value of one of those factors, the probability of a binary outcome using the logistic (sigmoid) function. It is mostly used for predictive modeling.

For example, 0 represents a negative class, and 1 represents a positive class. Logistic regression is commonly used in binary classification problems where the outcome variable reveals either of the two categories (0 and 1).

Common pitfall: Misunderstanding that logistic regression is not a classification algorithm but a probability estimator.

What is linear regression, and what are the different assumptions of linear regression algorithms?
Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It is a type of supervised machine learning algorithm that computes the linear relationship between the dependent variable and one or more independent features by fitting a linear equation with observed data. It predicts the output variables based on the independent input variable.

For example, if you want to predict someone's salary, you use various factors such as years of experience, education level, industry of employment, and location of the job. Linear regression uses all these parameters to predict the salary as it is considered a linear relation between all these factors and the price of the house.

Assumptions for linear regression include:

Linearity: Linear regression assumes there is a linear relationship between the independent and dependent variables. This means that changes in the independent variable lead to proportional changes in the dependent variable, whether positively or negatively.
Independence of errors: The observations should be independent from each other, that is, the errors from one observation should not influence another.
Homoscedasticity (equal variance): Linear regression assumes the variance of the errors is constant across all levels of the independent variable(s). This indicates that the amount of the independent variable(s) has no impact on the variance of the errors.
Normality of residuals: This means that the residuals should follow a bell-shaped curve. If the residuals are not normally distributed, then linear regression will not be an accurate model.
No multicollinearity: Linear regression assumes there is no correlation between the independent variables chosen for the model.
What are support vectors in SVM (Support Vector Machine)?
A Support Vector Machine (SVM) is a supervised machine learning algorithm used mainly for classification tasks. It finds the optimal hyperplane in an N-dimensional space that separates data points into different classes while maximizing the margin between the closest points of each class.

Support vectors are the most important data points, useful in defining the optimal hyperplane that separates different classes.

What is the difference between KNN and K-means?
KNN stands for K-nearest neighbors is a classification (or regression) algorithm that, to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points.

K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.

KNN vs. K-means

This table shows the difference between KNN and K-means depending on the use case, data usage, purpose, and other features.

Feature	K-Nearest Neighbors (KNN)	K-Means Clustering
Algorithm Type	Supervised Learning (Classification/Regression)	Unsupervised Learning (Clustering)
Purpose	Classifies new data points on labeled training data	Groups unlabeled data points into clusters
Data Usage	Uses the entire dataset for predictions	Splits the data into clusters iteratively
Scalability	Slow for large datasets because all data points are needed for predictions	Faster for large datasets because initial centroids are already set
Example Use Case	Image Classification, Recommendation Systems	Customer Grouping
What's the difference between bagging and boosting?
Ensemble techniques in machine learning combine multiple weak models into a strong, more accurate predictive model, using the collective intelligence of diverse models to improve performance. Bagging and boosting are different ensemble techniques that use multiple models to reduce error and optimize the model.

Bagging vs. boosting

The bagging technique uses multiple models trained on different subsets of data. It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods and is a special case of the model averaging approach. Boosting is an ensemble modeling technique designed to create a strong classifier by combining multiple weak classifiers. The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones.

Bagging: Builds multiple models in parallel using bootstrapped datasets to reduce variance (e.g., Random Forest).
Boosting: Builds models sequentially, each trying to correct errors from the previous, reducing bias (e.g., XGBoost).
Compare and contrast linear regression and logistic regression. When would you use one over the other?
A linear regression algorithm defines a linear relationship between independent and dependent variables. It uses a linear equation to identify the line of best fit (straight line) for a problem, which it then uses to predict the output of the dependent variables.

Linear and logistic regression

A logistic regression algorithm predicts a binary outcome for an event based on a dataset's previous observations. Its output lies between 0 and 1. The algorithm uses independent variables to predict the occurrence or failure of specific events.

You use linear regression when the outcome is a continuous value, such as price or temperature. You should use logistic regression when the outcome is a categorical value like spam/not spam, yes/no, etc.

Linear vs. logistic regression

What is regularization (L1 and L2), and why is it useful?
Regularization is a technique in machine learning to prevent models from overfitting. Overfitting happens when a model doesn't just learn from the underlying patterns (signals) in the training data but also picks up and amplifies the noise in it. This leads to a model that performs well on training data but poorly on new data.

L1 and L2 regularization are methods used to mitigate overfitting in machine learning models by adding a penalty term on coefficients to the model's loss function. This penalty discourages the model from assigning too much importance to any single feature (represented by large coefficients), making the model more straightforward. Regularization keeps the model balanced and focused on the true signal, enhancing its ability to generalize to unseen data.

A regression model that uses the L1 regularization technique is called lasso regression, and a model that uses the L2 is called ridge regression.

L1 Regularization: Also called a lasso regression, this adds the absolute value of the sum ("absolute value of magnitude") of coefficients as a penalty term to the loss function.
L2 Regularization: Also called a ridge regression, this adds the squared sum ("squared magnitude") of coefficients as the penalty term to the loss function.
What are precision, recall, F1 score, and AUC-ROC?
Once a machine learning model has been created, it is important to evaluate and test how well it performs on data. An evaluation metric is a mathematical quantifier of the quality of the model. Precision, Recall, F1 Score, and AUC-ROC are all evaluation metrics.

Precision is all about how accurate your positive predictions are. Of all the items your model labeled as positive, how many were actually positive? Formula = TP / (TP + FP). It tells you how much you can trust the positives your model finds.
Recall focuses on finding all the actual positives. It measures how well your model catches everything it should've caught. Formula = TP / (TP + FN). It's especially useful when missing a positive is costly like missing a cancer diagnosis.
F1 Score: F1 Score combines Recall and Precision into one performance metric. The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than Accuracy, especially if you have an uneven class distribution.
AUC-ROC: The AUC-ROC curve is a tool for evaluating the performance of binary classification models. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different thresholds, showing how well a model can distinguish between two classes, such as positive and negative outcomes. It provides a graphical representation of the model's ability to distinguish between two classes, like a positive class for the presence of a disease and a negative class for the absence of a disease.
Key: TP = True Positive, FP = False Positive, FN = False Negative.

What is a confusion matrix, and how do you interpret it?
A confusion matrix is a simple table that shows how well a classification model is performing by comparing its predictions to the actual results. It breaks down the predictions into four categories: correct predictions for both classes (true positives and true negatives) and incorrect predictions (false positives and false negatives). This helps you understand where the model is making mistakes so you can improve it.

TP: True Positives
TN: True Negatives
FP: False Positives
FN: False Negatives
Confusion matrix

Example: From the matrix below, there are 165 total cases:

True Negatives: 50
False Positives: 10
False Negatives: 5
True Positives: 100
Depending on the project you're working on, you can use metrics such as Accuracy, Precision, Recall, and F1 Score to evaluate your project. A confusion matrix is a visualization of it.

An example of confusion matrix

Common pitfall: Misreading the matrix layout or assuming it works for multi-class without modification.

What feature selection methods do you prefer when building a predictive model? How do you determine which features to keep or discard?
When building a predictive model, I like to combine practical steps and proven techniques to ensure that the features I include actually help the model rather than add noise, redundancy, or overfitting risk.

I'll approach like this:

Start with domain knowledge: Talk to stakeholders and review documentation to understand what features make the most sense in our business context.
Use filter methods for a first pass: I run statistical checks like correlation, ANOVA, chi-square tests, or mutual information to remove irrelevant or redundant features. Filter methods are fast, which is especially helpful when you're working with high-dimensional data.
Apply wrapper methods for performance tuning: For a more refined selection, I use wrapper methods like Recursive Feature Elimination (RFE). These methods evaluate subsets of features based on how well the model performs, which helps surface the most predictive combinations. They take more time but are worth it for high-impact models.
Leverage embedded methods for efficiency: Models like Lasso (L1), Ridge (L2), and tree-based models (Random Forest, XGBoost) have built-in feature importance. I like these because they optimize feature selection during model training, balancing speed and accuracy.
Hybrid approach: Sometimes, I start with a filter method to reduce dimensions and then fine-tune with wrapper or embedded methods. This hybrid approach saves time and improves performance.
How I decide what to drop or keep:

If a feature is highly correlated with another, I drop the weaker or noisier one.
If it has low variance and no predictive power, it goes.
If it helps interpretability or improves metrics on validation data, I keep it.
If it harms generalization or adds complexity, I drop it.
Explain the difference between false positive and false negative rates in a confusion matrix. How do these metrics impact model evaluation in a fraud detection scenario?
False Positive Rate (FPR) is the proportion of actual negatives that are incorrectly identified to be true. False Negative Rate is the proportion of actual positives that are incorrectly identified as negatives.

In a fraud detection scenario, both of these have damaging consequences:

False Positive Rate (FPR): If a model has a high false positive rate, it means the system flags many legitimate transactions as fraudulent. This will lead to customer frustration, as their transactions will be flagged regularly, and they could leave.

False Negative Rate (FNR): If a model has a high false negative rate, it means many fraudulent transactions are not detected, which could lead to significant financial business loss.

Coding Challenges
Write an SQL query to get the second-highest salary from an employee table.
To find the second highest salary, you can take one of two common methods: a subquery or a window function.

Method 1: Subquery method You first get the maximum salary from the table. Then, you find the highest salary that's less than that max, giving you the second highest salary.

This method is clean and efficient:

SELECT MAX(salary) AS SecondHighest
FROM employee
WHERE salary < (SELECT MAX(salary) FROM employee);
Method 2: Window function with DENSE_RANK() You rank all salaries in descending order using DENSE_RANK(), then filter for rank 2 to get the second highest. The LIMIT 1 ensures only one row is returned in case of ties. This method is better for flexibility if you want to choose the third highest or fourth, etc.

SELECT salary AS SecondHighest
FROM (
  SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) as rank
  FROM employee
) ranked
WHERE rank = 2
LIMIT 1;
Joining orders with customer information
To join order details with corresponding customer information, you use a simple inner join:

SELECT o.*, c.name, c.email
FROM orders o
JOIN customers c ON o.customer_id = c.id;
This pulls all orders where a matching customer exists. If you need every order, even those without matching customers, switch to a LEFT JOIN:

SELECT o.*, c.name, c.email
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.id;
Write an SQL query to find the top 5 customers by revenue.
To get the highest-spending customers, group by customer, sum their order totals, sort by that total, and limit the results:

SELECT customer_id, SUM(total_amount) AS revenue
FROM orders
GROUP BY customer_id
ORDER BY revenue DESC
LIMIT 5;
To add customer names, just join with the customers' table.

Common pitfall: Not grouping properly before ordering can result in incorrect aggregates.

Write an SQL query to find the top five customers by purchase amount in the last quarter.
To find your top customers who also bought across multiple categories, filter purchases within 3 months, group by customer, and apply category constraints with HAVING:

SELECT customer_id, SUM(amount) AS total_spent
FROM purchases
WHERE purchase_date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH)
GROUP BY customer_id
HAVING COUNT(DISTINCT category_id) >= 3
ORDER BY total_spent DESC
LIMIT 5;
This makes sure each customer bought from at least 3 categories. WHERE filters rows before grouping, while HAVING filters groups after.

How do you remove duplicates from a DataFrame?
To remove duplicates from a DataFrame:

df = df.drop_duplicates()
You can also refine this by specifying columns:

# Drop duplicates based on specific columns
df = df.drop_duplicates(subset=['customer_id', 'product_id'])
Control which duplicates to keep:

# Keep first or last occurrence
df = df.drop_duplicates(keep='last')  # or 'first'
Given a stream of data, how would you find the median in real time?
To compute the median in a stream of numbers, use two heaps:

Max-heap for the lower half
Min-heap for the upper half
Keep both heaps balanced
The median is either the top of one heap or the average of both tops
import heapq

class MedianFinder:
    def __init__(self):
        self.small = []  # max heap (negative values)
        self.large = []  # min heap
    
    def add_num(self, num):
        if len(self.small) == len(self.large):
            heapq.heappush(self.large, -heapq.heappushpop(self.small, -num))
        else:
            heapq.heappush(self.small, -heapq.heappushpop(self.large, num))
    
    def find_median(self):
        if len(self.small) == len(self.large):
            return (self.large[0] - self.small[0]) / 2.0
        else:
            return self.large[0]
Merge overlapping intervals
To merge overlapping intervals, first sort them, then iterate and merge as needed:

def merge_intervals(intervals):
    intervals.sort(key=lambda x: x[0])
    merged = [intervals[0]]
    for current in intervals[1:]:
        last = merged[-1]
        if current[0] <= last[1]:
            last[1] = max(last[1], current[1])
        else:
            merged.append(current)
    return merged
Sorting takes O(n log n), and the merge step is linear, making this efficient for large datasets.

How do you handle null values in pandas?
Basic null handling options:

df.fillna(0)      # Replace with 0  
df.dropna()       # Drop rows with nulls
Other methods to consider:

# Fill with mean/median/mode
df['column'].fillna(df['column'].mean())
df['column'].fillna(df['column'].median())
df['column'].fillna(df['column'].mode()[0])

# Forward/backward fill
df.fillna(method='ffill')  # Use previous value
df.fillna(method='bfill')  # Use next value

# Interpolation
df.interpolate()
How would you group and aggregate data in Python?
Basic group and sum:

df.groupby('category')['sales'].sum()
For more complex aggregations:

# Multiple aggregations
df.groupby('category').agg({
    'sales': ['sum', 'mean', 'count'],
    'profit': ['min', 'max']
})

# Custom aggregation functions
df.groupby('category')['sales'].agg(lambda x: x.max() - x.min())

# Reset index to convert back to a regular DataFrame
df.groupby('category')['sales'].sum().reset_index()
Why does RANK() skip sequence numbers in SQL?
Given values: 100, 90, 90, 80
RANK(): Skips ranks after ties
→ 1, 2, 2, 4
DENSE_RANK(): No skipping
→ 1, 2, 2, 3
ROW_NUMBER(): Ignores ties
→ 1, 2, 3, 4
Why use a RIGHT JOIN when a LEFT JOIN can suffice?
A RIGHT JOIN is the same as a LEFT JOIN with the table order reversed:

SELECT * FROM A RIGHT JOIN B ON A.id = B.id;
SELECT * FROM B LEFT JOIN A ON A.id = B.id;
What's the difference between .apply() and .map() in pandas?
.map(): Works only on Series, applies a function element-wise
.apply(): More versatile, works on both Series and DataFrames
.applymap(): Applies a function to every element in a DataFrame
# map - simple transformation of Series values
df['category'] = df['category_id'].map({1: 'Electronics', 2: 'Clothing'})

# apply on Series - more complex operations
df['name'] = df['name'].apply(lambda x: x.title())

# apply on DataFrame - process entire rows or columns
df.apply(lambda x: x.max() - x.min())

# applymap - element-wise operation on entire DataFrame
df.applymap(lambda x: f"{x:.2f}" if isinstance(x, float) else x)
How would you implement K-Means clustering in Python?
Basic usage:

from sklearn.cluster import KMeans  
kmeans = KMeans(n_clusters=3).fit(X)
labels = kmeans.labels_
Best practices for K-Means:

# Scale features
from sklearn.preprocessing import StandardScaler
X_scaled = StandardScaler().fit_transform(X)

# Use elbow method to find optimal k
distortions = []
K_range = range(1, 10)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    distortions.append(kmeans.inertia_)

# Plot elbow curve
import matplotlib.pyplot as plt
plt.plot(K_range, distortions)
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('Elbow Method For Optimal k')
How do you find RMSE and MSE in a linear regression model?
To evaluate a regression model:

from sklearn.metrics import mean_squared_error  
import numpy as np  
mse = mean_squared_error(y_true, y_pred)  
rmse = np.sqrt(mse)
MSE: Penalizes large errors heavily.
RMSE: More interpretable because it's in the same unit as the target.
Another alternative:

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)
MAE is less sensitive to outliers.

How can you calculate accuracy using a confusion matrix?
accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)  # How many selected items are relevant
recall = TP / (TP + FN)     # How many relevant items are selected
f1_score = 2 * (precision * recall) / (precision + recall)  # Harmonic mean

# For imbalanced datasets, consider:
specificity = TN / (TN + FP)
balanced_accuracy = (recall + specificity) / 2
Real Business Scenarios
How would you measure the success of a new product launch?
First, define success for that specific launch. Is it getting 1,000 new users in 30 days? Hitting a revenue or CTR goal? Building awareness in a new market? Or collecting feedback for future improvements?

Once that's clear, measure success using a mix of quantitative and qualitative KPIs across teams like marketing, product, and customer success. Some metrics I'll look at:

Launch campaign metrics: To gauge marketing performance, look at leads generated, channel performance (email, ads, social), website traffic, and press coverage.
Product adoption metrics: After launch, track trials, usage, activation, and user retention. These show how well the product is landing with your target audience.
Market impact metrics: Measure business impact through revenue, market share, and win rates against competitors.
Qualitative feedback: Talk to sales reps, product teams, and customers. Internal and external feedback helps you understand the "why" behind the numbers. Blending data with direct feedback gives you a more transparent, more nuanced view of what's working and what to improve.
Common pitfall: Focusing only on quantitative metrics without applying nuance to them. An example is tracking metrics like downloads without tracking engagement. Or tracking views without knowing who is viewing them.

You notice a sudden drop in website traffic, how would you investigate it?
To analyze a sudden drop in website traffic, I would follow these steps:

Determine if it's a drop or a trend: The first thing to do is try to understand whether your traffic has been declining for a while or if it has suddenly dropped for a day or two.
Rule out any manual actions: Sometimes, traffic drops happen because of a Google penalty. Check to see if there are any recent changes that you might have fallen foul of. Also, make sure that updates you made to your website's content didn't cause a problem.
Check for algorithm updates: Google frequently updates its ranking algorithm. When your site's performance drops suddenly, it's worth investigating whether an update might be responsible.
Investigate technical issues: Some of the most common technical issues are indexing errors, site speed, performance, and mobile usability.
Check competitor activity: Sometimes, traffic dips occur because competitors have stepped up their game. SEO tools like Ahrefs, SEMrush, or Moz help track your competitors' backlinks, keywords, and rankings. Check to see whether your competitors have started outranking you for previously held keywords or launched new content targeting your primary audience.
What if a model is 95% accurate, but the business is unhappy with the results?
There could be multiple reasons why a business could be unsatisfied with a model that has high accuracy. Some reasons are:

Focusing on the wrong metric: Sometimes, a model is not optimized for the specific business problem. Its accuracy is tied to the wrong thing. For example, in fraud detection, it is possible to still miss fraudulent transactions even with high accuracy scores.
Unrealistic expectations: The model might have unrealistic expectations placed on it to solve problems when, in reality, it is meant to be used in conjunction with other methods and metrics to give a nuanced view.
Overfitting: It is possible that the high accuracy comes from the model learning the training data rather than learning how to generalize.
To handle this problem, I'll:

Reevaluate the business goals: Sometimes, the business goals need to be defined so that there is a specific metric or group of metrics for the model to be trained towards.
Improve the model performance: You should do a deep dive into the model and fix any issues that you might notice, including overfitting, data issues or feature selection.
You're given a random dataset, how do you check if it's useful?
To check the quality of a random dataset, I'll:

Understand the problem context: The first thing to do is to make sure you understand the goal you aim to achieve before looking at the dataset. This allows you to know from first glance whether the dataset matches the problem. If the data has irrelevant columns, you should remove them.

Test the data quality: For any problem you are solving, it has most likely been solved before. You can test your dataset against a trusted dataset to measure any deviations that might be in the data. The dataset also needs to represent real-world scenarios.

Technical checks: With technical checks, it's good to remove duplicates in the data. Noise could be present with blurry images or mislabeled samples. You also have to make sure everything is formatted correctly in a consistent format.

Assess practical utility: The dataset has to be big enough for what you need it to do. For traditional machine learning, the dataset should be more than 10 times greater than the number of features per class. For deep learning, you should aim for 100 features per class to help avoid overfitting.

A dataset is only useful when it aligns with the problem's context and goals. It must pass accuracy, completeness, and balance checks. Finally, it must meet size and representative requirements.

How would you evaluate a classification model for medical diagnosis?
Healthcare classification models use machine learning to analyze vast amounts of patient data. They identify complex patterns and relationships, helping healthcare professionals predict health risks more accurately. These models help doctors make the right decision and reduce diagnostic errors. To evaluate classification models, you have to know the right metrics to use.

Accuracy, sensitivity, and specificity metrics: Accuracy, sensitivity, and specificity are critical in evaluating medical models. Accuracy measures the overall correctness of predictions. Sensitivity, or recall, shows the model's ability to identify positive cases. Specificity indicates how well it identifies negative cases. These metrics are vital for diagnostic accuracy assessment in various medical fields.
ROC curves and AUC analysis: ROC curves and AUC analysis are key metrics for healthcare AI performance. They evaluate a model's ability to distinguish between classes at different thresholds. A higher AUC score means better performance in distinguishing between positive and negative cases.
Cross-validation techniques: Cross-validation estimates a model's performance on unseen data. Techniques like k-fold cross-validation split data into subsets for training and testing, providing a robust assessment of the model's ability to generalize.
What is the difference between batch and online learning?
Batch learning is a term in artificial intelligence that refers to the process of training a machine learning model on a large set of data all at once, instead of continuously updating the model as new data comes in. This method allows for greater consistency and efficiency in the training process, as the model can learn from a fixed set of data before being deployed for use.

In batch learning, the model sees the entire dataset multiple times (known as epochs), refining its understanding with each pass. By processing data in large chunks, it converges more slowly but generally achieves higher accuracy.

Online learning takes a continuous, incremental approach. Instead of waiting for all the data to be available, you feed it to the model bit by bit, just like learning something new every day instead of cramming for a final exam. The model updates with each new data point, so it's constantly learning and evolving.

For example, imagine you're monitoring customer behavior on a website. Every time a user clicks or makes a purchase, your model gets smarter, learning from that single interaction and refining its predictions for the next.

What are the pros and cons of deep learning vs. traditional ML?
Deep learning uses multi-layered neural networks to handle complex tasks like image recognition, NLP, and recommendation systems. Think CNNs, RNNs, and Transformers.

Pros:

Handles complex, high-dimensional data (images, audio, text)
Works with both structured and unstructured data
Learns non-linear relationships
Scales well across use cases with techniques like transfer learning
Great at generalizing from large datasets
Cons:

Requires lots of data and compute
Heavily dependent on data quality
Hard to interpret (black box)
Comes with privacy and security concerns
Traditional ML uses simpler, more interpretable algorithms like decision trees, logistic regression, and support vector machines.

Pros:

Works well with smaller datasets
Faster to train and more straightforward to interpret
Lower computational cost
More transparent and explainable
Cons:

Struggles with complex/non-linear data
Needs manual feature engineering
Doesn't scale well with large datasets
Can overfit if not tuned properly
How do you monitor model performance in production?
You monitor model performance in production by tracking both functional and operational metrics.

Functional monitoring checks the health of the data and the model:

Data quality: Monitor for missing values, duplicates, and syntax errors.
Data/feature drift: Compare current input data to training data using stats like KL divergence, PSI, chi-squared, etc.
Model drift: Check if model accuracy drops over time due to changing patterns in the data.
Operational monitoring keeps the system running smoothly:

System health: Tracks latency, errors, and memory usage.
Input data health: Watch for type mismatches, nulls, and out-of-range values.
Model performance: Use precision/recall, RMSE, or top-k accuracy depending on the use case.
Business KPIs: Tie model performance to actual business outcomes (e.g., conversions, revenue impact).
Advanced Topics
How would you detect concept drift in your model?
Concept drift happens when the relationship between your input features and target variable changes over time, causing your model's performance to drop. It's common in dynamic environments (e.g., user behavior, market trends). COVID-19 is a real-world example: models trained pre-pandemic broke down because behavior and data patterns shifted.

How to detect it:

Set up reference vs. detection windows: Compare a stable past dataset (e.g., January traffic) against a current window (e.g., this week). This gives you a baseline.
Compare distributions: Use statistical tests (e.g., Kolmogorov–Smirnov, PSI, KL divergence) to detect shifts in data or feature distributions.
Track model performance over time: Drop in precision, recall, or overall accuracy compared to your baseline = red flag.
Run significance tests: This tells you if the drift is real or just noise.
How do you ensure fairness and remove bias from your models?
Fairness means your model makes decisions that don't unfairly favor or penalize any group. Bias can sneak in at any stage, like data collection, labeling, training, and even deployment, so it needs to be addressed early and often.

How to ensure fairness:

Start with diverse training data: Your data should reflect all the groups your model impacts. If it's skewed, the model will be too.
Preprocess to balance representation: Use techniques like oversampling underrepresented groups or reweighting the data.
Use bias detection tools: Libraries like Fairlearn, AIF360, and What-If Tool can help you spot performance gaps across subgroups.
Apply fairness constraints during training: Use regularization, adversarial debiasing, or post-processing adjustments to reduce harm to specific groups.
Build transparency into the model: Use interpretable models (e.g., decision trees, linear models) or explanation tools like SHAP and LIME.
Audit regularly across subgroups: Don't rely only on overall accuracy—look at performance across gender, race, age, etc.
Bring in human oversight: Humans should always be part of the loop, especially in high-stakes decisions (e.g., lending, hiring).
Which is better - specializing a model with fine-tuning or generalizing it with more data?
Fine-tuning a model is the process of adapting a pre-trained model for specific tasks or use cases. The reasoning behind fine-tuning is that it is easier and cheaper to improve the capabilities of a pre-trained base model that has already learned road knowledge about the task than it is to train a new model from scratch.

Generalization is a measure of how your model performs in predicting unseen data. Generalizing with more data is improving the model's ability to make predictions on new data rather than the data it was trained on.

Choosing whether to generalize with more data or fine-tune to achieve your goal depends on the specific situation.

For specialization with fine-tuning:

It is better when high performance is needed on a very specific task or domain.
It is more efficient to use when you have limited resources, but good data for a specific task.
It can achieve strong results with smaller models.
For generalization with more data:

It is better for models that need to handle a wide range of tasks.
It is great for situations where overfitting will be a problem.


Machine Learning Interview Questions For Freshers
1. What are some real-life applications of clustering algorithms?
Clustering algorithms are used in various real-life applications such as:

Customer segmentation for targeted marketing
Recommendation systems for personalized suggestions
Anomaly detection in fraud prevention
Image compression to reduce storage
Healthcare for grouping patients with similar conditions
Document categorization in search engines
2. How to choose an optimal number of clusters?
Elbow Method: Plot the explained variance or within-cluster sum of squares (WCSS) against the number of clusters. The "elbow" point, where the curve starts to flatten, indicates the optimal number of clusters.
Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters. The optimal number of clusters is the one with the highest average silhouette score.
Gap Statistic: Compares the clustering result with a random clustering of the same data. A larger gap between the real and random clustering suggests a more appropriate number of clusters.
3. What is feature engineering? How does it affect the model’s performance? 
Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations.

Also, there are times when multiple pieces of information are clubbed and provided as a single data column. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot.

4. What is overfitting in machine learning and how can it be avoided?
Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier.

To avoid overfitting there are multiple methods that we can use:

Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.
Using regularization methods like L1 or L2 regularization which is used to penalize the model's weights to avoid overfitting.
5. Why we cannot use linear regression for a classification task?
The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values. 

If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task.

6. Why do we perform normalization?
To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth.

7. What is the difference between precision and recall?
Precision is the ratio between the true positives(TP) and all the positive examples (TP+FP) predicted by the model. In other words, precision measures how many of the predicted positive examples are actually true positives. It is a measure of the model's ability to avoid false positives and make accurate positive predictions.

Precision
=
T
P
T
P
+
F
P
Precision= 
TP+FP
TP
​
 

In recall, we calculate the ratio of true positives (TP) and the total number of examples (TP+FN) that actually fall in the positive class. Recall measures how many of the actual positive examples are correctly identified by the model. It is a measure of the model's ability to avoid false negatives and identify all positive examples correctly.

Recall
=
T
P
T
P
+
F
N
Recall= 
TP+FN
TP
​
 

8. What is the difference between upsampling and downsampling?
In upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class and adding them to the dataset repeat this process till the dataset gets balanced for each class. But, here is a disadvantage the training accuracy becomes high as in each epoch model trained more than once in each epoch but the same high accuracy is not observed in the validation accuracy. 

In downsampling, we decrease the number of samples in the majority class by selecting some random number of points that are equal to the number of data points in the minority class so that the distribution becomes balanced. In this case, we have to suffer from data loss which may lead to the loss of some critical information as well. 

9. What is data leakage and how can we identify it?
If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable's information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage.

10. Explain the classification report and the metrics it includes.
The classification report provides key metrics to evaluate a model’s performance, including:

Precision: The proportion of true positives to all predicted positives, measuring accuracy of positive predictions.
Recall: The proportion of true positives to all actual positives, indicating how well the model finds positive instances.
F1-Score: The harmonic mean of precision and recall, balancing the two metrics.
Support: The number of true instances for each class in the dataset.
Accuracy: The overall proportion of correct predictions.
Macro Average: The average of precision, recall, and F1-score across all classes, treating them equally.
Weighted Average: The average of metrics, weighted by class support, giving more importance to frequent classes.
11. What are some of the hyperparameters of the random forest regressor which help to avoid overfitting?
The most important hyperparameters of a Random Forest are:

max_depth: Sometimes the larger depth of the tree can create overfitting. To overcome it, the depth should be limited.
n-estimator: It is the number of decision trees we want in our forest.
min_sample_split: It is the minimum number of samples an internal node must hold in order to split into further nodes.
max_leaf_nodes: It helps the model to control the splitting of the nodes and in turn, the depth of the model is also restricted.
12. What is the bias-variance tradeoff?
First, let’s understand what is bias and variance:

Bias refers to the difference between the actual values and the predicted values by the model. Low bias means the model has learned the pattern in the data and high bias means the model is unable to learn the patterns present in the data i.e the underfitting.
Variance refers to the change in accuracy of the model's prediction on which the model has not been trained. Low variance is a good case but high variance means that the performance of the training data and the validation data vary a lot.
If the bias is too low but the variance is too high then that case is known as overfitting. So, finding a balance between these two situations is known as the bias-variance trade-off.

13. Is it always necessary to use an 80:20 ratio for the train test split?
No, there is no such necessary condition that the data must be split into 80:20 ratio. The main purpose of the splitting is to have some data which the model has not seen previously so, that we can evaluate the performance of the model.

If the dataset contains let’s say 50,000 rows of data then only 1000 or maybe 2000 rows of data is enough to evaluate the model’s performance.

14. What is Principal Component Analysis?
PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly.

By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy.

15. What is one-shot learning?
One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of training on large datasets. This is useful when we haven't large datasets. It is applied to find the similarity and dissimilarities between the two images.

16. What is the difference between Manhattan Distance and Euclidean distance?
Both Manhattan Distance and Euclidean distance are two distance measurement techniques. 

Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. 
M
D
=
∣
x
1
−
x
2
∣
+
 
∣
y
1
−
y
2
∣
MD=∣x 
1
​
 −x 
2
​
 ∣+ ∣y 
1
​
 −y 
2
​
 ∣

Euclidean Distance (ED) is calculated as the square root of the sum of squared differences between the coordinates of two points along each dimension.
E
D
=
(
x
1
−
x
2
)
2
+
(
y
1
−
y
2
)
2
ED= 
(x 
1
​
 −x 
2
​
 ) 
2
 +(y 
1
​
 −y 
2
​
 ) 
2
 
​
 

Generally, these two metrics are used to evaluate the effectiveness of the clusters formed by a clustering algorithm.

17. What is the difference between one hot encoding and ordinal encoding?
One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented. In one hot encoding, we create a separate column for each category and add 0 or 1 as per the value corresponding to that row.

In ordinal encoding, we replace the categories with numbers from 0 to n-1 based on the order or rank where n is the number of unique categories present in the dataset. The main difference between one-hot encoding and ordinal encoding is that one-hot encoding results in a binary matrix representation of the data in the form of 0 and 1, it is used when there is no order or ranking between the dataset whereas ordinal encoding represents categories as ordinal values.

18. How can you conclude about the model's performance using the confusion matrix?
Confusion matrix summarizes the performance of a classification model. In a confusion matrix, we get four types of output (in case of a binary classification problem) which are TP, TN, FP, and FN. As we know that there are two diagonals possible in a square, and one of these two diagonals represents the numbers for which our model's prediction and the true labels are the same. Our target is also to maximize the values along these diagonals. From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.

Machine Learning Interveiw Questions
Machine Learning Interview Questions and Answers
19. Explain the working principle of SVM.
A data set that is not separable in different classes in one plane may be separable in another plane. This is exactly the idea behind the SVMin this a low dimensional data is mapped to high dimensional data so, that it becomes separable in the different classes. A hyperplane is determined after mapping the data into a higher dimension which can separate the data into categories.

SVM model can even learn non-linear boundaries with the objective that there should be as much margin as possible between the categories in which the data has been categorized. To perform this mapping different types of kernels are used like radial basis kernel, gaussian kernel, polynomial kernel, and many others.

20. What is the difference between the k-means and k-means++ algorithms?
The only difference between the two is in the way centroids are initialized. In the k-means algorithm, the centroids are initialized randomly from the given points. There is a drawback in this method that sometimes this random initialization leads to non-optimized clusters due to maybe initialization of two clusters close to each other. 

To overcome this problem k-means++ algorithm was formed. In k-means++, the first centroid is selected randomly from the data points. The selection of subsequent centroids is based on their separation from the initial centroids. The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected. This guarantees that the centroids are evenly spread apart and lowers the possibility of convergence to less-than-ideal clusters. This helps the algorithm reach the global minima instead of getting stuck at some local minima.

Read more about it here.

21. Explain some measures of similarity which are generally used in Machine learning.
Some of the most commonly used similarity measures are as follows:

Cosine Similarity: By considering the two vectors in n - dimension we evaluate the cosine of the angle between the two. The range of this similarity measure varies from [-1, 1] where the value 1 represents that the two vectors are highly similar and -1 represents that the two vectors are completely different from each other.
Euclidean or Manhattan Distance: These two values represent the distances between the two points in an n-dimensional plane. The only difference between the two is in the way the two are calculated.
Jaccard Similarity: It is also known as IoU or Intersection over union it is widely used in the field of object detection to evaluate the overlap between the predicted bounding box and the ground truth bounding box.
22. Whether decision tree or random forest is more robust to the outliers.
Decision trees and random forests are both relatively robust to outliers. A random forest model is an ensemble of multiple decision trees so, the output of a random forest model is an aggregate of multiple decision trees.

So, when we average the results the chances of overfitting get reduced. Hence we can say that the random forest models are more robust to outliers.

23. What is the difference between L1 and L2 regularization? What is their significance?
L1 regularization (Lasso regularization)adds the sum of the absolute values of the model's weights to the loss function. This penalty encourages sparsity in the model by pushing the weights of less important features to exactly zero. As a result, L1 regularization automatically performs feature selection, removing irrelevant or redundant features from the model, which can improve interpretability and reduce overfitting.

L2 regularization (Ridge regularization) in which we add the square of the weights to the loss function. In both of these regularization methods, weights are penalized but there is a subtle difference between the objective they help to achieve. 

In L2 regularization the weights are not penalized to 0 but they are near zero for irrelevant features. It is often used to prevent overfitting by shrinking the weights towards zero, especially when there are many features and the data is noisy.

24. What is a radial basis function?
RBF (radial basis function) is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center.

The formula for the radial basis function is as follows:

K
(
x
,
x
′
)
=
e
x
p
(
−
∥
x
−
x
′
∥
2
2
σ
2
)
K(x,x 
′
 
 )=exp(− 
2σ 
2
 
∥
∥
​
 x−x 
′
 
  
∥
∥
​
  
2
 
​
 )

Machine learning systems frequently use the RBF function for a variety of functions, including:

RBF networks can be used to approximate complex functions. By training the network's weights to suit a set of input-output pairs, 
RBF networks can be used for unsupervised learning to locate data groups. By treating the RBF centers as cluster centers,
RBF networks can be used for classification tasks by training the network's weights to divide inputs into groups based on how far from the RBF nodes they are.
It is one of the very famous kernels which is generally used in the SVM algorithm to map low dimensional data to a higher dimensional plane so, we can determine a boundary that can separate the classes in different regions of those planes with as much margin as possible. 

25. Explain SMOTE method used to handle data imbalance.
In SMOTE, we synthesize new data points using the existing ones from the minority classes by using linear interpolation. The advantage of using this method is that the model does not get trained on the same data. But the disadvantage of using this method is that it adds undesired noise to the dataset and can lead to a negative effect on the model’s performance.

26. Does the accuracy score always a good metric to measure the performance of a classification model?
No, there are times when we train our model on an imbalanced dataset the accuracy score is not a good metric to measure the performance of the model. In such cases, we use precision and recall to measure the performance of a classification model.

Also, f1-score is another metric that can be used to measure performance but in the end, f1-score is also calculated using precision and recall as the f1-score is nothing but the harmonic mean of the precision and recall.

27. What is KNN Imputer and how does it work?
KNN Imputer imputes missing values in a dataset compared to traditional methods like using mean, median, or mode. It is based on the K-Nearest Neighbors (KNN) algorithm, which fills missing values by referencing the values of the nearest neighbors.

Here’s how it works:

Neighborhood-based Imputation: The KNN Imputer identifies the k nearest neighbors to the data point with the missing value, based on a distance metric (e.g., Euclidean distance).
Imputation Process: Once the nearest neighbors are found, the missing value is imputed (filled) using a statistical measure, such as the mean or median, of the values from these neighbors.
Distance Parameter: The k parameter is used to define how many neighbors to consider when imputing a missing value, and the distance metric controls how similarity is measured between data points.
28. Explain the working procedure of the XGBoost model.
XGBoost model is an ensemble technique of machine learning in this method weights are optimized in a sequential manner by passing them to the decision trees. After each pass, the weights become better and better as each tree tries to optimize the weights, and finally, we obtain the best weights for the problem at hand. Techniques like regularized gradient and mini-batch gradient descent have been used to implement this algorithm so, that it works in a very fast and optimized manner.

29. What is the purpose of splitting a given dataset into training and validation data?
The main purpose is to keep some data left over on which the model has not been trained so, that we can evaluate the performance of our machine learning model after training. Also, sometimes we use the validation dataset to choose among the multiple state-of-the-art machine learning models. Like we first train some models let’s say LogisticRegression, XGBoost, or any other than test their performance using validation data and choose the model which has less difference between the validation and the training accuracy.

30. Explain some methods to handle missing values in that data.
Some of the methods to handle missing values are as follows:

Removing the rows with null values may lead to the loss of some important information.
Removing the column having null values if it has very less valuable information. it may lead to the loss of some important information.
Imputing null values with descriptive statistical measures like mean, mode, and median.
Using methods like KNN Imputer to impute the null values in a more sophisticated way.
31. What is the difference between k-means and the KNN algorithm?
K-means algorithm is one of the popular unsupervised machine learning algorithms which is used for clustering purposes. But, KNN is a model which is generally used for the classification task and is a supervised machine learning algorithm. The k-means algorithm helps us to label the data by forming clusters within the dataset.

32. What is Linear Discriminant Analysis?
Linear Discriminant Analysis (LDA) is a supervised machine learning dimensionality reduction technique because it uses target variables also for dimensionality reduction. It is commonly used for classification problems. The LDA mainly works on two objectives:

Maximize the distance between the means of the two classes.
Minimize the variation within each class.
33. How can we visualize high-dimensional data in 2-d?
One of the most common and effective methods is by using the t-SNE algorithm which is a short form for t-Distributed Stochastic Neighbor Embedding. This algorithm uses some non-linear complex methods to reduce the dimensionality of the given data. We can also use PCA or LDA to convert n-dimensional data to 2 - dimensional so, that we can plot it to get visuals for better analysis. But the difference between the PCA and t-SNE is that the former tries to preserve the variance of the dataset but the t-SNE tries to preserve the local similarities in the dataset.

34. What is the reason behind the curse of dimensionality?
As the dimensionality of the input data increases the amount of data required to generalize or learn the patterns present in the data increases. For the model, it becomes difficult to identify the pattern for every feature from the limited number of datasets or we can say that the weights are not optimized properly due to the high dimensionality of the data and the limited number of examples used to train the model. Due to this after a certain threshold for the dimensionality of the input data, we have to face the curse of dimensionality.

35. Which metric is more robust to outliers: MAE, MSE, or RMSE?
Out of the three metrics—Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)—MAE is more robust to outliers.

The reason behind this is the way each metric handles error values:

MSE and RMSE both square the error values. When there are outliers, the error is typically large, and squaring it results in even larger error values. This causes outliers to disproportionately affect the overall error, leading to misleading results and potentially distorting the model’s performance.
MAE, on the other hand, takes the absolute value of the errors. Since it does not square the error terms, the influence of large errors (outliers) is linear rather than exponential, making MAE less sensitive to outliers.
36. Why removing highly correlated features are considered a good practice?
When two features are highly correlated, they may provide similar information to the model, which may cause overfitting. If there are highly correlated features in the dataset then they unnecessarily increase the dimensionality of the feature space and sometimes create the problem of the curse of dimensionality. If the dimensionality of the feature space is high then the model training may take more time than expected, it will increase the complexity of the model and chances of error. This somehow also helps us to achieve data compression as the features have been removed without much loss of data.

37. What is the difference between the content-based and collaborative filtering algorithms of recommendation systems?
In a content-based recommendation system, similarities in the content and services are evaluated, and then by using these similarity measures from past data we recommend products to the user. But on the other hand in collaborative filtering, we recommend content and services based on the preferences of similar users.

For example, if one user has taken A and B services in past and a new user has taken service A then service A will be recommended to him based on the other user's preferences.

38. How you would assess the goodness-of-fit for a linear regression model? Which metrics would you consider most important and why?
To evaluate the performance of a linear regression model, important key metrics are: R-squared, Adjusted R-squared, RMSE, and F-Statistics. R-squared is particularly important as it reflects the proportion of variance in the dependent variable that can be explained by the independent variables, providing a measure of how well our model fits the data. However, Adjusted R-squared also plays a crucial role, especially when comparing models with different numbers of predictors. It adjusts for the complexity of the model, helping to prevent overfitting and ensuring the robustness of our findings.

To learn more about regression metrics, check out: Regression Metrics

39. What is the null hypothesis in linear regression problem?
In linear regression, the null hypothesis id that there is no relationship between the independent variable(s) and the dependent variable. This is formally represented as 
H
0
:
β
1
=
0
H 
0
​
 :β 
1
​
 =0, where 
β
1
β 
1
​
 ​ is the coefficient of the independent variable.

Essentially, the null hypothesis suggests that the predictor variable does not contribute to predicting the outcome. For instance, if the null hypothesis states that the slope of the regression line is zero, then a student's score in an English class would not be a useful predictor of their overall grade-point average.

The alternative hypothesis, denoted as 
H
1
:
β
1
≠
0
H 
1
​
 :β 
1
​
 

=0, proposes that changes in the independent variable are indeed associated with changes in the dependent variable, indicating a meaningful relationship.

40. Can SVMs be used for both classification and regression tasks?
Yes, Support Vector Machines (SVMs) can be used for both classification and regression. For classification, SVMs work by finding a hyperplane that separates different classes in the data with the largest gap possible.

For regression, which involves predicting a continuous number, SVMs are adapted into a version called Support Vector Regression (SVR). SVR tries to fit as many data points as possible within a certain range of the predicted line, allowing some errors but penalizing those that are too large. This makes it useful for predicting values in situations where the data shows complex patterns.

To learn how to implement Support Vector Regression, you can refer to: Support Vector Regression (SVR) using Linear and Non-Linear Kernels in Scikit Learn

41. Explain the concept of weighting in KNN? What are the different ways to assign weights, and how do they affect the model's predictions?
Weighting in KNN assigns different levels of importance to the neighbors based on their distance from the query point, influencing how each neighbor affects the model's predictions.

The weights can be assigned using:

Uniform Weighting: All neighbors have equal weight regardless of their distance.
Distance Weighting: Weights are inversely proportional to the distance, giving closer neighbors more influence.
User-defined Weights: Weights are assigned based on domain knowledge or specific data characteristics.
Effect on Model's Prediction:

Uniform Weighting: Simple but may not perform well with noisy data or varied distances.
Distance Weighting: Improves accuracy by emphasizing closer neighbors, useful for irregular class boundaries but sensitive to anomalies.
User-defined Weights: Optimizes performance when specific insights about the dataset are applied, though less generalizable.
42. What are the assumptions behind the K-means algorithm? How do these assumptions affect the results?
The assumptions of K-Means algorithm include:

Cluster Shape: Assumes clusters are spherical and of similar size, affecting how well it handles non-spherical groups.
Scale of Features: Assumes features are on similar scales; different ranges can distort the distance calculation.
Clusters are Balanced: Assumes clusters have a roughly equal number of observations, which can bias results against smaller clusters.
Similar Density: Assumes all clusters have similar density, impacting the algorithm's effectiveness with clusters of varying densities.
If these assumptions are not met, the model will perform poorly making difficult to process and select clustering techniques that align with the data characteristics.

Check out the article: K Means Clustering Assumptions

43. Can you explain the concept of convergence in K-means? What conditions must be met for K-means to converge?
Convergence in K-means occurs when the cluster centroids stabilize, and the assignment of data points to clusters no longer changes. This happens when the algorithm has minimized the sum of squared distances between points and their corresponding centroids.

Conditions for K-means to Converge:

Proper Initialization: The initial placement of centroids significantly impacts convergence. Techniques like k-means++ help ensure a better start.
Data Characteristics: The algorithm converges more effectively if the data naturally clusters into well-separated groups. Overlapping or complex cluster shapes can hinder convergence.
Correct Number of Clusters (k): Choosing the right number of clusters is critical; too many or too few can lead to slow convergence or convergence to suboptimal solutions.
Algorithm Parameters: Setting a maximum number of iterations and a small tolerance for centroid change helps prevent infinite loops and determines when the algorithm should stop if centroids move minimally between iterations.
44. What is the significance of tree pruning in XGBoost? How does it affect the model?
Tree pruning in XGBoost is used to reduce model complexity and prevent overfitting. XGBoost implements a "pruning-as-you-grow" strategy where it starts by growing a full tree up to a maximum depth, then prunes back the branches that contribute minimal gains in terms of loss reduction. This is guided by the gamma parameter, which sets a minimum loss reduction required to make further partitions on a leaf node.

Effect on the Model:

Reduces Overfitting: By trimming unnecessary branches, pruning helps in creating simpler models that generalize better to unseen data, reducing the likelihood of overfitting.
Improves Performance: Pruning helps in removing splits that have little impact, which can enhance the model's performance by focusing on more significant attributes.
Optimizes Computational Efficiency: It decreases the complexity of the final model, which can lead to faster training and prediction times as there are fewer nodes to traverse during decision making.
45. How does Random Forest ensure diversity among the trees in the model?
Random Forest ensures diversity among the trees in its ensemble through two main mechanisms:

Bootstrap Aggregating (Bagging): Each tree in a Random Forest is trained on a different bootstrap sample, a random subset of the data. This sampling with replacement means that each tree sees different portions of the data, leading to variations in their learning and decision-making processes.
Feature Randomness: When splitting a node during the construction of the tree, Random Forest randomly selects a subset of features instead of using all available features. This variation in the feature set ensures that trees do not follow the same paths or use the same splits, thereby increasing the diversity among the trees.
The diversity among trees reduces the variance of the model without significantly increasing the bias.

46. What is the concept of information gain in decision trees? How does it guide the creation of the tree structure?
Information gain is a measure used in decision trees to select the best feature that splits the data into the most informative subsets. It is calculated based on the reduction in entropy or impurity after a dataset is split on an attribute. Entropy is a measure of the randomness or uncertainty in the data set, and information gain quantifies how much splitting on a particular attribute reduces that randomness.

47. How does the independence assumption affect the accuracy of a Naive Bayes classifier?
Naive Bayes classifier operates under the assumption that all features in the dataset are independent of each other given the class label. This assumption simplifies the computation of the classifier's probability model, as it allows the conditional probability of the class given multiple features to be calculated as the product of the individual probabilities for each feature.

Affect of accuracy on a Naive Bayes classifier:

Strengths in High-Dimensional Data: In practice, the independence assumption can sometimes lead to good performance, especially in high-dimensional settings like text classification, despite the interdependencies among features. This is because the errors in probability estimates may cancel out across the large number of features.
Limitations Due to Feature Dependency: The accuracy of Naive Bayes can be adversely affected when features are not independent, particularly if the dependencies between features are strong and critical to predicting the class. The model may underperform in such scenarios because it fails to capture the interactions between features.
Generalization Capability: The simplistic nature of the independence assumption often allows Naive Bayes to perform well on smaller datasets or in cases where data for training is limited, as it does not require as complex a model as other classifiers.
48. Why does PCA maximize the variance in the data?
PCA aims to maximize the variance because variance represents how much information is spread out in a given direction. The higher the variance along a direction, the more information that direction holds about the data. By focusing on the directions of highest variance, PCA helps us:

Preserve information while reducing the dimensionality.
Simplify the data by eliminating less important features (those with low variance)
49. How do you evaluate the effectiveness of a machine learning model in an imbalanced dataset scenario? What metrics would you use instead of accuracy?
We can use Precision, Recall, F1 score and ROC-AUC to evaluate the effectiveness of machine learning model in imbalanced dataset scenario. The best metric is F1 score as it combines both precision and recall into single metric that is important in imbalanced datasets where a high number of true negatives can skew accuracy. By focusing on both false positives and false negatives, the F1-score ensures that both the positive class detection and false positives are accounted for.

If the cost of false positives (Type I errors) and false negatives (Type II errors) is similar, F1-Score strikes a good balance.
It is especially useful when you need to prioritize performance in detecting the minority class (positive class).
However, if you are more concerned about false positives or false negatives specifically, you may opt for:

Precision (if false positives are more costly) or
Recall (if false negatives are more costly).
50. How the One-Class SVM algorithm works for anomaly detection?
One-Class SVM is an unsupervised anomaly detection algorithm. It is often used when only normal data is available. The model learns a decision boundary around normal data points using a kernel, typically an RBF, to map the data into a higher-dimensional space. The algorithm identifies support vectors—data points closest to the boundary—and any new data point outside this boundary is flagged as an anomaly. Key parameters like 'nu' control the fraction of outliers allowed, while the kernel defines the boundary shape.

51. Explain the concept of "concept drift" in anomaly detection.
Concept driftrefers to the change in the underlying distribution or patterns in the data over time, which can make previously normal data points appear as anomalies. In anomaly detection, this is particularly challenging because a model trained on old data may not recognize new, evolving patterns as part of the normal data distribution. Concept drift can occur suddenly or gradually and needs to be monitored closely. To address this, models can be adapted through periodic retraining with new data or by using adaptive anomaly detection algorithms.


Machine Learning Interview Questions For Freshers
1. What are some real-life applications of clustering algorithms?
Clustering algorithms are used in various real-life applications such as:

Customer segmentation for targeted marketing
Recommendation systems for personalized suggestions
Anomaly detection in fraud prevention
Image compression to reduce storage
Healthcare for grouping patients with similar conditions
Document categorization in search engines
2. How to choose an optimal number of clusters?
Elbow Method: Plot the explained variance or within-cluster sum of squares (WCSS) against the number of clusters. The "elbow" point, where the curve starts to flatten, indicates the optimal number of clusters.
Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters. The optimal number of clusters is the one with the highest average silhouette score.
Gap Statistic: Compares the clustering result with a random clustering of the same data. A larger gap between the real and random clustering suggests a more appropriate number of clusters.
3. What is feature engineering? How does it affect the model’s performance? 
Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations.

Also, there are times when multiple pieces of information are clubbed and provided as a single data column. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot.

4. What is overfitting in machine learning and how can it be avoided?
Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier.

To avoid overfitting there are multiple methods that we can use:

Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.
Using regularization methods like L1 or L2 regularization which is used to penalize the model's weights to avoid overfitting.
5. Why we cannot use linear regression for a classification task?
The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values. 

If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task.

6. Why do we perform normalization?
To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth.

7. What is the difference between precision and recall?
Precision is the ratio between the true positives(TP) and all the positive examples (TP+FP) predicted by the model. In other words, precision measures how many of the predicted positive examples are actually true positives. It is a measure of the model's ability to avoid false positives and make accurate positive predictions.

Precision
=
T
P
T
P
+
F
P
Precision= 
TP+FP
TP
​
 

In recall, we calculate the ratio of true positives (TP) and the total number of examples (TP+FN) that actually fall in the positive class. Recall measures how many of the actual positive examples are correctly identified by the model. It is a measure of the model's ability to avoid false negatives and identify all positive examples correctly.

Recall
=
T
P
T
P
+
F
N
Recall= 
TP+FN
TP
​
 

8. What is the difference between upsampling and downsampling?
In upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class and adding them to the dataset repeat this process till the dataset gets balanced for each class. But, here is a disadvantage the training accuracy becomes high as in each epoch model trained more than once in each epoch but the same high accuracy is not observed in the validation accuracy. 

In downsampling, we decrease the number of samples in the majority class by selecting some random number of points that are equal to the number of data points in the minority class so that the distribution becomes balanced. In this case, we have to suffer from data loss which may lead to the loss of some critical information as well. 

9. What is data leakage and how can we identify it?
If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable's information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage.

10. Explain the classification report and the metrics it includes.
The classification report provides key metrics to evaluate a model’s performance, including:

Precision: The proportion of true positives to all predicted positives, measuring accuracy of positive predictions.
Recall: The proportion of true positives to all actual positives, indicating how well the model finds positive instances.
F1-Score: The harmonic mean of precision and recall, balancing the two metrics.
Support: The number of true instances for each class in the dataset.
Accuracy: The overall proportion of correct predictions.
Macro Average: The average of precision, recall, and F1-score across all classes, treating them equally.
Weighted Average: The average of metrics, weighted by class support, giving more importance to frequent classes.
11. What are some of the hyperparameters of the random forest regressor which help to avoid overfitting?
The most important hyperparameters of a Random Forest are:

max_depth: Sometimes the larger depth of the tree can create overfitting. To overcome it, the depth should be limited.
n-estimator: It is the number of decision trees we want in our forest.
min_sample_split: It is the minimum number of samples an internal node must hold in order to split into further nodes.
max_leaf_nodes: It helps the model to control the splitting of the nodes and in turn, the depth of the model is also restricted.
12. What is the bias-variance tradeoff?
First, let’s understand what is bias and variance:

Bias refers to the difference between the actual values and the predicted values by the model. Low bias means the model has learned the pattern in the data and high bias means the model is unable to learn the patterns present in the data i.e the underfitting.
Variance refers to the change in accuracy of the model's prediction on which the model has not been trained. Low variance is a good case but high variance means that the performance of the training data and the validation data vary a lot.
If the bias is too low but the variance is too high then that case is known as overfitting. So, finding a balance between these two situations is known as the bias-variance trade-off.

13. Is it always necessary to use an 80:20 ratio for the train test split?
No, there is no such necessary condition that the data must be split into 80:20 ratio. The main purpose of the splitting is to have some data which the model has not seen previously so, that we can evaluate the performance of the model.

If the dataset contains let’s say 50,000 rows of data then only 1000 or maybe 2000 rows of data is enough to evaluate the model’s performance.

14. What is Principal Component Analysis?
PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly.

By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy.

15. What is one-shot learning?
One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of training on large datasets. This is useful when we haven't large datasets. It is applied to find the similarity and dissimilarities between the two images.

16. What is the difference between Manhattan Distance and Euclidean distance?
Both Manhattan Distance and Euclidean distance are two distance measurement techniques. 

Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. 
M
D
=
∣
x
1
−
x
2
∣
+
 
∣
y
1
−
y
2
∣
MD=∣x 
1
​
 −x 
2
​
 ∣+ ∣y 
1
​
 −y 
2
​
 ∣

Euclidean Distance (ED) is calculated as the square root of the sum of squared differences between the coordinates of two points along each dimension.
E
D
=
(
x
1
−
x
2
)
2
+
(
y
1
−
y
2
)
2
ED= 
(x 
1
​
 −x 
2
​
 ) 
2
 +(y 
1
​
 −y 
2
​
 ) 
2
 
​
 

Generally, these two metrics are used to evaluate the effectiveness of the clusters formed by a clustering algorithm.

17. What is the difference between one hot encoding and ordinal encoding?
One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented. In one hot encoding, we create a separate column for each category and add 0 or 1 as per the value corresponding to that row.

In ordinal encoding, we replace the categories with numbers from 0 to n-1 based on the order or rank where n is the number of unique categories present in the dataset. The main difference between one-hot encoding and ordinal encoding is that one-hot encoding results in a binary matrix representation of the data in the form of 0 and 1, it is used when there is no order or ranking between the dataset whereas ordinal encoding represents categories as ordinal values.

18. How can you conclude about the model's performance using the confusion matrix?
Confusion matrix summarizes the performance of a classification model. In a confusion matrix, we get four types of output (in case of a binary classification problem) which are TP, TN, FP, and FN. As we know that there are two diagonals possible in a square, and one of these two diagonals represents the numbers for which our model's prediction and the true labels are the same. Our target is also to maximize the values along these diagonals. From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.

Machine Learning Interveiw Questions
Machine Learning Interview Questions and Answers
19. Explain the working principle of SVM.
A data set that is not separable in different classes in one plane may be separable in another plane. This is exactly the idea behind the SVMin this a low dimensional data is mapped to high dimensional data so, that it becomes separable in the different classes. A hyperplane is determined after mapping the data into a higher dimension which can separate the data into categories.

SVM model can even learn non-linear boundaries with the objective that there should be as much margin as possible between the categories in which the data has been categorized. To perform this mapping different types of kernels are used like radial basis kernel, gaussian kernel, polynomial kernel, and many others.

20. What is the difference between the k-means and k-means++ algorithms?
The only difference between the two is in the way centroids are initialized. In the k-means algorithm, the centroids are initialized randomly from the given points. There is a drawback in this method that sometimes this random initialization leads to non-optimized clusters due to maybe initialization of two clusters close to each other. 

To overcome this problem k-means++ algorithm was formed. In k-means++, the first centroid is selected randomly from the data points. The selection of subsequent centroids is based on their separation from the initial centroids. The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected. This guarantees that the centroids are evenly spread apart and lowers the possibility of convergence to less-than-ideal clusters. This helps the algorithm reach the global minima instead of getting stuck at some local minima.

Read more about it here.

21. Explain some measures of similarity which are generally used in Machine learning.
Some of the most commonly used similarity measures are as follows:

Cosine Similarity: By considering the two vectors in n - dimension we evaluate the cosine of the angle between the two. The range of this similarity measure varies from [-1, 1] where the value 1 represents that the two vectors are highly similar and -1 represents that the two vectors are completely different from each other.
Euclidean or Manhattan Distance: These two values represent the distances between the two points in an n-dimensional plane. The only difference between the two is in the way the two are calculated.
Jaccard Similarity: It is also known as IoU or Intersection over union it is widely used in the field of object detection to evaluate the overlap between the predicted bounding box and the ground truth bounding box.
22. Whether decision tree or random forest is more robust to the outliers.
Decision trees and random forests are both relatively robust to outliers. A random forest model is an ensemble of multiple decision trees so, the output of a random forest model is an aggregate of multiple decision trees.

So, when we average the results the chances of overfitting get reduced. Hence we can say that the random forest models are more robust to outliers.

23. What is the difference between L1 and L2 regularization? What is their significance?
L1 regularization (Lasso regularization)adds the sum of the absolute values of the model's weights to the loss function. This penalty encourages sparsity in the model by pushing the weights of less important features to exactly zero. As a result, L1 regularization automatically performs feature selection, removing irrelevant or redundant features from the model, which can improve interpretability and reduce overfitting.

L2 regularization (Ridge regularization) in which we add the square of the weights to the loss function. In both of these regularization methods, weights are penalized but there is a subtle difference between the objective they help to achieve. 

In L2 regularization the weights are not penalized to 0 but they are near zero for irrelevant features. It is often used to prevent overfitting by shrinking the weights towards zero, especially when there are many features and the data is noisy.

25. Explain SMOTE method used to handle data imbalance.
In SMOTE, we synthesize new data points using the existing ones from the minority classes by using linear interpolation. The advantage of using this method is that the model does not get trained on the same data. But the disadvantage of using this method is that it adds undesired noise to the dataset and can lead to a negative effect on the model’s performance.

26. Does the accuracy score always a good metric to measure the performance of a classification model?
No, there are times when we train our model on an imbalanced dataset the accuracy score is not a good metric to measure the performance of the model. In such cases, we use precision and recall to measure the performance of a classification model.

Also, f1-score is another metric that can be used to measure performance but in the end, f1-score is also calculated using precision and recall as the f1-score is nothing but the harmonic mean of the precision and recall.

27. What is KNN Imputer and how does it work?
KNN Imputer imputes missing values in a dataset compared to traditional methods like using mean, median, or mode. It is based on the K-Nearest Neighbors (KNN) algorithm, which fills missing values by referencing the values of the nearest neighbors.

Here’s how it works:

Neighborhood-based Imputation: The KNN Imputer identifies the k nearest neighbors to the data point with the missing value, based on a distance metric (e.g., Euclidean distance).
Imputation Process: Once the nearest neighbors are found, the missing value is imputed (filled) using a statistical measure, such as the mean or median, of the values from these neighbors.
Distance Parameter: The k parameter is used to define how many neighbors to consider when imputing a missing value, and the distance metric controls how similarity is measured between data points.
28. Explain the working procedure of the XGBoost model.
XGBoost model is an ensemble technique of machine learning in this method weights are optimized in a sequential manner by passing them to the decision trees. After each pass, the weights become better and better as each tree tries to optimize the weights, and finally, we obtain the best weights for the problem at hand. Techniques like regularized gradient and mini-batch gradient descent have been used to implement this algorithm so, that it works in a very fast and optimized manner.

29. What is the purpose of splitting a given dataset into training and validation data?
The main purpose is to keep some data left over on which the model has not been trained so, that we can evaluate the performance of our machine learning model after training. Also, sometimes we use the validation dataset to choose among the multiple state-of-the-art machine learning models. Like we first train some models let’s say LogisticRegression, XGBoost, or any other than test their performance using validation data and choose the model which has less difference between the validation and the training accuracy.

30. Explain some methods to handle missing values in that data.
Some of the methods to handle missing values are as follows:

Removing the rows with null values may lead to the loss of some important information.
Removing the column having null values if it has very less valuable information. it may lead to the loss of some important information.
Imputing null values with descriptive statistical measures like mean, mode, and median.
Using methods like KNN Imputer to impute the null values in a more sophisticated way.
31. What is the difference between k-means and the KNN algorithm?
K-means algorithm is one of the popular unsupervised machine learning algorithms which is used for clustering purposes. But, KNN is a model which is generally used for the classification task and is a supervised machine learning algorithm. The k-means algorithm helps us to label the data by forming clusters within the dataset.

32. What is Linear Discriminant Analysis?
Linear Discriminant Analysis (LDA) is a supervised machine learning dimensionality reduction technique because it uses target variables also for dimensionality reduction. It is commonly used for classification problems. The LDA mainly works on two objectives:

Maximize the distance between the means of the two classes.
Minimize the variation within each class.
33. How can we visualize high-dimensional data in 2-d?
One of the most common and effective methods is by using the t-SNE algorithm which is a short form for t-Distributed Stochastic Neighbor Embedding. This algorithm uses some non-linear complex methods to reduce the dimensionality of the given data. We can also use PCA or LDA to convert n-dimensional data to 2 - dimensional so, that we can plot it to get visuals for better analysis. But the difference between the PCA and t-SNE is that the former tries to preserve the variance of the dataset but the t-SNE tries to preserve the local similarities in the dataset.

34. What is the reason behind the curse of dimensionality?
As the dimensionality of the input data increases the amount of data required to generalize or learn the patterns present in the data increases. For the model, it becomes difficult to identify the pattern for every feature from the limited number of datasets or we can say that the weights are not optimized properly due to the high dimensionality of the data and the limited number of examples used to train the model. Due to this after a certain threshold for the dimensionality of the input data, we have to face the curse of dimensionality.

35. Which metric is more robust to outliers: MAE, MSE, or RMSE?
Out of the three metrics—Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)—MAE is more robust to outliers.

The reason behind this is the way each metric handles error values:

MSE and RMSE both square the error values. When there are outliers, the error is typically large, and squaring it results in even larger error values. This causes outliers to disproportionately affect the overall error, leading to misleading results and potentially distorting the model’s performance.
MAE, on the other hand, takes the absolute value of the errors. Since it does not square the error terms, the influence of large errors (outliers) is linear rather than exponential, making MAE less sensitive to outliers.
36. Why removing highly correlated features are considered a good practice?
When two features are highly correlated, they may provide similar information to the model, which may cause overfitting. If there are highly correlated features in the dataset then they unnecessarily increase the dimensionality of the feature space and sometimes create the problem of the curse of dimensionality. If the dimensionality of the feature space is high then the model training may take more time than expected, it will increase the complexity of the model and chances of error. This somehow also helps us to achieve data compression as the features have been removed without much loss of data.

37. What is the difference between the content-based and collaborative filtering algorithms of recommendation systems?
In a content-based recommendation system, similarities in the content and services are evaluated, and then by using these similarity measures from past data we recommend products to the user. But on the other hand in collaborative filtering, we recommend content and services based on the preferences of similar users.

For example, if one user has taken A and B services in past and a new user has taken service A then service A will be recommended to him based on the other user's preferences.

38. How you would assess the goodness-of-fit for a linear regression model? Which metrics would you consider most important and why?
To evaluate the performance of a linear regression model, important key metrics are: R-squared, Adjusted R-squared, RMSE, and F-Statistics. R-squared is particularly important as it reflects the proportion of variance in the dependent variable that can be explained by the independent variables, providing a measure of how well our model fits the data. However, Adjusted R-squared also plays a crucial role, especially when comparing models with different numbers of predictors. It adjusts for the complexity of the model, helping to prevent overfitting and ensuring the robustness of our findings.

To learn more about regression metrics, check out: Regression Metrics

39. What is the null hypothesis in linear regression problem?
In linear regression, the null hypothesis id that there is no relationship between the independent variable(s) and the dependent variable. This is formally represented as 
H
0
:
β
1
=
0
H 
0
​
 :β 
1
​
 =0, where 
β
1
β 
1
​
 ​ is the coefficient of the independent variable.

Essentially, the null hypothesis suggests that the predictor variable does not contribute to predicting the outcome. For instance, if the null hypothesis states that the slope of the regression line is zero, then a student's score in an English class would not be a useful predictor of their overall grade-point average.

The alternative hypothesis, denoted as 
H
1
:
β
1
≠
0
H 
1
​
 :β 
1
​
 

=0, proposes that changes in the independent variable are indeed associated with changes in the dependent variable, indicating a meaningful relationship.

40. Can SVMs be used for both classification and regression tasks?
Yes, Support Vector Machines (SVMs) can be used for both classification and regression. For classification, SVMs work by finding a hyperplane that separates different classes in the data with the largest gap possible.

For regression, which involves predicting a continuous number, SVMs are adapted into a version called Support Vector Regression (SVR). SVR tries to fit as many data points as possible within a certain range of the predicted line, allowing some errors but penalizing those that are too large. This makes it useful for predicting values in situations where the data shows complex patterns.

To learn how to implement Support Vector Regression, you can refer to: Support Vector Regression (SVR) using Linear and Non-Linear Kernels in Scikit Learn

41. Explain the concept of weighting in KNN? What are the different ways to assign weights, and how do they affect the model's predictions?
Weighting in KNN assigns different levels of importance to the neighbors based on their distance from the query point, influencing how each neighbor affects the model's predictions.

The weights can be assigned using:

Uniform Weighting: All neighbors have equal weight regardless of their distance.
Distance Weighting: Weights are inversely proportional to the distance, giving closer neighbors more influence.
User-defined Weights: Weights are assigned based on domain knowledge or specific data characteristics.
Effect on Model's Prediction:

Uniform Weighting: Simple but may not perform well with noisy data or varied distances.
Distance Weighting: Improves accuracy by emphasizing closer neighbors, useful for irregular class boundaries but sensitive to anomalies.
User-defined Weights: Optimizes performance when specific insights about the dataset are applied, though less generalizable.
42. What are the assumptions behind the K-means algorithm? How do these assumptions affect the results?
The assumptions of K-Means algorithm include:

Cluster Shape: Assumes clusters are spherical and of similar size, affecting how well it handles non-spherical groups.
Scale of Features: Assumes features are on similar scales; different ranges can distort the distance calculation.
Clusters are Balanced: Assumes clusters have a roughly equal number of observations, which can bias results against smaller clusters.
Similar Density: Assumes all clusters have similar density, impacting the algorithm's effectiveness with clusters of varying densities.
If these assumptions are not met, the model will perform poorly making difficult to process and select clustering techniques that align with the data characteristics.

Check out the article: K Means Clustering Assumptions

43. Can you explain the concept of convergence in K-means? What conditions must be met for K-means to converge?
Convergence in K-means occurs when the cluster centroids stabilize, and the assignment of data points to clusters no longer changes. This happens when the algorithm has minimized the sum of squared distances between points and their corresponding centroids.

Conditions for K-means to Converge:

Proper Initialization: The initial placement of centroids significantly impacts convergence. Techniques like k-means++ help ensure a better start.
Data Characteristics: The algorithm converges more effectively if the data naturally clusters into well-separated groups. Overlapping or complex cluster shapes can hinder convergence.
Correct Number of Clusters (k): Choosing the right number of clusters is critical; too many or too few can lead to slow convergence or convergence to suboptimal solutions.
Algorithm Parameters: Setting a maximum number of iterations and a small tolerance for centroid change helps prevent infinite loops and determines when the algorithm should stop if centroids move minimally between iterations.
44. What is the significance of tree pruning in XGBoost? How does it affect the model?
Tree pruning in XGBoost is used to reduce model complexity and prevent overfitting. XGBoost implements a "pruning-as-you-grow" strategy where it starts by growing a full tree up to a maximum depth, then prunes back the branches that contribute minimal gains in terms of loss reduction. This is guided by the gamma parameter, which sets a minimum loss reduction required to make further partitions on a leaf node.

Effect on the Model:

Reduces Overfitting: By trimming unnecessary branches, pruning helps in creating simpler models that generalize better to unseen data, reducing the likelihood of overfitting.
Improves Performance: Pruning helps in removing splits that have little impact, which can enhance the model's performance by focusing on more significant attributes.
Optimizes Computational Efficiency: It decreases the complexity of the final model, which can lead to faster training and prediction times as there are fewer nodes to traverse during decision making.
45. How does Random Forest ensure diversity among the trees in the model?
Random Forest ensures diversity among the trees in its ensemble through two main mechanisms:

Bootstrap Aggregating (Bagging): Each tree in a Random Forest is trained on a different bootstrap sample, a random subset of the data. This sampling with replacement means that each tree sees different portions of the data, leading to variations in their learning and decision-making processes.
Feature Randomness: When splitting a node during the construction of the tree, Random Forest randomly selects a subset of features instead of using all available features. This variation in the feature set ensures that trees do not follow the same paths or use the same splits, thereby increasing the diversity among the trees.
The diversity among trees reduces the variance of the model without significantly increasing the bias.

46. What is the concept of information gain in decision trees? How does it guide the creation of the tree structure?
Information gain is a measure used in decision trees to select the best feature that splits the data into the most informative subsets. It is calculated based on the reduction in entropy or impurity after a dataset is split on an attribute. Entropy is a measure of the randomness or uncertainty in the data set, and information gain quantifies how much splitting on a particular attribute reduces that randomness.

47. How does the independence assumption affect the accuracy of a Naive Bayes classifier?
Naive Bayes classifier operates under the assumption that all features in the dataset are independent of each other given the class label. This assumption simplifies the computation of the classifier's probability model, as it allows the conditional probability of the class given multiple features to be calculated as the product of the individual probabilities for each feature.

Affect of accuracy on a Naive Bayes classifier:

Strengths in High-Dimensional Data: In practice, the independence assumption can sometimes lead to good performance, especially in high-dimensional settings like text classification, despite the interdependencies among features. This is because the errors in probability estimates may cancel out across the large number of features.
Limitations Due to Feature Dependency: The accuracy of Naive Bayes can be adversely affected when features are not independent, particularly if the dependencies between features are strong and critical to predicting the class. The model may underperform in such scenarios because it fails to capture the interactions between features.
Generalization Capability: The simplistic nature of the independence assumption often allows Naive Bayes to perform well on smaller datasets or in cases where data for training is limited, as it does not require as complex a model as other classifiers.
48. Why does PCA maximize the variance in the data?
PCA aims to maximize the variance because variance represents how much information is spread out in a given direction. The higher the variance along a direction, the more information that direction holds about the data. By focusing on the directions of highest variance, PCA helps us:

Preserve information while reducing the dimensionality.
Simplify the data by eliminating less important features (those with low variance)
49. How do you evaluate the effectiveness of a machine learning model in an imbalanced dataset scenario? What metrics would you use instead of accuracy?
We can use Precision, Recall, F1 score and ROC-AUC to evaluate the effectiveness of machine learning model in imbalanced dataset scenario. The best metric is F1 score as it combines both precision and recall into single metric that is important in imbalanced datasets where a high number of true negatives can skew accuracy. By focusing on both false positives and false negatives, the F1-score ensures that both the positive class detection and false positives are accounted for.

If the cost of false positives (Type I errors) and false negatives (Type II errors) is similar, F1-Score strikes a good balance.
It is especially useful when you need to prioritize performance in detecting the minority class (positive class).
However, if you are more concerned about false positives or false negatives specifically, you may opt for:

Precision (if false positives are more costly) or
Recall (if false negatives are more costly).
50. How the One-Class SVM algorithm works for anomaly detection?
One-Class SVM is an unsupervised anomaly detection algorithm. It is often used when only normal data is available. The model learns a decision boundary around normal data points using a kernel, typically an RBF, to map the data into a higher-dimensional space. The algorithm identifies support vectors—data points closest to the boundary—and any new data point outside this boundary is flagged as an anomaly. Key parameters like 'nu' control the fraction of outliers allowed, while the kernel defines the boundary shape.

51. Explain the concept of "concept drift" in anomaly detection.
Concept driftrefers to the change in the underlying distribution or patterns in the data over time, which can make previously normal data points appear as anomalies. In anomaly detection, this is particularly challenging because a model trained on old data may not recognize new, evolving patterns as part of the normal data distribution. Concept drift can occur suddenly or gradually and needs to be monitored closely. To address this, models can be adapted through periodic retraining with new data or by using adaptive anomaly detection algorithms.

. Explain Diffusion Model architecture.
Diffusion models are generative models that iteratively transform simple noise distributions into complex data distributions.

Key components include:

Forward Process: Gradually adds noise to the data over several steps, leading to a noise distribution.
Reverse Process: Learns to reverse the noise addition process, progressively denoising the data to generate new samples.
The model is trained to predict the noise added at each step, enabling it to generate realistic data by reversing the diffusion process. Diffusion models have shown impressive results in image and audio generation tasks.

12. Explain the different agents in Artificial Intelligence.
Artificial Intelligence (AI) agents can be classified into several types based on their capabilities and the complexity of their decision-making processes:

Simple Reflex Agents: These agents act solely on the current percept, ignoring the rest of the percept history. They use condition-action rules to decide actions.
Model-Based Reflex Agents: These agents maintain an internal state that depends on the percept history and reflects some of the unobserved aspects of the current state.
Goal-Based Agents: These agents act to achieve specific goals, considering future consequences of their actions. They use search and planning to decide the best actions.
Utility-Based Agents: These agents choose actions based on a utility function that evaluates the desirability of different states. They aim to maximize their overall happiness or satisfaction.
Learning Agents: These agents can learn from their experiences and adapt their behavior. They consist of four main components: the learning element, performance element, critic, and problem generator.
13. What is a rational agent, and what is rationality?
Rational Agent: A rational agent is an agent that acts to achieve the best possible outcome or, when there is uncertainty, the best expected outcome. Rationality is about making the right decisions based on the current information and expected future benefits.
Rationality: Rationality refers to the quality of being based on or in accordance with reason or logic. In AI, an agent is considered rational if it consistently performs actions that maximize its performance measure, given its percept sequence and built-in knowledge.
14. How do coordination mechanisms impact agents in multiagent environments?
Coordination mechanisms are essential in multiagent environments to manage the interactions between agents. These mechanisms ensure that agents work together harmoniously, avoiding conflicts and enhancing collective performance. Common coordination mechanisms include:

Communication Protocols: Methods for agents to exchange information and negotiate actions.
Distributed Planning: Techniques that enable agents to plan their actions considering others' plans.
Market-Based Mechanisms: Economic models where agents bid for tasks or resources.
Coordination Algorithms: Algorithms designed to optimize the joint performance of all agents.
15. How does an agent formulate a problem?
An agent formulates a problem by defining the following components:

Initial State: The starting point or condition of the problem.
Actions: The set of possible actions the agent can take.
Transition Model: The description of what each action does, i.e., the outcome of applying an action to a state.
Goal State: The desired outcome or condition the agent aims to achieve.
Path Cost: A function that assigns a cost to each path or sequence of actions.
To learn more refer to: How does an agent formulate a problem?

16. What are the different types of search algorithms used in problem-solving?
Search algorithms are categorized into:

Uninformed Search Algorithms: These algorithms have no additional information about states beyond the problem definition. Examples include:
Breadth-First Search (BFS)
Depth-First Search (DFS)
Uniform Cost Search
Iterative Deepening Search
Informed Search Algorithms: These algorithms use heuristics to estimate the cost of reaching the goal from a given state. Examples include:
A* Search
Greedy Best-First Search
Beam Search
To learn more refer to: Search Algorithms in AI

17. What is the difference between informed and uninformed search AI algorithms?
The key difference between informed and uninformed search AI algorithms is as follows:

Uninformed Search Algorithms: These algorithms do not have any domain-specific knowledge beyond the problem definition. They search through the problem space blindly, exploring all possible states.
Informed Search Algorithms: These algorithms use heuristics, which provide additional information to guide the search more efficiently towards the goal. They can often find solutions faster and more efficiently than uninformed search algorithms.
18. What is the role of heuristics in local search algorithms?
Heuristics play a critical role in local search algorithms by providing a way to estimate how close a given state is to the goal state. This guidance helps the algorithm to make more informed decisions about which neighboring state to explore next, improving the efficiency and effectiveness of the search process.

19. What is Fuzzy Logic?
Fuzzy Logic is a type of logic that deals with "in-between" values instead of just "true" or "false." It’s like saying something is "partly true" or "kind of false," making it useful for handling uncertain or vague information, like deciding if it’s "warm" or "cold" when the temperature is neither fully hot nor fully cold.

20. What Is Game Theory?
Game Theory is a branch of mathematics and economics that studies strategic interactions between rational decision-makers. It provides tools to analyze situations where multiple agents make decisions that affect each other's outcomes. Game theory concepts are used to model and predict behaviors in competitive and cooperative scenarios.

21. What is Reinforcement Learning, and explain the key components of a Reinforcement Learning problem?
Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. The key components of an RL problem include:

Agent: The learner or decision-maker.
Environment: The external system with which the agent interacts.
State: A representation of the current situation of the agent.
Actions: The set of all possible moves the agent can make.
Reward: A scalar feedback signal indicating the success of an action.
Policy: A strategy that defines the agent's behavior by mapping states to actions.
Value Function: A function that estimates the expected cumulative reward for each state or state-action pair.
22. What strategies do you use to optimize AI models for performance in production?
Model Quantization: Reducing the precision of model weights (e.g., from 32-bit to 8-bit) to decrease size and improve inference speed.
Pruning: Removing less important parts of the model (e.g., redundant neurons or weights) to reduce complexity and size.
Hardware Acceleration: Utilizing GPUs, TPUs, or specialized AI chips to speed up computations.
Model Caching: Storing frequently used results to avoid repeated computations.
Monitoring and Retraining: Continuously monitoring model performance and retraining if performance declines due to data drift.
23. What are the different components of an expert system?
An expert system consists of several key components:

Knowledge Base: A repository of domain-specific knowledge, including facts and rules.
Inference Engine: The component that applies logical rules to the knowledge base to deduce new information and make decisions.
User Interface: The means through which users interact with the expert system.
Explanation Facility: Provides explanations of the reasoning process and the conclusions reached.
Knowledge Acquisition Module: Tools and techniques used to gather and update the knowledge base.
24. What are embeddings in machine learning?
Embeddings in machine learning are representations of objects, such as words or images, in a continuous vector space. These vectors capture semantic relationships and similarities between the objects. For example, in natural language processing, word embeddings map words to high-dimensional vectors that reflect their meanings and relationships based on their usage in large text corpora.

25. How does reward maximization work in Reinforcement Learning?
In Reinforcement Learning, reward maximization involves the agent taking actions that maximize its cumulative reward over time. The agent uses a policy to select actions based on the expected future rewards. The learning process involves updating the value function and policy to improve the expected rewards, typically using algorithms like Q-learning, SARSA, or policy gradient methods.

26. What is gradient descent in machine learning?
Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models. It iteratively adjusts the model parameters in the opposite direction of the gradient of the loss function with respect to the parameters. The step size is determined by the learning rate.

Gradient descent variants include:

Batch Gradient Descent: Uses the entire dataset to compute the gradient.
Stochastic Gradient Descent (SGD): Uses one sample at a time to compute the gradient.
Mini-Batch Gradient Descent: Uses a small batch of samples to compute the gradient.
27. What is the difference between genetic algorithms and local search optimization algorithms?
The difference between genetic algorithms and local search optimization algorithms are as follows:

Genetic Algorithms (GAs): These are population-based optimization algorithms inspired by the process of natural selection. They use operators like selection, crossover, and mutation to evolve a population of solutions over generations.
Local Search Algorithms: These algorithms explore the solution space by moving from one solution to a neighboring solution, typically focusing on improving a single solution at a time. Examples include hill climbing and simulated annealing.
28. Discuss the concept of local optima and how it influences the effectiveness of local search algorithms.
Local optima are solutions that are better than all their neighboring solutions but may not be the best overall solution (global optimum). Local search algorithms can get stuck in local optima, leading to suboptimal solutions. Techniques like simulated annealing and genetic algorithms are used to mitigate this issue by allowing occasional moves to worse solutions, helping the search escape local optima.

29. What is the difference between propositional logic and first-order logic, and how are they used in knowledge representation?
The Key difference between propositional logic and first-order logic are as follows:
Propositional Logic: Deals with propositions that can either be true or false. It uses logical connectives like AND, OR, and NOT to build complex statements.
First-Order Logic (FOL): Extends propositional logic by including quantifiers and predicates that can express relationships between objects. FOL is more expressive and can represent more complex statements about the world.
In knowledge representation, propositional logic is used for simple, straightforward scenarios, while first-order logic is used for more complex representations involving objects and their relationships.

30. Discuss the trade-offs between exploration and exploitation in local search algorithms.
In local search algorithms, exploration refers to the process of trying out new, possibly suboptimal solutions to discover better ones. Exploitation involves refining current solutions to improve them. The trade-off involves balancing the two: too much exploitation can lead to getting stuck in local optima, while too much exploration can waste resources and time. Effective algorithms balance both to find optimal solutions efficiently.

31. What are the differences between the hill climbing and simulated annealing algorithms?
The key differences between the hill climbing and simulated annealing algorithms are as follows:

Hill Climbing: A local search algorithm that continuously moves towards better neighboring solutions. It can easily get stuck in local optima because it only considers immediate improvements.
Simulated Annealing: A probabilistic algorithm that explores the solution space more broadly. It uses a temperature parameter to occasionally accept worse solutions, helping to escape local optima and potentially find the global optimum.
32. Explain the concept of a knowledge base in AI and discuss its role in intelligent systems.
A knowledge base in AI is a centralized repository of information, including facts, rules, and relationships about a particular domain. It enables intelligent systems to reason, make decisions, and solve problems by applying inference mechanisms to the stored knowledge. The knowledge base is crucial for expert systems, decision support systems, and other AI applications that rely on domain-specific information.

33. How do knowledge representation and reasoning techniques support intelligent systems?
Knowledge representation and reasoning techniques provide the means to encode information about the world and manipulate it to derive new information, make decisions, and solve problems. They support intelligent systems by enabling:

Symbolic Representation: Capturing complex relationships and entities in a structured form.
Logical Reasoning: Applying rules and logic to infer new knowledge and make decisions.
Semantic Understanding: Interpreting the meaning of information to provide contextually relevant responses.
34. State the differences between model-free and model-based Reinforcement Learning.
Model-Free Reinforcement Learning: The agent learns to make decisions based solely on the rewards received from the environment. It does not build a model of the environment's dynamics. Examples include Q-learning and SARSA.
Model-Based Reinforcement Learning: The agent builds a model of the environment's dynamics and uses it to predict future states and rewards. It allows for planning and more efficient learning. Examples include Dyna-Q and Monte Carlo Tree Search.
35. What is Generative AI? What are some popular Generative AI architectures?
Generative AI refers to models that can generate new, original content based on the data they were trained on. These models can create text, images, music, and other media. Popular generative AI architectures include:

Generative Adversarial Networks (GANs): Consist of a generator and a discriminator that compete to produce realistic data.
Variational Autoencoders (VAEs): Use probabilistic methods to generate new data points similar to the training data.
Transformer Models: Such as GPT-3 and DALL-E, which are capable of generating coherent text and images based on given prompts.
36. What are the key differences between zero-sum and non-zero-sum games?
Zero-Sum Games: In these games, one person’s win means another person’s loss. The total amount of gains and losses always adds up to zero. For example, in chess or poker, if one player wins, the other loses by the same amount.
Non-Zero-Sum Games: In these games, everyone’s outcome is not connected like a seesaw. All players can either win or lose together, depending on how they play. For example, in trade negotiations, both sides can benefit if they cooperate, or both can lose if they don’t agree. Similarly, in the Prisoner’s Dilemma, players can choose to help each other and gain, or betray and lose.
37. What is the concept of constraint satisfaction problem (CSP)?
A Constraint Satisfaction Problem (CSP) is a mathematical problem defined by a set of variables, a domain of possible values for each variable, and a set of constraints specifying allowable combinations of values. The goal is to find a complete assignment of values to variables that satisfies all constraints. CSPs are used in scheduling, planning, and resource allocation problems.

38. What do you mean by inference in AI?
Inference in AI refers to the process of deriving new knowledge or conclusions from existing information using logical reasoning. It involves applying rules and algorithms to known data to infer new facts, make predictions, or solve problems. Inference is a key component of expert systems, decision-making processes, and machine learning models.

39. What are the advantages and disadvantages of forward chaining and backward chaining inference in rule-based systems?
The advantages and disadvantages of forward chaining and backward chaining inference in rule-based systems are as follows:

Forward Chaining:

Advantages: Efficient for problems where all data is available from the start. Suitable for data-driven scenarios.
Disadvantages: Can generate a large number of intermediate facts, leading to inefficiency.
Backward Chaining:

Advantages: Goal-directed, focusing on relevant data and rules. Efficient for goal-driven scenarios.
Disadvantages: Can be inefficient if the search space is large or if there are many possible rules to apply.
40. How do Bayesian networks model probabilistic relationships between variables?
Bayesian networks model probabilistic relationships using a directed acyclic graph (DAG) where nodes represent random variables, and edges represent conditional dependencies. Each node has a probability distribution that quantifies the effect of its parents. Bayesian networks allow for efficient representation and computation of joint probabilities, enabling reasoning under uncertainty and probabilistic inference.

42. Discuss the concept of alpha-beta pruning in adversarial search algorithms.
Alpha-beta pruning is an optimization technique for the minimax algorithm in adversarial search (e.g., game playing). It eliminates branches in the search tree that cannot affect the final decision, thus reducing the number of nodes evaluated.

Alpha (α): The best value that the maximizer currently can guarantee at that level or above.
Beta (β): The best value that the minimizer currently can guarantee at that level or below.
During the search, branches are pruned if:

Maximizer: Finds a move that is better than the current beta value (beta cut-off).
Minimizer: Finds a move that is worse than the current alpha value (alpha cut-off).
43. Explain the concept of backtracking search and its role in finding solutions to CSPs.
Backtracking search is a depth-first search algorithm for solving Constraint Satisfaction Problems (CSPs). It incrementally builds candidates for the solutions and abandons a candidate ("backtracks") as soon as it determines that the candidate cannot possibly be completed to a valid solution.

Role in CSPs: Backtracking is used to systematically explore the possible assignments of values to variables while ensuring that the constraints are satisfied. If a partial assignment violates a constraint, the algorithm backtracks to the previous step to try a different value.
44. Explain the role of the minimax algorithm in adversarial search for optimal decision-making.
The minimax algorithm is used in decision-making for two-player games, where one player (maximizer) tries to maximize their score while the other player (minimizer) tries to minimize it. The algorithm evaluates the game tree, considering all possible moves:

Maximizer's Turn: Chooses the move with the highest score.
Minimizer's Turn: Chooses the move with the lowest score.
The goal is to find the optimal strategy by assuming that both players play optimally. The minimax algorithm recursively evaluates the game tree until the terminal nodes, assigning values to each move and choosing the best move for the current player.

45. Explain the A* algorithm and its heuristic search strategy.
A* algorithm is used to find the shortest path in a graph. It uses both the actual cost to reach a node (g(n)) and a heuristic estimate of the cost to reach the goal from that node (h(n)).

Formula: f(n)=g(n)+h(n)
Heuristic: A function that estimates the cost of reaching the goal from the current node. It guides the search process by prioritizing nodes with the lowest estimated total cost (f(n)).
A* efficiently finds the shortest path by balancing exploration of new nodes (guided by the heuristic) and exploitation of known paths (guided by the actual cost).

46. Explain the concept of the Markov Decision Process (MDP) and its relevance to Reinforcement Learning.
Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in environments with stochastic outcomes. It consists of:

States (S): The possible situations in the environment.
Actions (A): The set of all possible actions the agent can take.
Transition Model (P): The probability of moving from one state to another, given an action.
Rewards (R): The immediate reward received after transitioning from one state to another.
Policy (π): A strategy that specifies the action to take in each state.
In Reinforcement Learning, MDPs provide the foundation for defining the environment, modeling the agent's interactions, and optimizing the policy to maximize cumulative rewards.

47. Explain the Hidden Markov Model.
Hidden Markov Model (HMM) is a statistical model used to represent systems that have hidden (unobservable) states. It consists of:

States: A set of hidden states the system can be in.
Observations: The observed data, which are probabilistically related to the hidden states.
Transition Probabilities: The probabilities of transitioning between hidden states.
Emission Probabilities: The probabilities of observing a certain output given a hidden state.
Initial Probabilities: The probabilities of starting in each hidden state.
HMMs are used in various applications like speech recognition, natural language processing, and bioinformatics to model sequences with underlying hidden patterns.

48. Explain the concept of autoencoders in deep learning.
Autoencoders are a type of neural network used for unsupervised learning, specifically for dimensionality reduction and feature learning. They consist of two main parts:

Encoder: Maps the input data to a lower-dimensional latent space.
Decoder: Reconstructs the input data from the latent representation.
The goal is to train the network so that the output closely matches the input, forcing the model to learn efficient representations of the data. Variants like denoising autoencoders and variational autoencoders add additional constraints or probabilistic elements to improve robustness and generative capabilities.

49. Explain Generative Adversarial Networks (GANs) architecture.
Generative Adversarial Networks (GANs) consist of two neural networks, a generator and a discriminator, that compete in a zero-sum game:

Generator: Creates fake data resembling the real data.
Discriminator: Distinguishes between real and fake data.
A GAN works like a game between a fake image creator (generator) and a fake image detector (discriminator). The creator tries to make fake images look real, while the detector tries to tell real images from fake ones. They improve together, and over time, the creator becomes so good that the detector can't tell the difference between real and fake images.

50. Explain Transformer Model architecture.
Transformer model was introduced in the paper "Attention is All You Need," revolutionized natural language processing with its attention mechanisms and parallel processing capabilities.

Key components include:

Self-Attention Mechanism: Allows each input token to attend to all other tokens, capturing long-range dependencies.
Positional Encoding: Adds information about the position of tokens in the sequence.
Encoder-Decoder Structure:
Encoder: Consists of multiple layers, each with self-attention and feed-forward neural networks.
Decoder: Similar to the encoder but includes an additional attention layer to attend to the encoder's output.
Transformers enable efficient training on large datasets and achieve state-of-the-art performance in tasks like machine translation and text generation.

What is a transformer model?
The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting.

The transformer architecture was first described in the seminal 2017 paper "Attention is All You Need" by Vaswani and others, which is now considered a watershed moment in deep learning.

Originally introduced as an evolution of the recurrent neural network (RNN)-based sequence-to-sequence models used for machine translation, transformer-based models have since attained cutting-edge advancements across nearly every machine learning (ML) discipline.

Despite their versatility, transformer models are still most commonly discussed in the context of natural language processing (NLP) use cases, such as chatbots, text generation, summarization, question answering and sentiment analysis.

The BERT (or Bidirectional Encoder Representations from Transformers) encoder-decoder model, introduced by Google in 2019, was a major landmark in the establishment of transformers and remains the basis of most modern word embedding applications, from modern vector databases to Google search.

Autoregressive decoder-only LLMs, such as the GPT-3 (short for Generative Pre-trained Transformer) model that powered the launch of OpenAI’s ChatGPT, catalyzed the modern era of generative AI (gen AI).

The ability of transformer models to intricately discern how each part of a data sequence influences and correlates with the others also lends them many multimodal uses.

For instance, vision transformers (ViTs) often exceed the performance of convolutional neural networks (CNNs) on image segmentation, object detection and related tasks. The transformer architecture also powers many diffusion models used for image generation, multimodal text-to-speech (TTS) and vision language models (VLMs).

3D design of balls rolling on a track
The latest AI News + Insights 
Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. 

Subscribe today
Why are transformer models important?
The central feature of transformer models is their self-attention mechanism, from which transformer models derive their impressive ability to detect the relationships (or dependencies) between each part of an input sequence. Unlike the RNN and CNN architectures that preceded it, the transformer architecture uses only attention layers and standard feedforward layers.

The benefits of self-attention, and specifically the multi-head attention technique that transformer models employ to compute it, are what enable transformers to exceed the performance of the RNNs and CNNs that had previously been state-of-the-art.

Before the introduction of transformer models, most NLP tasks relied on recurrent neural networks (RNNs). The way RNNs process sequential data is inherently serialized: they ingest the elements of an input sequence one at a time and in a specific order.

This hinders the ability of RNNs to capture long-range dependencies, meaning RNNs can only process short text sequences effectively.
This deficiency was somewhat addressed by the introduction of long short term memory networks (LSTMs), but remains a fundamental shortcoming of RNNs.

Attention mechanisms, conversely, can examine an entire sequence simultaneously and make decisions about how and when to focus on specific time steps of that sequence.

In addition to significantly improving the ability to understand long-range dependencies, this quality of transformers also allows for parallelization: the ability to perform many computational steps at once, rather than in a serialized manner.

Being well-suited to parallelism enables transformer models to take full advantage of the power and speed offered by GPUs during both training and inference. This possibility, in turn, unlocked the opportunity to train transformer models on unprecedentedly massive datasets through self-supervised learning.

Especially for visual data, transformers also offer some advantages over convolutional neural networks. CNNs are inherently local, using convolutions to process smaller subsets of input data one piece at a time.

Therefore, CNNs also struggle to discern long-range dependencies, such as correlations between words (in text) or pixels (in images) that aren’t neighboring one another. Attention mechanisms don’t have this limitation.

Mixture of Experts | 4 July, episode 62

Anthropic’s Project Vend, computer science education and AI prompts in papers
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.

Watch the latest podcast episodes 
What is self-attention?
Understanding the mathematical concept of attention, and more specifically self-attention, is essential to understanding the success of transformer models in so many fields. Attention mechanisms are, in essence, algorithms designed to determine which parts of a data sequence an AI model should “pay attention to” at any particular moment.

Consider a language model interpreting the English text " on Friday, the judge issued a sentence. "

The preceding word “ the ” suggests that “ judge ” is acting as a noun—as in, a person presiding over a legal trial—rather than a verb meaning to appraise or form an opinion.
That context for the word “ judge ” suggests that “ sentence ” probably refers to a legal penalty, rather than a grammatical “sentence.”
The word “ issued ” further implies that “ sentence ” refers to the legal concept, not the grammatical concept.
Therefore, when interpreting the word “ sentence ,” the model should pay close attention to “ judge ” and “ issued. ” It should also pay some attention to the word “ the .” It can more or less ignore the other words.
How does self-attention work?
Broadly speaking, a transformer model’s attention layers assess and use the specific context of each part of a data sequence in 4 steps:

The model “reads” raw data sequences and converts them into vector embeddings, in which each element in the sequence is represented by its own feature vector(s) that numerically reflect qualities such as semantic meaning.

The model determines similarities, correlations and other dependencies (or lack thereof) between each vector and each other vector. In most transformer models, the relative importance of one vector to another is determined by computing the dot product between each vector. If the vectors are well aligned, multiplying them together will yield a large value. If they’re not aligned, their dot product will be small or negative.

These “alignment scores” are converted into attention weights. This is achieved by using alignment scores as inputs to a softmax activation function, which normalizes all values to a range between 0–1 such that they all add up to a total of 1. So for instance, assigning an attention weight of 0 between “Vector A” and “Vector B” means that Vector B should be ignored when making predictions about Vector A. Assigning Vector B an attention weight of 1 means that it should receive 100% of the model’s attention when making decisions about Vector A.

These attention weights are used to emphasize or deemphasize the influence of specific input elements at specific times. In other words, attention weights help transformer models focus on or ignore specific information at a specific moment.
Before training, a transformer model doesn’t yet “know” how to generate optimal vector embeddings and alignment scores. During training, the model makes predictions across millions of examples drawn from its training data, and a loss function quantifies the error of each prediction.

Through an iterative cycle of making predictions and then updating model weights through backpropagation and gradient descent, the model “learns” to generate vector embeddings, alignment scores and attention weights that lead to accurate outputs.

How do transformer models work?
Transformer models such as relational databases generate query, key and value vectors for each part of a data sequence, and use them to compute attention weights through a series of matrix multiplications.

Relational databases are designed to simplify the storage and retrieval of relevant data: they assign a unique identifier (“key”) to each piece of data, and each key is associated with a corresponding value. The “Attention is All You Need” paper applied that conceptual framework to processing the relationships between each token in a sequence of text.

The query vector represents the information a specific token is “seeking.” In other words, a token’s query vector is used to compute how other tokens might influence its meaning, conjugation or connotations in context.
The key vectors represent the information that each token contains. Alignment between query and key is used to compute attention weights that reflect how relevant they are in the context of that text sequence.
The value (or value vector) “returns” the information from each key vector, scaled by its respective attention weight. Contributions from keys that are strongly aligned with a query are weighted more heavily; contributions from keys that are not relevant to a query will be weighted closer to zero.
For an LLM, the model’s “database” is the vocabulary of tokens it has learned from the text samples in its training data. Its attention mechanism uses information from this “database” to understand the context of language.

Tokenization and input embeddings
Whereas characters—letters, numbers or punctuation marks—are the base unit we humans use to represent language, the smallest unit of language that AI models use is a token. Each token is assigned an ID number, and these ID numbers (rather than the words or even the tokens themselves) are the way LLMs navigate their vocabulary “database.” This tokenization of language significantly reduces the computational power needed to process text.

To generate query and key vectors to feed into the transformer’s attention layers, the model needs an initial, contextless vector embedding for each token. These initial token embeddings can be either learned during training or taken from a pretrained word embedding model.

Positional encoding
The order and position of words can significantly impact their semantic meanings. Whereas the serialized nature of RNNs inherently preserves information about the position of each token, transformer models must explicitly add positional information for the attention mechanism to consider.

With positional encoding, the model adds a vector of values to each token’s embedding, derived from its relative position, before the input enters the attention mechanism. The nearer the 2 tokens are, the more similar their positional vectors will be and therefore, the more their alignment score will increase from adding positional information. The model thereby learns to pay greater attention to nearby tokens.

Generating query, key and value vectors
When positional information has been added, each updated token embedding is used to generate three new vectors. These query, key and value vectors are generated by passing the original token embeddings through each of three parallel feedforward neural network layers that precede the first attention layer. Each parallel subset of that linear layer has a unique matrix of weights, learned through self-supervised pretraining on a massive dataset of text.

The embeddings are multiplied by the weight matrix WQ to yield the query vectors (Q), which have dk dimensions
The embeddings are multiplied by the weight matrix WK to yield the key vector (K), also with dimensions dk 
The embeddings are multiplied by the weight matrix WV to yield the value vectors (V), with dimensions dv
A diagram of a transformer model's attention mechanism
A simplified diagram of the transformer's attention mechanism: the original vector embeddings for the tokens of an input sentence are multiplied by W, K and V weight matrices to yield their respective W, K and V vectors.

Computing self-attention
The transformer’s attention mechanism’s primary function is to assign accurate attention weights to the pairings of each token’s query vector with the key vectors of all the other tokens in the sequence. When achieved, you can think of each token 
x
  as now having a corresponding vector of attention weights, in which each element of that vector represents the extent to which some other token should influence it.

Each other token’s value vector is now multiplied by its respective attention weight.
These attention-weighted value vectors are all summed together. The resulting vector represents the aggregated contextual information being provided to token 
x
 by all of the other tokens in the sequence.
Finally, the resulting vector of attention-weighted changes from each token is added to the token 
x
’s original, post-positional encoding vector embedding.
In essence, 
x
's vector embedding has been updated to better reflect the context provided by the other tokens in the sequence.

Multi-head attention
To capture the many multifaceted ways tokens might relate to one another, transformer models implement multi-head attention across multiple attention blocks.

Before being fed into the first feedforward layer, each original input token embedding is split into h evenly sized subsets. Each piece of the embedding is fed into one of h parallel matrices of Q, K and V weights, each of which are called a query head, key head or value head. The vectors output by each of these parallel triplets of query, key and value heads are then fed into a corresponding subset of the next attention layer, called an attention head.

Diagram of multi-head attention
The simplified multi-head attention diagram made famous in "Attention is All You Need"

In the final layers of each attention block, the outputs of these h parallel circuits are eventually concatenated back together before being sent to the next feedforward layer. In practice, model training results in each circuit learning different weights that capture a separate aspect of semantic meanings.

Concatenation in multi-head attention
The outputs "Z" of each attention head are concatenated together. In this example, h = 8.

Residual connections and layer normalization
In some situations, passing along the contextually-updated embedding output by the attention block might result in an unacceptable loss of information from the original sequence.

To address this, transformer models often balance the contextual information provided by the attention mechanism with the original semantic meaning of each token. After the attention-updated subsets of the token embedding have all been concatenated back together, the updated vector is then added to the token’s original (position-encoded) vector embedding. The original token embedding is supplied by a residual connection between that layer and an earlier layer of the network.

The resulting vector is fed into another linear feedforward layer, where it’s normalized back to a constant size before being passed along to the next attention block. Together, these measures help preserve stability in training and help ensure that the text’s original meaning is not lost as the data moves deeper into the neural network.

Generating outputs
Eventually, the model has enough contextual information to inform its final outputs. The nature and function of the output layer will depend on the specific task the transformer model has been designed for.

In autoregressive LLMs, the final layer uses a softmax function to determine the probability that the next word will match each token in its vocabulary “database.” Depending on the specific sampling hyperparameters, the model uses those probabilities to determine the next token of the output sequence.

Transformer models in natural language processing (NLP)
Transformer models are most commonly associated with NLP, having originally been developed for machine translation use cases. Most notably, the transformer architecture gave rise to the large language models (LLMs) that catalyzed the advent of generative AI.

Most of the LLMs that the public is most familiar with, from closed source models such as OpenAI’s GPT series and Anthropic’s Claude models to open source models including Meta Llama or IBM® Granite®, are autoregressive decoder-only LLMs.

Autoregressive LLMs are designed for text generation, which also extends naturally to adjacent tasks such as summarization and question answering. They’re trained through self-supervised learning, in which the model is provided the first word of a text passage and tasked with iteratively predicting the next word until the end of the sequence.

Information provided by the self-attention mechanism enables the model to extract context from the input sequence and maintain the coherence and continuity of its output.

Encoder-decoder masked language models (MLMs), such as BERT and its many derivatives, represent the other main evolutionary branch of transformer-based LLMs. In training, an MLM is provided a text sample with some tokens masked—hidden—and tasked with completing the missing information.

While this training methodology is less effective for text generation, it helps MLMs excel at tasks requiring robust contextual information, such as translation, text classification and learning embeddings.

Transformer models in other fields
Though transformer models were originally designed for, and continue to be most prominently associated with natural language use cases, they can be used in nearly any situation involving sequential data. This has led to the development of transformer-based models in other fields, from fine-tuning LLMs into multimodal systems to dedicated time series forecasting models and ViTs for computer vision.

Some data modalities are more naturally suited to transformer-friendly sequential representation than others. Time series, audio and video data are inherently sequential, whereas image data is not. Despite this, ViTs and other attention-based models have achieved state-of-the-art results for many computer vision tasks, including image captioning, object detection, image segmentation and visual question answering.

To use transformer models for data not conventionally thought of as "sequential" requires a conceptual workaround to represent that data as a sequence. For instance, to use attention mechanisms to understand visual data, ViTs use patch embeddings to make image data interpretable as sequences.

First, an image is split into an array of patches. For instance, a 224x224 pixel image can be subdivided into 256 14x14 pixel patches, dramatically reducing the number of computational steps required to process the image.
Next, a linear projection layer maps each patch to a vector embedding.
Positional information is added to each of these patch embeddings, akin to the positional encoding described earlier in this article.
These patch embeddings can now essentially function as a sequence of token embeddings, allowing the image to be interpreted by an attention mechanism.


What is Retrieval-Augmented Generation (RAG) ?
Last Updated : 10 Feb, 2025
Retrieval-augmented generation (RAG) is an innovative approach in the field of natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to enhance the quality of generated text.

What-is-RAG_
Retrieval-Augmented Generation (RAG)
Why is Retrieval-Augmented Generation important?
In traditional LLMs, the model generates responses based solely on the data it was trained on, which may not include the most current information or specific details required for certain tasks. RAG addresses this limitation by incorporating a retrieval mechanism that allows the model to access external databases or documents in real-time.

This hybrid model aims to leverage the vast amounts of information available in large-scale databases or knowledge bases making it particularly effective for tasks that require accurate and contextually relevant information.

How does Retrieval-Augmented Generation work?
The system first searches external sources for relevant information based on the user’s query Instead of relying only on existing training data.

How-Rag-works
1. Creating External Data

External data refers to new information beyond the LLM’s original training dataset. It can come from various sources, such as APIs, databases, or document repositories, and may exist in different formats like text files or structured records. To make this data understandable to AI, it is first divided in chunks in case of massive datasets and converted into numerical representations (embeddings) using specialized models and then stored in a vector database. This creates a knowledge library that the AI system can reference during retrieval.

2. Retrieving Relevant Information

When a user submits a query, the system converts it into a vector representation and matches it against stored vectors in the database. This enables precise retrieval of the most relevant information. For example, if the Y.O.G.I Botis asked, "What are the key topics in the DSA course?", it would retrieve both the course syllabus and relevant study materials. This ensures the response is highly relevant and tailored to the user's learning needs.

3. Augmenting the LLM Prompt

Once the relevant data is retrieved, it is incorporated into the user’s input (prompt) using prompt engineering techniques. This enhances the model’s contextual understanding, allowing it to generate more detailed, factually accurate, and insightful responses.

4. Keeping External Data Updated

To ensure the system continues to provide reliable and up-to-date responses, external data must be refreshed periodically. This can be done through automated real-time updates or scheduled batch processing. Keeping vector embeddings updated allows the RAG system to always retrieve the most current and relevant information for generating responses.

What problems does RAG solve?
The retrieval-augmented generation (RAG) approach helps solve several challenges in natural language processing (NLP) and AI applications:

Factual Inaccuracies and Hallucinations: Traditional generative models can produce plausible but incorrect information. RAG reduces this risk by retrieving verified, external data to ground responses in factual knowledge.
Outdated Information: Static models rely on training data that may become obsolete. RAG dynamically retrieves up-to-date information, ensuring relevance and accuracy in real-time.
Contextual Relevance: Generative models often struggle with maintaining context in complex or multi-turn conversations. RAG retrieves relevant documents to enrich the context, improving coherence and relevance.
Domain-Specific Knowledge: Generic models may lack expertise in specialized fields. RAG integrates domain-specific external knowledge for tailored and precise responses.
Cost and Efficiency: Fine-tuning large models for specific tasks is expensive. RAG eliminates the need for retraining by dynamically retrieving relevant data, reducing costs and computational load.
Scalability Across Domains: RAG is adaptable to diverse industries, from healthcare to finance, without extensive retraining, making it highly scalable
Challenges and Future Directions
Despite its advantages, RAG faces several challenges:

Complexity: Combining retrieval and generation adds complexity to the model, requiring careful tuning and optimization to ensure both components work seamlessly together.
Latency: The retrieval step can introduce latency, making it challenging to deploy RAG models in real-time applications.
Quality of Retrieval: The overall performance of RAG heavily depends on the quality of the retrieved documents. Poor retrieval can lead to suboptimal generation, undermining the model’s effectiveness.
Bias and Fairness: Like other AI models, RAG can inherit biases present in the training data or retrieved documents, necessitating ongoing efforts to ensure fairness and mitigate biases.
RAG Applications with Examples
Here are some examples to illustrate the applications of RAG we discussed earlier:

1. Advanced Question-Answering System
Scenario: Imagine a customer support chatbot for an online store. A customer asks, "What is the return policy for a damaged item?"
RAG in Action: The chatbot retrieves the store's return policy document from its knowledge base. RAG then uses this information to generate a clear and concise answer like, "If your item is damaged upon arrival, you can return it free of charge within 30 days of purchase. Please visit our returns page for detailed instructions."
2. Content Creation and Summarization
Scenario: You're building a travel website and want to create a summary of the Great Barrier Reef.
RAG in Action: RAG can access and process vast amounts of information about the Great Barrier Reef from various sources. It can then provide a concise summary highlighting key points like its location, size, biodiversity, and conservation efforts.
3. Conversational Agents and Chatbots
Scenario: A virtual assistant for a financial institution. A user asks, "What are some factors to consider when choosing a retirement plan?"
RAG in Action: The virtual assistant retrieves relevant information about retirement plans and investment strategies. RAG then uses this knowledge to provide the user with personalized guidance based on their age, income, and risk tolerance.
4. Information Retrieval
Scenario: You're searching the web for information about the history of artificial intelligence (AI).
RAG in Action: A RAG-powered search engine can not only return relevant webpages but also generate informative snippets that summarize the content of each page. This allows you to quickly grasp the key points of each result without having to visit every single webpage.
5. Educational Tools and Resources
Scenario: An online learning platform for science courses. A student is studying about the human body and has a question about the function of the heart.
RAG in Action: The platform uses RAG to access relevant information about the heart's anatomy and function from the course materials. It then presents the student with an explanation, diagrams, and perhaps even links to video resources, all tailored to their specific learning needs.
Example Scenario: AI Chatbot for Medical Information
Imagine a scenario where a person is experiencing symptoms of an illness and seeks information from an AI chatbot. Traditionally, the AI would rely solely on its training data to respond, potentially leading to inaccurate or incomplete information. However, with the Retrieval-Augmented Generation (RAG) approach, the AI can provide more accurate and reliable answers by incorporating knowledge from trustworthy medical sources.

Step-by-Step Process of RAG in Action
Retrieval Stage: The RAG system accesses a vast medical knowledge base, including textbooks, research papers, and reputable health websites. It searches this database to find relevant information related to the queried medical condition's symptoms. Using advanced techniques, the system identifies and retrieves passages that contain useful information.
Generation Stage: With the retrieved knowledge, the RAG system generates a response that includes factual information about the symptoms of the medical condition. The generative model processes the retrieved passages along with the user query to craft a coherent and contextually relevant response. The response may include a list of common symptoms associated with the queried medical condition, along with additional context or explanations to help the user understand the information better.
In this example, RAG enhances the AI chatbot's ability to provide accurate and reliable information about medical symptoms by leveraging external knowledge sources. This approach improves the user experience and ensures that the information provided is trustworthy and up-to-date.

What are the available options for customizing a Large Language Model (LLM) with data, and which method—prompt engineering, RAG, fine-tuning, or pretraining—is considered the most effective?

When customizing a Large Language Model (LLM) with data, several options are available, each with its own advantages and use cases. The best method depends on your specific requirements and constraints. Here's a comparison of the options:

Prompt Engineering:
Description: Crafting specific prompts that guide the model to generate desired outputs.
Pros: Simple and quick to implement, no need for additional training.
Cons: Limited by the model's capabilities, may require trial and error to find effective prompts.
Retrieval-Augmented Generation (RAG):
Description: Augmenting the model with external knowledge sources during inference to improve the relevance and accuracy of responses.
Pros: Enhances the model's responses with real-time, relevant information, reducing reliance on static training data.
Cons: Requires access to and integration with external knowledge sources, which can be challenging.
Fine-tuning:
Description: Adapting the model to specific tasks or domains by training it on a small dataset of domain-specific examples.
Pros: Allows the model to learn domain-specific language and behaviors, potentially improving performance.
Cons: Requires domain-specific data and can be computationally expensive, especially for large models.
Pretraining:
Description: Training the model from scratch or on a large, general-purpose dataset to learn basic language understanding.
Pros: Provides a strong foundation for further customization and adaptation.
Cons: Requires a large amount of general-purpose data and computational resources.
Which Method is Best?
The best method depends on your specific requirements:

Use Prompt Engineering if you need a quick and simple solution for specific tasks or queries.
Use RAG if you need to enhance your model's responses with real-time, relevant information from external sources.
Use Fine-tuning if you have domain-specific data and want to improve the model's performance on specific tasks.
Use Pretraining if you need a strong foundation for further customization and adaptation.
 

What is a Large Language Model (LLM)
Last Updated : 22 Jan, 2025
Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing.

This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP).

What are Large Language Models(LLMs)?
A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Language Model.

Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.

There are many techniques that were tried to perform natural language-related tasks but the LLM is purely based on the deep learning methodologies. LLM (Large language model) models are highly efficient in capturing the complex entity relationships in the text at hand and can generate the text using the semantic and syntactic of that particular language in which we wish to do so.

Author generated image with the use of AI
If we talk about the size of the advancements in the GPT (Generative Pre-trained Transformer) model only then:

GPT-1 which was released in 2018 contains 117 million parameters having 985 million words.
GPT-2 which was released in 2019 contains 1.5 billion parameters.
GPT-3 which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.
GPT-4 model is released in the early 2023 and it is likely to contain trillions of parameters.
GPT-4 Turbo was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.
How do Large Language Models work?
Large Language Models (LLMs) operate on the principles of deep learning, leveraging neural network architectures to process and understand human languages.

These models, are trained on vast datasets using self-supervised learning techniques. The core of their functionality lies in the intricate patterns and relationships they learn from diverse language data during training. LLMs consist of multiple layers, including feedforward layers, embedding layers, and attention layers. They employ attention mechanisms, like self-attention, to weigh the importance of different tokens in a sequence, allowing the model to capture dependencies and relationships.

Architecture of LLM
Large Language Model's (LLM) architecture is determined by a number of factors, like the objective of the specific model design, the available computational resources, and the kind of language processing tasks that are to be carried out by the LLM. The general architecture of LLM consists of many layers such as the feed forward layers, embedding layers, attention layers. A text which is embedded inside is collaborated together to generate predictions.

Important components to influence Large Language Model architecture:

Model Size and Parameter Count
input representations
Self-Attention Mechanisms
Training Objectives
Computational Efficiency
Decoding and Output Generation
Transformer-Based LLM Model Architectures
Transformer-based models, which have revolutionized natural language processing tasks, typically follow a general architecture that includes the following components:

Transformer-Geeksforgeeks

Input Embeddings: The input text is tokenized into smaller units, such as words or sub-words, and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.
Positional Encoding: Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.
Encoder: Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.
Self-Attention Mechanism: Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.
Feed-Forward Neural Network: After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.
Decoder Layers: In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.
Multi-Head Attention: Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.
Layer Normalization: Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model's ability to generalize across different inputs.
Output Layers: The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.
It's important to keep in mind that the actual architecture of transformer-based models can change and be enhanced based on particular research and model creations. To fulfill different tasks and objectives, several models like GPT, BERT, and T5 may integrate more components or modifications.

Popular Large Language Models
Now let's look at some of the famous LLMs which has been developed and are up for inference.

GPT-3: GPT 3 is developed by OpenAI, stands for Generative Pre-trained Transformer 3. This model powers ChatGPT and is widely recognized for its ability to generate human-like text across a variety of applications.
BERT: It is created by Google, is commonly used for natural language processing tasks and generating text embeddings, which can also be utilized for training other models.
RoBERTa: RoBERTa is an advanced version of BERT, stands for Robustly Optimized BERT Pretraining Approach. Developed by Facebook AI Research, it enhances the performance of the transformer architecture.
BLOOM: It is the first multilingual LLM, designed collaboratively by multiple organizations and researchers. It follows an architecture similar to GPT-3, enabling diverse language-based tasks.
For implementation details, these models are available on open-source platforms like Hugging Face and OpenAI for Python-based applications.

Large Language Models Use Cases
Code Generation: LLMs can generate accurate code based on user instructions for specific tasks.
Debugging and Documentation: They assist in identifying code errors, suggesting fixes, and even automating project documentation.
Question Answering: Users can ask both casual and complex questions, receiving detailed, context-aware responses.
Language Translation and Correction: LLMs can translate text between over 50 languages and correct grammatical errors.
Prompt-Based Versatility: By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios.
Use cases of LLM are not limited to the above-mentioned one has to be just creative enough to write better prompts and you can make these models do a variety of tasks as they are trained to perform tasks on one-shot learning and zero-shot learning methodologies as well. Due to this only Prompt Engineering is a totally new and hot topic in academics for people who are looking forward to using ChatGPT-type models extensively.

Applications of Large Language Models
LLMs, such as GPT-3, have a wide range of applications across various domains. Few of them are:

Natural Language Understanding (NLU):
Large language models power advanced chatbots capable of engaging in natural conversations.
They can be used to create intelligent virtual assistants for tasks like scheduling, reminders, and information retrieval.
Content Generation:
Creating human-like text for various purposes, including content creation, creative writing, and storytelling.
Writing code snippets based on natural language descriptions or commands.
Language Translation: Large language models can aid in translating text between different languages with improved accuracy and fluency.
Text Summarization: Generating concise summaries of longer texts or articles.
Sentiment Analysis: Analyzing and understanding sentiments expressed in social media posts, reviews, and comments.
Difference Between NLP and LLM 
NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-

Automotive routine task
Improve search 
Search engine optimization
Analyzing and organizing large documents
Social Media Analytics.
while on the other hand, LLM is a Large Language Model, and is more specific to human- like text, providing content generation, and personalized recommendations. 

What are the Advantages of Large Language Models?
Large Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications:

LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.
LLMs efficiently handle vast amounts of data, making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.
LLMs can be fine-tuned on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.
LLMs enable the automation of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.
Challenges in Training of Large Language Models
High Costs: Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.
Time-Intensive: Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.
Data Challenges: Obtaining large text datasets is difficult, and concerns about the legality of data scraping for commercial purposes have arisen.
Environmental Impact: Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns.
Conclusion
Due to the challenges faced in training LLM transfer learning is promoted heavily to get rid of all of the challenges discussed above. LLM has the capability to bring revolution in the AI-powered application but the advancements in this field seem a bit difficult because just increasing the size of the model may increase its performance but after a particular time a saturation in the performance will come and the challenges to handle these models will be bigger than the performance boost achieved by further increasing the size of the models.


# LANGCHAIN 

What is a Large Language Model (LLM)
Last Updated : 22 Jan, 2025
Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing.

This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP).

What are Large Language Models(LLMs)?
A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Language Model.

Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.

There are many techniques that were tried to perform natural language-related tasks but the LLM is purely based on the deep learning methodologies. LLM (Large language model) models are highly efficient in capturing the complex entity relationships in the text at hand and can generate the text using the semantic and syntactic of that particular language in which we wish to do so.

Author generated image with the use of AI
If we talk about the size of the advancements in the GPT (Generative Pre-trained Transformer) model only then:

GPT-1 which was released in 2018 contains 117 million parameters having 985 million words.
GPT-2 which was released in 2019 contains 1.5 billion parameters.
GPT-3 which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.
GPT-4 model is released in the early 2023 and it is likely to contain trillions of parameters.
GPT-4 Turbo was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.
How do Large Language Models work?
Large Language Models (LLMs) operate on the principles of deep learning, leveraging neural network architectures to process and understand human languages.

These models, are trained on vast datasets using self-supervised learning techniques. The core of their functionality lies in the intricate patterns and relationships they learn from diverse language data during training. LLMs consist of multiple layers, including feedforward layers, embedding layers, and attention layers. They employ attention mechanisms, like self-attention, to weigh the importance of different tokens in a sequence, allowing the model to capture dependencies and relationships.

Architecture of LLM
Large Language Model's (LLM) architecture is determined by a number of factors, like the objective of the specific model design, the available computational resources, and the kind of language processing tasks that are to be carried out by the LLM. The general architecture of LLM consists of many layers such as the feed forward layers, embedding layers, attention layers. A text which is embedded inside is collaborated together to generate predictions.

Important components to influence Large Language Model architecture:

Model Size and Parameter Count
input representations
Self-Attention Mechanisms
Training Objectives
Computational Efficiency
Decoding and Output Generation
Transformer-Based LLM Model Architectures
Transformer-based models, which have revolutionized natural language processing tasks, typically follow a general architecture that includes the following components:

Transformer-Geeksforgeeks

Input Embeddings: The input text is tokenized into smaller units, such as words or sub-words, and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.
Positional Encoding: Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.
Encoder: Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.
Self-Attention Mechanism: Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.
Feed-Forward Neural Network: After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.
Decoder Layers: In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.
Multi-Head Attention: Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.
Layer Normalization: Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model's ability to generalize across different inputs.
Output Layers: The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.
It's important to keep in mind that the actual architecture of transformer-based models can change and be enhanced based on particular research and model creations. To fulfill different tasks and objectives, several models like GPT, BERT, and T5 may integrate more components or modifications.

Popular Large Language Models
Now let's look at some of the famous LLMs which has been developed and are up for inference.

GPT-3: GPT 3 is developed by OpenAI, stands for Generative Pre-trained Transformer 3. This model powers ChatGPT and is widely recognized for its ability to generate human-like text across a variety of applications.
BERT: It is created by Google, is commonly used for natural language processing tasks and generating text embeddings, which can also be utilized for training other models.
RoBERTa: RoBERTa is an advanced version of BERT, stands for Robustly Optimized BERT Pretraining Approach. Developed by Facebook AI Research, it enhances the performance of the transformer architecture.
BLOOM: It is the first multilingual LLM, designed collaboratively by multiple organizations and researchers. It follows an architecture similar to GPT-3, enabling diverse language-based tasks.
For implementation details, these models are available on open-source platforms like Hugging Face and OpenAI for Python-based applications.

Large Language Models Use Cases
Code Generation: LLMs can generate accurate code based on user instructions for specific tasks.
Debugging and Documentation: They assist in identifying code errors, suggesting fixes, and even automating project documentation.
Question Answering: Users can ask both casual and complex questions, receiving detailed, context-aware responses.
Language Translation and Correction: LLMs can translate text between over 50 languages and correct grammatical errors.
Prompt-Based Versatility: By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios.
Use cases of LLM are not limited to the above-mentioned one has to be just creative enough to write better prompts and you can make these models do a variety of tasks as they are trained to perform tasks on one-shot learning and zero-shot learning methodologies as well. Due to this only Prompt Engineering is a totally new and hot topic in academics for people who are looking forward to using ChatGPT-type models extensively.

Applications of Large Language Models
LLMs, such as GPT-3, have a wide range of applications across various domains. Few of them are:

Natural Language Understanding (NLU):
Large language models power advanced chatbots capable of engaging in natural conversations.
They can be used to create intelligent virtual assistants for tasks like scheduling, reminders, and information retrieval.
Content Generation:
Creating human-like text for various purposes, including content creation, creative writing, and storytelling.
Writing code snippets based on natural language descriptions or commands.
Language Translation: Large language models can aid in translating text between different languages with improved accuracy and fluency.
Text Summarization: Generating concise summaries of longer texts or articles.
Sentiment Analysis: Analyzing and understanding sentiments expressed in social media posts, reviews, and comments.
Difference Between NLP and LLM 
NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-

Automotive routine task
Improve search 
Search engine optimization
Analyzing and organizing large documents
Social Media Analytics.
while on the other hand, LLM is a Large Language Model, and is more specific to human- like text, providing content generation, and personalized recommendations. 

What are the Advantages of Large Language Models?
Large Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications:

LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.
LLMs efficiently handle vast amounts of data, making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.
LLMs can be fine-tuned on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.
LLMs enable the automation of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.
Challenges in Training of Large Language Models
High Costs: Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.
Time-Intensive: Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.
Data Challenges: Obtaining large text datasets is difficult, and concerns about the legality of data scraping for commercial purposes have arisen.
Environmental Impact: Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns.
Conclusion
Due to the challenges faced in training LLM transfer learning is promoted heavily to get rid of all of the challenges discussed above. LLM has the capability to bring revolution in the AI-powered application but the advancements in this field seem a bit difficult because just increasing the size of the model may increase its performance but after a particular time a saturation in the performance will come and the challenges to handle these models will be bigger than the performance boost achieved by further increasing the size of the models.


